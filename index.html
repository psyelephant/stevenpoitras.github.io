<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8">
    <title>The Nutanix Bible</title>
    <link rel="stylesheet" type="text/css" href="css/nutanixbible.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="http://code.jquery.com/jquery-latest.min.js"></script>
  </head>
  <body data-type="book">

    <div class="container">
  
        <section data-type="copyright-page" class="page-title" id="the-nutanix-bible-5AoSd">
            <img src="assets/Bible.svg" alt="" class="biblesvg" />
            <h1>The Nutanix Bible</h1>
            <p>by Steven Poitras</p>
            <img src="assets/ornament1.svg" alt="" class="ornament" />
            <p class="small"><b>Copyright (c) 2015:</b> The Nutanix Bible and StevenPoitras.com, 2015. Unauthorized use and/or duplication of this material without express and written permission from this blog’s author and/or owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to Steven Poitras and StevenPoitras.com with appropriate and specific direction to the original content.</p>
        </section>

        <nav data-type="toc" class="glossary">
        <ol>
            <li><a href="book.html">Foreword</a>
                    <ol>
            <li><a href="book.html#id-RWaFk"></a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#introduction-31XsY">Introduction</a>
            </li>
        </ol>
                </li>
            <li><a href="book.html">A Brief Lesson in History</a>
                    <ol>
            <li><a href="book.html#the-evolution-of-the-datacenter-6e8IX">The Evolution of the Datacenter</a>
                    <ol>
            <li><a href="book.html#the-era-of-the-mainframe-RkKTy">The Era of the Mainframe</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#the-move-to-stand-alone-servers-RPzIr">The Move to Stand-Alone Servers</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#centralized-storage-RllSe">Centralized Storage</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#the-introduction-of-virtualization-3a0fQ">The Introduction of Virtualization</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#virtualization-matures-ZoBsd">Virtualization Matures</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#solid-state-disks-ssds-vkNFB">Solid State Disks (SSDs)</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#the-importance-of-latency-QQnFl">The Importance of Latency</a>
                    <ol>
            <li><a href="book.html#looking-at-the-bandwidth-89xt2">Looking at the Bandwidth</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#the-impact-to-memory-latency-KrlSo">The Impact to Memory Latency</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#book-of-web-scale-zkBio">Book of Web-Scale</a>
                    <ol>
            <li><a href="book.html#hyper-convergence-O7pCP">Hyper-Convergence</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#software-defined-intelligence-v8WtB">Software-Defined Intelligence</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#distributed-autonomous-systems-ElQiA">Distributed Autonomous Systems</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#incremental-and-linear-scale-out-kKYCy">Incremental and linear scale out</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#making-sense-of-it-all-2Kwsx">Making Sense of It All</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html">Book of Prism</a>
                    <ol>
            <li><a href="book.html#architecture-59Gug">Architecture</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#navigation-ZoBsd">Navigation</a>
                    <ol>
            <li><a href="book.html#prism-central-jEXiJ">Prism Central</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#prism-element-LvVtG">Prism Element</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#usage-and-troubleshooting-aQahQ">Usage and Troubleshooting</a>
                    <ol>
            <li><a href="book.html#nutanix-software-upgrade-AvxHd">Nutanix Software Upgrade</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#hypervisor-upgrade-MBpTq">Hypervisor Upgrade</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#cluster-expansion-add-node-VGvSL">Cluster Expansion (add node)</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#capacity-planning-x8nTv">Capacity Planning</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#apis-and-interfaces-v8WtB">APIs and Interfaces</a>
                    <ol>
            <li><a href="book.html#acli-e0acX">ACLI</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#ncli-0ozUK">NCLI</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#powershell-cmdlets-r2kfb">PowerShell CMDlets</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html">Book of Acropolis</a>
                    <ol>
            <li><a href="book.html#architecture-59Gug">Architecture</a>
                    <ol>
            <li><a href="book.html#acropolis-services-68Pt2">Acropolis Services</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#converged-platform-JJlUl">Converged Platform</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#software-defined-jEXiJ">Software-Defined</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#cluster-components-lMlIe">Cluster Components</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#drive-breakdown-XjGH2">Drive Breakdown</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#distributed-storage-fabric-WzDUk">Distributed Storage Fabric</a>
                    <ol>
            <li><a href="book.html#data-structure-9zbIg">Data Structure</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#io-path-overview-LdEtG">I/O Path Overview</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#data-protection-O7pCP">Data Protection</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#scalable-metadata-nLVFV">Scalable Metadata</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#data-path-resiliency-e0acX">Data Path Resiliency</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#elastic-dedupe-engine-D7yFl">Elastic Dedupe Engine</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#compression-Y0EC1">Compression</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#erasure-coding-ZLoid">Erasure Coding</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#storage-tiering-and-prioritization-7Wwhp">Storage Tiering and Prioritization</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#disk-balancing-emOTX">Disk Balancing</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#availability-domains-gZxiv">Availability Domains</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#snapshots-and-clones-eKOHX">Snapshots and Clones</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#replication-and-multi-site-disaster-recovery-OKAsP">Replication and Multi-Site Disaster Recovery</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#cloud-connect-rGxtb">Cloud Connect</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#metro-availability-dPMtp">Metro Availability</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#volumes-api-Zxdsd">Volumes API</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#networking-and-io-OObcP">Networking and I/O</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#data-locality-n17FV">Data Locality</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#shadow-clones-KmPIo">Shadow Clones</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#storage-layers-and-monitoring-9lJcg">Storage Layers and Monitoring</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#application-mobility-fabric-coming-soon-Me4Iq">Application Mobility Fabric - coming soon!</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#hypervisor-QwvHl">Hypervisor</a>
                    <ol>
            <li><a href="book.html#node-architecture-KAPho">Node Architecture</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#kvm-architecture-DxPIl">KVM Architecture</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#configuration-maximums-and-scalability-zyySo">Configuration Maximums and Scalability</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#networking-qLgu2">Networking</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#how-it-works-8KNc2">How It Works</a>
                    <ol>
            <li><a href="book.html#iscsi-multi-pathing-dDWtp">iSCSI Multi-pathing</a>
            </li>
            <li><a href="book.html#ip-address-management-DkjFl">IP Address Management</a>
            </li>
        </ol>
                </li>
            <li><a href="book.html#administration-Pm8fr">Administration</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#important-pages-jXjFJ">Important Pages</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#command-reference-oMmTL">Command Reference</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#metrics-and-thresholds-E72IA">Metrics and Thresholds</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#troubleshooting-andamp-advanced-administration-K0mSo">Troubleshooting &amp; Advanced Administration</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#administration-xWrIv">Administration</a>
                    <ol>
            <li><a href="book.html#important-pages-OexCP">Important Pages</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#cluster-commands-D8wHl">Cluster Commands</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#metrics-and-thresholds-WQack">Metrics and Thresholds</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#gflags-DApul">Gflags</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#troubleshooting-andamp-advanced-administration-J4lFl">Troubleshooting &amp; Advanced Administration</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html">Book of vSphere</a>
                    <ol>
            <li><a href="book.html#architecture-RWaFk">Architecture</a>
                    <ol>
            <li><a href="book.html#node-architecture-3GWta">Node Architecture</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#configuration-maximums-and-scalability-6ZBUd">Configuration Maximums and Scalability</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#how-it-works-5qaC2">How It Works</a>
                    <ol>
            <li><a href="book.html#array-offloads-vaai-6YyH1">Array Offloads – VAAI</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#cvm-autopathing-aka-hapy-3a0fQ">CVM Autopathing aka Ha.py</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#administration-ZoBsd">Administration</a>
                    <ol>
            <li><a href="book.html#important-pages-yJDfd">Important Pages</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#command-reference-wbKIx">Command Reference</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#metrics-and-thresholds-WjDtk">Metrics and Thresholds</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#troubleshooting-andamp-advanced-administration-Dvlcl">Troubleshooting &amp; Advanced Administration</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html">Book of Hyper-V</a>
                    <ol>
            <li><a href="book.html#architecture-RWaFk">Architecture</a>
                    <ol>
            <li><a href="book.html#node-architecture-3GWta">Node Architecture</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#configuration-maximums-and-scalability-6ZBUd">Configuration Maximums and Scalability</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#how-it-works-5qaC2">How It Works</a>
                    <ol>
            <li><a href="book.html#array-offloads-odx-6YyH1">Array Offloads – ODX</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#administration-5bnsD">Administration</a>
                    <ol>
            <li><a href="book.html#important-pages-3a0fQ">Important Pages</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#command-reference-AboId">Command Reference</a>
                    <ol>
        </ol>
                </li>
            <li><a href="book.html#metrics-and-thresholds-EYksA">Metrics and Thresholds</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html#troubleshooting-andamp-advanced-administration-KpXSo">Troubleshooting &amp; Advanced Administration</a>
                    <ol>
        </ol>
                </li>
        </ol>
                </li>
            <li><a href="book.html">Afterword</a>
                    <ol>
        </ol>
                </li>
        </ol>
        </nav>

        <div class="separator"></div>

        <section data-type="preface" class="preface" id="foreword-5AoSd">
            <h1>Foreword</h1>

            <section data-type="sect1" id="id-RWaFk">
                <p>&nbsp;</p>

                <figure class="dheeraj" id="id-5vZt9">
                    <img align="middle" alt="" class="iimagesv2dheeraj_pandeyjpg" src="imagesv2/Dheeraj_Pandey.jpg">
                    <figcaption><span class="label">Figure 1-1. </span>Dheeraj Pandey, CEO, Nutanix</figcaption>
                </figure>

                <blockquote>
                    <p>I am honored to write a foreword for this book that we've come to call "The Nutanix Bible." First and foremost, let me address the name of the book, which to some would seem not fully inclusive vis-à-vis their own faiths, or to others who are agnostic or atheist. There is a Merriam Webster meaning of the word "bible" that is not literally about scriptures: "a publication that is preeminent especially in authoritativeness or wide readership". And that is how you should interpret its roots. It started being written by one of the most humble yet knowledgeable employees at Nutanix, Steven Poitras, our first Solution Architect who continues to be authoritative on the subject without wielding his "early employee" primogeniture. Knowledge to him was not power -- the act of sharing that knowledge is what makes him eminently powerful in this company. Steve epitomizes culture in this company -- by helping everyone else out with his authority on the subject, by helping them automate their chores in Power Shell or Python, by building insightful reference architectures (that are beautifully balanced in both content and form), by being a real-time buddy to anyone needing help on Yammer or Twitter, by being transparent with engineers on the need to self-reflect and self-improve, and by being ambitious.</p>

                    <p>When he came forward to write a blog, his big dream was to lead with transparency, and to build advocates in the field who would be empowered to make design trade-offs based on this transparency. It is rare for companies to open up on design and architecture as much as Steve has with his blog. Most open source companies -- who at the surface might seem transparent because their code is open source -- never talk in-depth about design, and "how it works" under the hood. When our competitors know about our product or design weaknesses, it makes us stronger -- because there is very little to hide, and everything to gain when something gets critiqued under a crosshair. A public admonition of a feature trade-off or a design decision drives the entire company on Yammer in quick time, and before long, we've a conclusion on whether it is a genuine weakness or a true strength that someone is fear-mongering on. Nutanix Bible, in essence, protects us from drinking our own kool aid. That is the power of an honest discourse with our customers and partners.</p>

                    <p>This ever-improving artifact, beyond being authoritative, is also enjoying wide readership across the world. Architects, managers, and CIOs alike, have stopped me in conference hallways to talk about how refreshingly lucid the writing style is, with some painfully detailed illustrations, visio diagrams, and pictorials. Steve has taken time to tell the web-scale story, without taking shortcuts. Democratizing our distributed architecture was not going to be easy in a world where most IT practitioners have been buried in dealing with the "urgent". The Bible bridges the gap between IT and DevOps, because it attempts to explain computer science and software engineering trade-offs in very simple terms. We hope that in the coming 3-5 years, IT will speak a language that helps them get closer to the DevOps' web-scale jargon.</p>

                    <p>With this first edition, we are converting Steve's blog into a book. The day we stop adding to this book is the beginning of the end of this company. I expect each and everyone of you to keep reminding us of what brought us this far: truth, the whole truth, and nothing but the truth, will set you free (from complacency and hubris).</p>

                    <p>Keep us honest.</p>
                </blockquote>

        <p class="sign">Dheeraj Pandey, CEO, Nutanix</p>

        <p>&nbsp;</p>

        <figure id="id-5bOFJ"><img align="middle" alt="" class="iimagesv2stujpg" src="imagesv2/Stu.jpg"  >
        <figcaption><span class="label">Figure 1-2. Stuart Miniman, Principal Research Contributor, Wikibon</span>
        </figcaption>
        </figure>

        <blockquote>
        <p>Users today are constantly barraged by new technologies. There is no limit of new opportunities for IT to change to a "new and better way", but the adoption of new technology and more importantly, the change of operations and processes is difficult. Even the huge growth of open source technologies has been hampered by lack of adequate documentation. Wikibon was founded on the principal that the community can help with this problem and in that spirit, The Nutanix Bible, which started as a blog post by Steve Poitras, has become a valuable reference point for IT practitioners that want to learn about hypercovergence and web-scale principles or to dig deep into Nutanix and hypervisor architectures. The concepts that Steve has written about are advanced software engineering problems that some of the smartest engineers in the industry have designed a solution for. The book explains these technologies in a way that is understandable to IT generalists without compromising the technical veracity.</p>

        <p>The concepts of distributed systems and software-led infrastructure are critical for IT practitioners to understand. I encourage both Nutanix customers and everyone who wants to understand these trends to read the book. The technologies discussed here power some of the largest datacenters in the world.</p>
        </blockquote>

        <p class="sign">Stuart Miniman, Principal Research Contributor, Wikibon</p>

        <p>&nbsp;</p>
        </section>

        <div class="separator"></div>

        <section data-type="sect2" id="introduction-31XsY">
        <h1>Introduction</h1>

        <figure id="id-5JrfB"><img align="middle" alt="" class="iimagesv2209855_10100371454234638_4947938_ojpg" src="imagesv2/209855_10100371454234638_4947938_o.jpg"  >
        <figcaption><span class="label">Figure 1-3. </span>Steven Poitras, Principal Solutions Architect, Nutanix</figcaption>
        </figure>

        <blockquote>
        <p>Welcome to The Nutanix Bible!&nbsp; I work with the Nutanix platform on a daily basis – trying to find issues, push its limits as well as administer it for my production benchmarking lab.&nbsp; This item is being produced to serve as a living document outlining tips and tricks used every day by myself and a variety of engineers here at Nutanix.</p>

        <p>NOTE: What you see here is an under the covers look at how things work.&nbsp; With that said, all topics discussed are abstracted by Nutanix and knowledge isn't required to successfully operate a Nutanix environment!</p>

        <p>Enjoy!</p>
        </blockquote>

        <p class="sign">Steven Poitras, Principal Solutions Architect, Nutanix</p>
        </section>
        </section>

        <div class="separator"></div>

        <div data-type="part" id="a-brief-lesson-in-history-6YeSv">
        <h1><span class="label">Part I. </span>A Brief Lesson in History</h1>

        <p>A brief look at the history of infrastructure and what has led us to where we are today.</p>

        <section data-type="chapter" id="the-evolution-of-the-datacenter-6e8IX">
        <h2>The Evolution of the Datacenter</h2>

        <p>The datacenter has evolved significantly over the last several decades. The following sections will examine each era in detail.&nbsp;&nbsp;</p>

        <section data-type="sect1" id="the-era-of-the-mainframe-RkKTy">
        <h2>The Era of the Mainframe</h2>

        <p>The mainframe ruled for many years and laid the core foundation of where we are today. It allowed companies to leverage the following key characteristics:</p>

        <ul>
        	<li>Natively converged CPU, main memory, and storage</li>
        	<li>Engineered internal redundancy</li>
        </ul>

        <p>But the mainframe also introduced the following issues:</p>

        <ul>
        	<li>The high costs of procuring infrastructure</li>
        	<li>Inherent complexity</li>
        	<li>&nbsp;A lack of flexibility and highly siloed environments</li>
        </ul>
        </section>

        <section data-type="sect1" id="the-move-to-stand-alone-servers-RPzIr">
        <h2>The Move to Stand-Alone Servers</h2>

        <p>With mainframes, it was very difficult for organizations within a business to leverage these capabilities which partly led to the entrance of pizza boxes or stand-alone servers. Key characteristics of stand-alone servers included:</p>

        <ul>
        	<li>CPU, main memory, and DAS storage</li>
        	<li>Higher flexibility than the mainframe</li>
        	<li>Accessed over the network</li>
        </ul>

        <p>These stand-alone servers introduced more issues:</p>

        <ul>
        	<li>Increased number of silos</li>
        	<li>Low or unequal resource utilization</li>
        	<li>The server became a single point of failure (SPOF) for both compute AND storage</li>
        </ul>
        </section>

        <section data-type="sect1" id="centralized-storage-RllSe">
        <h2>Centralized Storage</h2>

        <p>Businesses always need to make money and data is a key piece of that puzzle. With direct-attached storage (DAS), organizations either needed more space than was locally available, or data high availability (HA) where a server failure wouldn’t cause data unavailability.</p>

        <p>Centralized storage replaced both the mainframe and the stand-alone server with sharable, larger pools of storage that also provided data protection. Key characteristics of centralized storage included:</p>

        <ul>
        	<li>Pooled storage resources led to better storage utilization</li>
        	<li>Centralized data protection via RAID eliminated the chance that server loss caused data loss</li>
        	<li>Storage were performed over the network</li>
        </ul>

        <p>Issues with centralized storage included:</p>

        <ul>
        	<li>They were potentially more expensive, however data is more valuable than the hardware</li>
        	<li>Increased complexity (SAN Fabric, WWPNs, RAID groups, volumes, spindle counts, etc.)</li>
        	<li>They required another management tool / team</li>
        </ul>
        </section>

        <section data-type="sect1" id="the-introduction-of-virtualization-3a0fQ">
        <h2>The Introduction of Virtualization</h2>

        <p>At this point in time, compute utilization was low and resource efficiency was impacting the bottom line. Virtualization was then introduced and enabled multiple workloads and operating systems (OSs) to run as virtual machines (VMs) on a single piece of hardware. Virtualization enabled businesses to increase utilization of their pizza boxes, but also increased the number of silos and the impacts of an outage. Key characteristics of virtualization included:</p>

        <ul>
        	<li>Abstracting the OS from hardware (VM)</li>
        	<li>Very efficient compute utilization led to workload consolidation</li>
        </ul>

        <p>Issues with virtualization included:</p>

        <ul>
        	<li>An increase in the number of silos and management complexity</li>
        	<li>A lack of VM high-availability, so if a compute node failed the impact was much larger</li>
        	<li>A lack of pooled resources</li>
        	<li>The need for another management tool / team</li>
        </ul>
        </section>

        <section data-type="sect1" id="virtualization-matures-ZoBsd">
        <h2>Virtualization Matures</h2>

        <p>The hypervisor became a very efficient and feature-filled solution. With the advent of tools, including VMware vMotion, HA, and DRS, users obtained the ability to provide VM high availability and migrate compute workloads dynamically. The only caveat was the reliance on centralized storage, causing the two paths to merge. The only down turn was the increased load on the storage array before and VM sprawl led to contention for storage I/O. Key characteristics included:</p>

        <ul>
        	<li>Clustering led to pooled compute resources</li>
        	<li>The ability to dynamically migrate workloads between compute nodes (DRS / vMotion)</li>
        	<li>The introduction of VM high availability (HA) in the case of a compute node failure</li>
        	<li>A requirement for centralized storage</li>
        </ul>

        <p>Issues included:</p>

        <ul>
        	<li>Higher demand on storage due to VM sprawl</li>
        	<li>Requirements to scale out more arrays creating more silos and more complexity</li>
        	<li>Higher $ / GB due to requirement of an array</li>
        	<li>The possibility of resource contention on array</li>
        	<li>It made storage configuration much more complex due to the necessity to ensure:
        	<ul>
        		<li>VM to datastore / LUN ratios</li>
        		<li>Spindle count to facilitate I/O requirements</li>
        	</ul>
        	</li>
        </ul>
        </section>

        <section data-type="sect1" id="solid-state-disks-ssds-vkNFB">
        <h2>Solid State Disks (SSDs)</h2>

        <p>SSDs helped alleviate this I/O bottleneck by providing much higher I/O performance without the need for tons of disk enclosures.&nbsp; However, given the extreme advances in performance, the controllers and network had not yet evolved to handle the vast I/O available. Key characteristics of SSDs included:</p>

        <ul>
        	<li>Much higher I/O characteristics than traditional HDD</li>
        	<li>Essentially eliminated seek times</li>
        </ul>

        <p>SSD issues included:</p>

        <ul>
        	<li>The bottleneck shifted from storage I/O on disk to the controller / network</li>
        	<li>Silos still remained</li>
        	<li>Array configuration complexity still remained</li>
        </ul>
        </section>
        </section>

        <section data-type="chapter" id="the-importance-of-latency-QQnFl">
        <h2>The Importance of Latency</h2>

        <p>The figure below characterizes the various latencies for specific types of I/O:</p>

        <p><em>L1 cache reference&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.5 ns</em></p>

        <p><em>Branch mispredict&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp; ns</em></p>

        <p><em>L2 cache reference&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14x L1 cache</em></p>

        <p><em>Mutex lock/unlock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 25&nbsp;&nbsp; ns</em></p>

        <p><em>Main memory reference&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 100&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 20x L2 cache, 200x L1 cache</em></p>

        <p><em>Compress 1K bytes with Zippy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3,000&nbsp;&nbsp; ns</em></p>

        <p><em>Send 1K bytes over 1 Gbps network&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10,000&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp; 0.01 ms</em></p>

        <p><em>Read 4K randomly from SSD*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 150,000&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp; 0.15 ms</em></p>

        <p><em>Read 1 MB sequentially from memory&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 250,000&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp; 0.25 ms</em></p>

        <p><em>Round trip within same datacenter&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 500,000&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp; 0.5&nbsp; ms</em></p>

        <p><em>Read 1 MB sequentially from SSD*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1,000,000&nbsp;&nbsp; ns&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; ms &nbsp;4X memory</em></p>

        <p><em>Disk seek&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10,000,000&nbsp;&nbsp; ns&nbsp;&nbsp; 10&nbsp;&nbsp;&nbsp; ms&nbsp; 20x datacenter roundtrip</em></p>

        <p><em>Read 1 MB sequentially from disk&nbsp;&nbsp;&nbsp;&nbsp; 20,000,000&nbsp;&nbsp; ns&nbsp;&nbsp; 20&nbsp;&nbsp;&nbsp; ms&nbsp; 80x memory, 20X SSD</em></p>

        <p><em>Send packet CA-&gt;Netherlands-&gt;CA&nbsp;&nbsp;&nbsp;&nbsp; 150,000,000&nbsp;&nbsp; ns&nbsp; 150&nbsp;&nbsp;&nbsp; ms</em></p>

        <p><em>(credit: Jeff Dean, https://gist.github.com/jboner/2841832)</em></p>

        <p>The table above shows that the CPU can access its caches at anywhere from ~0.5-7ns (L1 vs. L2). For main memory, these accesses occur at ~100ns, whereas a local 4K SSD read is ~150,000ns or 0.15ms.</p>

        <p>If we take a typical enterprise-class SSD (in this case the Intel S3700 - <a href="http://download.intel.com/newsroom/kits/ssd/pdfs/Intel_SSD_DC_S3700_Product_Specification.pdf">SPEC</a>), this device is capable of the following:</p>

        <ul>
        	<li>Random I/O performance:
        	<ul>
        		<li>Random 4K Reads: Up to 75,000 IOPS</li>
        		<li>Random 4K Writes: Up to 36,000 IOPS</li>
        	</ul>
        	</li>
        	<li>Sequential bandwidth:
        	<ul>
        		<li>Sustained Sequential Read: Up to 500MB/s</li>
        		<li>Sustained Sequential Write: Up to 460MB/s</li>
        	</ul>
        	</li>
        	<li>Latency:
        	<ul>
        		<li>Read: 50us</li>
        		<li>Write: 65us</li>
        	</ul>
        	</li>
        </ul>

        <section data-type="sect1" id="looking-at-the-bandwidth-89xt2">
        <h2>Looking at the Bandwidth</h2>

        <p>For traditional storage, there are a few main types of media for I/O:</p>

        <ul>
        	<li>Fiber Channel (FC)
        	<ul>
        		<li>4-, 8-, and 10-Gb</li>
        	</ul>
        	</li>
        	<li>Ethernet (including FCoE)
        	<ul>
        		<li>1-, 10-Gb, (40-Gb IB), etc.</li>
        	</ul>
        	</li>
        </ul>

        <p>For the calculation below, we are using the 500MB/s Read and 460MB/s Write BW available from the Intel S3700.</p>

        <p>The calculation is done as follows:</p>

        <p>numSSD = ROUNDUP((numConnections * connBW (in GB/s))/ ssdBW (R or W))</p>

        <p><i>NOTE:&nbsp;</i><em>Numbers were rounded up as a partial SSD isn’t possible. This also does not account for the necessary CPU required to handle all of the I/O and assumes unlimited controller CPU power.</em></p>

        <table>
        	<tbody>
        		<tr>
        			<td colspan="2" rowspan="1">Network BW</td>
        			<td colspan="2" rowspan="1">SSDs required to saturate network BW</td>
        		</tr>
        		<tr>
        			<td >
        			<p>Controller Connectivity</p>
        			</td>
        			<td>Available Network BW</td>
        			<td>Read I/O</td>
        			<td>Write I/O</td>
        		</tr>
        		<tr>
        			<td>Dual 4Gb FC</td>
        			<td>8Gb == 1GB</td>
        			<td>2</td>
        			<td>3</td>
        		</tr>
        		<tr>
        			<td>Dual 8Gb FC</td>
        			<td>16Gb == 2GB</td>
        			<td>4</td>
        			<td>5</td>
        		</tr>
        		<tr>
        			<td>Dual 16Gb FC</td>
        			<td>32Gb == 4GB</td>
        			<td>8</td>
        			<td>9</td>
        		</tr>
        		<tr>
        			<td>Dual 1Gb ETH</td>
        			<td>2Gb == 0.25GB</td>
        			<td>1</td>
        			<td>1</td>
        		</tr>
        		<tr>
        			<td>Dual 10Gb ETH</td>
        			<td>20Gb == 2.5GB</td>
        			<td>5</td>
        			<td>6</td>
        		</tr>
        	</tbody>
        </table>

        <p>As the table shows, if you wanted to leverage the theoretical maximum performance an SSD could offer, the network can become a bottleneck with anywhere from 1 to 9 SSDs depending on the type of networking leveraged</p>
        </section>

        <section data-type="sect1" id="the-impact-to-memory-latency-KrlSo">
        <h2>The Impact to Memory Latency</h2>

        <p>Typical main memory latency is ~100ns (will vary), we can perform the following calculations:</p>

        <ul>
        	<li>Local memory read latency = 100ns + [OS / hypervisor overhead]</li>
        	<li>Network memory read latency = 100ns + NW RTT latency + [2 x OS / hypervisor overhead]</li>
        </ul>

        <p>If we assume a typical network RTT is ~0.5ms (will vary by switch vendor) which is ~500,000ns that would come down to:</p>

        <ul>
        	<li>Network memory read latency = 100ns + 500,000ns + [2 x OS / hypervisor overhead]</li>
        </ul>

        <p>If we theoretically assume a very fast network with a 10,000ns RTT:</p>

        <ul>
        	<li>Network memory read latency = 100ns + 10,000ns + [2 x OS / hypervisor overhead]</li>
        </ul>

        <p>What that means is even with a theoretically fast network, there is a 10,000% overhead when compared to a non-network memory access. With a slow network this can be upwards of a 500,000% latency overhead.</p>

        <p>In order to alleviate this overhead, server side caching technologies are introduced.</p>
        </section>
        </section>

        <section data-type="chapter" id="book-of-web-scale-zkBio">
        <h1>Book of Web-Scale</h1>

        <p>This section will present some of the core concepts behind “Web-scale” infrastructure and why we leverage them. Before I get started, I just wanted to clearly state the Web-scale doesn’t mean you need to be “web-scale” (e.g. Google, Facebook, or Microsoft).&nbsp; These constructs are applicable and beneficial at any scale (3-nodes or thousands of nodes).</p>

        <p>Historical challenges included:</p>

        <ul>
        	<li>Complexity, complexity, complexity</li>
        	<li>Desire for incremental based growth</li>
        	<li>The need to be agile</li>
        </ul>

        <p>There are a few key constructs used when talking about “Web-scale” infrastructure:</p>

        <ul>
        	<li>Hyper-convergence</li>
        	<li>Software defined intelligence</li>
        	<li>Distributed autonomous systems</li>
        	<li>Incremental and linear scale out</li>
        </ul>

        <p>Other related items:</p>

        <ul>
        	<li>API-based automation and rich analytics</li>
        	<li>Self-healing</li>
        </ul>

        <p>The following sections will provide a technical perspective on what they actually mean.</p>

        <section data-type="sect1" id="hyper-convergence-O7pCP">
        <h2>Hyper-Convergence</h2>

        <p>There are differing opinions on what hyper-convergence actually is.&nbsp; It also varies based on the scope of components (e.g. virtualization, networking, etc.). However, the core concept comes down to the following: natively combining two or more components into a single unit. ‘Natively’ is the key word here. In order to be the most effective, the components must be natively integrated and not just bundled together. In the case of Nutanix, we natively converge compute + storage to form a single node used in our appliance.&nbsp; For others, this might be converging storage with the network, etc. What it really means:</p>

        <ul>
        	<li>Natively integrating two or more components into a single unit which can be easily scaled</li>
        </ul>

        <p>Benefits include:</p>

        <ul>
        	<li>Single unit to scale</li>
        	<li>Localized I/O</li>
        	<li>Eliminates traditional compute / storage silos by converging them</li>
        </ul>
        </section>

        <section data-type="sect1" id="software-defined-intelligence-v8WtB">
        <h2>Software-Defined Intelligence</h2>

        <p>Software-defined intelligence is taking the core logic from normally proprietary or specialized hardware (e.g. ASIC / FPGA) and doing it in software on commodity hardware. For Nutanix, we take the traditional storage logic (e.g. RAID, deduplication, compression, etc.) and put that into software that runs in each of the Nutanix CVMs on standard x86 hardware. What it really means:</p>

        <ul>
        	<li>Pulling key logic from hardware and doing it in software on commodity hardware</li>
        </ul>

        <p>Benefits include:</p>

        <ul>
        	<li>Rapid release cycles</li>
        	<li>Elimination of proprietary hardware reliance</li>
        	<li>Utilization of commodity hardware for better economics</li>
        </ul>
        </section>

        <section data-type="sect1" id="distributed-autonomous-systems-ElQiA">
        <h2>Distributed Autonomous Systems</h2>

        <p>Distributed autonomous systems involve moving away from the traditional concept of having a single unit responsible for doing something and distributing that role among all nodes within the cluster.&nbsp; You can think of this as creating a purely distributed system. Traditionally, vendors have assumed that hardware will be reliable, which, in most cases can be true.&nbsp; However, core to distributed systems is the idea that hardware will eventually fail and handling that fault in an elegant and non-disruptive way is key.</p>

        <p>These distributed systems are designed to accommodate and remediate failure, to form something that is self-healing and autonomous.&nbsp; In the event of a component failure, the system will transparently handle and remediate the failure, continuing to operate as expected. Alerting will make the user aware, but rather than being a critical time-sensitive item, any remediation (e.g. replace a failed node) can be done on the admin’s schedule.&nbsp; Another way to put it is fail in-place (rebuild without replace) For items where a “master” is needed an election process is utilized, in the event this master fails a new master is elected.&nbsp; To distribute the processing of tasks MapReduce concepts are leveraged. What it really means:</p>

        <ul>
        	<li>Distributing roles and responsibilities to all nodes within the system</li>
        	<li>Utilizing concepts like MapReduce to perform distributed processing of tasks</li>
        	<li>Using an election process in the case where a “master” is needed</li>
        </ul>

        <p>Benefits include:</p>

        <ul>
        	<li>Eliminates any single points of failure (SPOF)</li>
        	<li>Distributes workload to eliminate any bottlenecks</li>
        </ul>
        </section>

        <section data-type="sect1" id="incremental-and-linear-scale-out-kKYCy">
        <h2>Incremental and linear scale out</h2>

        <p>Incremental and linear scale out relates to the ability to start with a certain set of resources and as needed scale them out while linearly increasing the performance of the system.&nbsp; All of the constructs mentioned above are critical enablers in making this a reality. For example, traditionally you’d have 3-layers of components for running virtual workloads: servers, storage, and network – all of which are scaled independently.&nbsp; As an example, when you scale out the number of servers you’re not scaling out your storage performance. With a hyper-converged platform like Nutanix, when you scale out with new node(s) you’re scaling out:</p>

        <ul>
        	<li>The number of hypervisor / compute nodes</li>
        	<li>The number of storage controllers</li>
        	<li>The compute and storage performance / capacity</li>
        	<li>The number of nodes participating in cluster wide operations</li>
        </ul>

        <p>What it really means:</p>

        <ul>
        	<li>The ability to incrementally scale storage / compute with linear increases to performance / ability</li>
        </ul>

        <p>Benefits include:</p>

        <ul>
        	<li>The ability to start small and scale</li>
        	<li>Uniform and consistent performance at any scale</li>
        </ul>
        </section>
        </section>

        <section data-type="sect1" id="making-sense-of-it-all-2Kwsx">
        <h2>Making Sense of It All</h2>

        <p>In summary:</p>

        <ol>
        	<li>Inefficient compute utilization led to the move to virtualization</li>
        	<li>Features including vMotion, HA, and DRS led to the requirement of centralized storage</li>
        	<li>VM sprawl led to the increase load and contention on storage</li>
        	<li>SSDs came in to alleviate the issues but changed the bottleneck to the network / controllers</li>
        	<li>Cache / memory accesses over the network face large overheads, minimizing their benefits</li>
        	<li>Array configuration complexity still remains the same</li>
        	<li>Server side caches were introduced to alleviate the load on the array / impact of the network, however introduces another component to the solution</li>
        	<li>Locality helps alleviate the bottlenecks / overheads traditionally faced when going over the network</li>
        	<li>Shifts the focus from infrastructure to ease of management and simplifying the stack</li>
        	<li>The birth of the Web-Scale world!</li>
        </ol>
        </section>
        </div>

        <div data-type="part" id="book-of-prism-6YeSv">
        <h1><span class="label">Part II. </span>Book of Prism</h1>

        <p class='definition'><strong>prism - /'prizɘm/ - noun - control plane</strong>
        <br/>
        one-click management and interface for datacenter operations.</p>

        <section data-type="chapter" id="architecture-59Gug">
        <h2>Architecture</h2>

        <p>Prism is a distributed resource management platform which allows users to manage and monitor objects and services across their Nutanix environment.</p>

        <p>These capabilities are broken down into two key categories:</p>

        <ul>
        	<li>Interfaces
        	<ul>
        		<li>HTML5 UI, REST API, CLI, PowerShell CMDlets, etc.</li>
        	</ul>
        	</li>
        	<li>Management
        	<ul>
        		<li>Policy definition and compliance, service design and status, analytics and monitoring</li>
        	</ul>
        	</li>
        </ul>

        <p>The figure highlights an image illustrating the conceptual nature of Prism as part of the Nutanix platform:</p>

        <figure id="id-5AGcY"><img alt="" class="iimagesv2arch_prismpng" src="imagesv2/arch_prism.png">
        <figcaption><span class="label">Figure 4-1. </span>High-Level Prism Architecture</figcaption>
        </figure>

        <p>Prism is broken down into two main components:</p>

        <ul>
        	<li>Prism Central (PC)
        	<ul>
        		<li>Multi-cluster manager responsible for managing multiple Nutanix clusters to provide a single, centralized management interface. &nbsp;Prism Central is an optional software appliance (VM) which can be deployed in addition to the Nutanix cluster (can run on it).</li>
        		<li>1-to-many cluster manager</li>
        	</ul>
        	</li>
        	<li>Prism Element (PE)
        	<ul>
        		<li>Localized cluster manager responsible for local cluster management and operations. &nbsp;Every Nutanix cluster has Prism Element built-in.</li>
        		<li>1-to-1 cluster manager</li>
        	</ul>
        	</li>
        </ul>

        <p>The figure shows an image illustrating the conceptual relationship between Prism Central and Prism Element:</p>

        <figure id="id-5wrcV" class="small"><img alt="" class="iimagesv2prism_arch2png" src="imagesv2/prism_arch2.png">
        <figcaption><span class="label">Figure 4-2. </span>Prism Architecture</figcaption>
        </figure>

        <div  data-type="note" class="note" id="pro-tip-5jkU4"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>For larger or distributed deployments (e.g. more than one cluster or multiple sites) it is recommended to use Prism Central to simplify operations and provide a single management UI for all clusters / sites.</p>
        </div>

        <h2>Prism Services</h2>

        <p>A Prism service runs on every CVM with an elected Prism Leader which is responsible for handling HTTP requests.&nbsp; Similar to other components which have a Master, if the Prism Leader fails, a new one will be elected. &nbsp;When a CVM which is not the Prism Leader gets a HTTP request it will permanently redirect the request to the current Prism Leader using HTTP response status code 301.</p>

        <p>Here we show a conceptual view of the Prism services and how HTTP request(s) are handled:</p>

        <figure id="id-3ggFJ"><img alt="" class="iimagesv2prism_services3png" src="imagesv2/prism_services3.png">
        <figcaption><span class="label">Figure 4-3. </span>Prism Services - Request Handling</figcaption>
        </figure>

        <div  data-type="note" class="note" id="prism-ports-lM8hQ"><h6>Note</h6>
        <h1>Prism ports</h1>

        <p>Prism listens on ports 80 and 9440, if HTTP traffic comes in on port 80 it is redirected to HTTPS on port 9440.</p>
        </div>

        <p>When using the cluster external IP (recommended),&nbsp;it will always be hosted by the current Prism Leader. &nbsp;In the event of a Prism Leader failure the cluster IP will be assumed by the newly elected Prism Leader and a gratuitous ARP (gARP) will be used to clean any stale ARP cache entries. &nbsp;In this scenario any time the cluster IP is used to access Prism, no redirection is necessary as that will already be the Prism Leader.</p>

        <div  data-type="note" class="note" id="pro-tip-mX0ud"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>You can determine the current Prism leader by running 'curl localhost:2019/prism/leader' on any CVM.</p>
        </div>
        </section>

        <section data-type="chapter" id="navigation-ZoBsd">
        <h2>Navigation</h2>

        <p>Prism is fairly straight forward and simple to use, however we'll cover some of the main pages and basic usage.</p>

        <p>Prism Central (if deployed) can be accessed using the IP address specified during configuration or corresponding DNS entry. &nbsp;Prism Element can be accessed via Prism Central (by clicking on a specific cluster) or by navigating to any Nutanix CVM or cluster IP (preferred).</p>

        <p>Once the page has been loaded you will be greeted with the Login page where you will use your Prism or Active Directory credentials to login.</p>

        <figure id="id-QQAIL" class="small"><img alt="" class="iimagesv2prismprism_loginpng" src="imagesv2/Prism/prism_login.png">
        <figcaption><span class="label">Figure 5-1. </span>Prism Login Page</figcaption>
        </figure>

        <p>Upon successful login you will be sent to the dashboard page which will provide overview information for managed cluster(s) in Prism Central or the local cluster in Prism Element.</p>

        <p>Prism Central and Prism Element will be covered in more detail in the following sections.</p>

        <section data-type="sect1" id="prism-central-jEXiJ">
        <h2>Prism Central</h2>

        <p>Prism Central contains the following main pages:</p>

        <ul>
        	<li>Home Page
        	<ul>
        		<li>Environment wide monitoring dashboard including detailed information on service status, capacity planning, performance, tasks, etc. &nbsp;To get further information on any of them you can click on the item of interest.</li>
        	</ul>
        	</li>
        	<li>Explore Page
        	<ul>
        		<li>Management and monitoring of services, cluster, VMs and hosts</li>
        	</ul>
        	</li>
        	<li>Analysis Page
        	<ul>
        		<li>Detailed performance analysis for cluster and managed objects with event correlation</li>
        	</ul>
        	</li>
        	<li>Alerts
        	<ul>
        		<li>Environment wide alerts</li>
        	</ul>
        	</li>
        </ul>

        <p>The figure shows a sample Prism Central dashboard where multiple clusters can be monitored / managed:</p>

        <figure id="id-PjpCm" class="large"><img alt="" class="iimagesv2prismpc_dashboardpng" src="imagesv2/Prism/pc_dashboard.png">
        <figcaption><span class="label">Figure 5-2. </span>Prism Central - Dashboard</figcaption>
        </figure>

        <p>From here you can monitor the overall status of your environment, and dive deeper if there are any alerts or items of interest.</p>

        <div  data-type="note" class="note" id="pro-tip-oAwT7"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>If everything is green, go back to doing something else :)</p>
        </div>
        </section>

        <section data-type="sect1" id="prism-element-LvVtG">
        <h2>Prism Element</h2>

        <p>Prism Element contains the following main pages:</p>

        <ul>
        	<li>Home Page
        	<ul>
        		<li>Local cluster monitoring dashboard including detailed information on alerts, capacity, performance, health, tasks, etc. &nbsp;To get further information on any of them you can click on the item of interest.</li>
        	</ul>
        	</li>
        	<li>Health Page
        	<ul>
        		<li>Environment, hardware and managed object health and state information. &nbsp;Includes NCC health check status as well.</li>
        	</ul>
        	</li>
        	<li>VM Page
        	<ul>
        		<li>Full VM management, monitoring and CRUD (Acropolis)</li>
        		<li>VM monitoring (non-Acropolis)</li>
        	</ul>
        	</li>
        	<li>Storage Page
        	<ul>
        		<li>Container management, monitoring and CRUD</li>
        	</ul>
        	</li>
        	<li>Hardware
        	<ul>
        		<li>Server, disk and network management,&nbsp;monitoring and health. &nbsp;Includes cluster expansion as well as node and disk removal.</li>
        	</ul>
        	</li>
        	<li>Data Protection
        	<ul>
        		<li>DR, Cloud Connect and Metro Availability configuration. &nbsp;Management of PD objects,&nbsp;snapshots, replication and restore.</li>
        	</ul>
        	</li>
        	<li>Analysis
        	<ul>
        		<li>Detailed performance analysis for cluster and managed objects with event correlation</li>
        	</ul>
        	</li>
        	<li>Alerts
        	<ul>
        		<li>Local cluster and environment alerts</li>
        	</ul>
        	</li>
        </ul>

        <p>The home page will provide detailed information on alerts, service status, capacity, performance, tasks, and much more. &nbsp;To get further information on any of them you can click on the item of interest.</p>

        <p>The figure shows a sample Prism Element dashboard where local cluster details are displayed:</p>

        <figure id="id-0WEtr"><img alt="" class="iimagesv2prismpe_dashboardpng" src="imagesv2/Prism/PE_dashboard.png">
        <figcaption><span class="label">Figure 5-3. </span>Prism Element - Dashboard</figcaption>
        </figure>

        <div  data-type="note" class="note" id="keyboard-shortcuts-Gl7fx"><h6>Note</h6>
        <h1>Keyboard Shortcuts</h1>

        <p>Accessibility and ease of use is a very critical construct in Prism. &nbsp;To simplify things for the end-user a set of shortcuts have been added to allow users to do everything from their keyboard.</p>

        <p>The following characterizes some of the key shortcuts:</p>

        <p>Change view (page context aware):</p>

        <ul>
        	<li>O - Overview View</li>
        	<li>D - Diagram View</li>
        	<li>T - Table View</li>
        </ul>

        <p>Activities and Events:</p>

        <ul>
        	<li>A - Alerts</li>
        	<li>P - Tasks</li>
        </ul>

        <p>Drop down and Menus (Navigate selection using arrow keys):</p>

        <ul>
        	<li>M - Menu drop-down</li>
        	<li>S - Settings (gear icon)</li>
        	<li>F - Search bar</li>
        	<li>U - User drop down</li>
        	<li>H - Help</li>
        </ul>
        </div>
        </section>
        </section>

        <section data-type="chapter" id="usage-and-troubleshooting-aQahQ">
        <h2>Usage and Troubleshooting</h2>

        <p>In the following sections we're cover some of the typical Prism uses as well as some common troubleshooting scenarios.</p>

        <section data-type="sect1" id="nutanix-software-upgrade-AvxHd">
        <h2>Nutanix Software Upgrade</h2>

        <p>Performing a Nutanix software upgrade is a very simple and non-disruptive process.</p>

        <p>To begin, start by logging into Prism and clicking on the gear icon on the top right (settings) or by pressing 'S' and selecting 'Upgrade Software':</p>

        <figure id="id-laNU1"><img alt="" class="iimagesv2prismupgradeupgrade_1png" src="imagesv2/Prism/upgrade/upgrade_1.png">
        <figcaption><span class="label">Figure 6-1. </span>Prism - Settings - Upgrade Software</figcaption>
        </figure>

        <p>This will launch the 'Upgrade Software' dialog box and will show your current software version and if there are any upgrade versions available. &nbsp;It is also possible to manually upload a NOS binary file.</p>

        <p>You can then download the upgrade version from the cloud or upload the version manually:</p>

        <figure id="id-dzetd"><img alt="" class="iimagesv2prismupgradeupgrade_2png" src="imagesv2/Prism/upgrade/upgrade_2.png">
        <figcaption><span class="label">Figure 6-2. </span>Upgrade Software - Main</figcaption>
        </figure>

        <p>It will then upload the upgrade software onto the Nutanix CVMs:</p>

        <figure id="id-ErWFP"><img alt="" class="iimagesv2prismupgradeupgrade_3png" src="imagesv2/Prism/upgrade/upgrade_3.png">
        <figcaption><span class="label">Figure 6-3. </span>Upgrade Software - Upload</figcaption>
        </figure>

        <p>After the software is loaded click on 'Upgrade' to start the upgrade process:</p>

        <figure id="id-aQYC8"><img alt="" class="iimagesv2prismupgradeupgrade_4png" src="imagesv2/Prism/upgrade/upgrade_4.png">
        <figcaption><span class="label">Figure 6-4. </span>Upgrade Software - Upgrade Validation</figcaption>
        </figure>

        <p>You'll then be prompted with a confirmation box:</p>

        <figure id="id-Wz9FZ"><img alt="" class="iimagesv2prismupgradeupgrade_5png" src="imagesv2/Prism/upgrade/upgrade_5.png">
        <figcaption><span class="label">Figure 6-5. </span>Upgrade Software - Confirm Upgrade</figcaption>
        </figure>

        <p>The upgrade will start with pre-upgrade checks then start upgrading the software in a rolling manner:</p>

        <figure id="id-kq4Sp"><img alt="" class="iimagesv2prismupgradeupgrade_6png" src="imagesv2/Prism/upgrade/upgrade_6.png">
        <figcaption><span class="label">Figure 6-6. </span>Upgrade Software - Execution</figcaption>
        </figure>

        <p>Once the upgrade is complete you'll see an updated status and have access to all of the new features:</p>

        <figure id="id-ZZMfY"><img alt="" class="iimagesv2prismupgradeupgrade_7png" src="imagesv2/Prism/upgrade/upgrade_7.png">
        <figcaption><span class="label">Figure 6-7. </span>Upgrade Software - Complete</figcaption>
        </figure>

        <div  data-type="note" class="note" id="note-w04hd"><h6>Note</h6>
        <h1>Note</h1>

        <p>Your Prism session will briefly disconnect during the upgrade when the current Prism Leader is upgraded. &nbsp;All VMs and services running remain unaffected.</p>
        </div>
        </section>

        <section data-type="sect1" id="hypervisor-upgrade-MBpTq">
        <h2>Hypervisor Upgrade</h2>

        <p>Similar to Nutanix software upgrades, hypervisor upgrades can be fully automated in a rolling manner via Prism.</p>

        <p>To begin follow the similar steps above to launch the 'Upgrade Software' dialogue box and select 'Hypervisor'.</p>

        <p>You can then download the hypervisor upgrade version from the cloud or upload the version manually:</p>

        <figure id="id-28LI4"><img alt="" class="iimagesv2prismupgradehyp_upgrade_1png" src="imagesv2/Prism/upgrade/hyp_upgrade_1.png">
        <figcaption><span class="label">Figure 6-8. </span>Upgrade Hypervisor - Main</figcaption>
        </figure>

        <p>It will then load the upgrade software onto the Hypervisors. &nbsp;After the software is loaded click on 'Upgrade' to start the upgrade process:</p>

        <figure id="id-qmrUD"><img alt="" class="iimagesv2prismupgradehyp_upgrade_2png" src="imagesv2/Prism/upgrade/hyp_upgrade_2.png">
        <figcaption><span class="label">Figure 6-9. </span>Upgrade Hypervisor - Upgrade Validation</figcaption>
        </figure>

        <p>You'll then be prompted with a confirmation box:</p>

        <figure id="id-lxNI1"><img alt="" class="iimagesv2prismupgradehyp_upgrade_3png" src="imagesv2/Prism/upgrade/hyp_upgrade_3.png">
        <figcaption><span class="label">Figure 6-10. </span>Upgrade Hypervisor - Confirm Upgrade</figcaption>
        </figure>

        <p>The system will then go through host pre-upgrade checks and upload the hypervisor upgrade to the cluster:</p>

        <figure id="id-mD2hy"><img alt="" class="iimagesv2prismupgradehyp_upgrade_4png" src="imagesv2/Prism/upgrade/hyp_upgrade_4.png">
        <figcaption><span class="label">Figure 6-11. </span>Upgrade Hypervisor - Pre-upgrade Checks</figcaption>
        </figure>

        <p>Once the pre-upgrade checks are complete the rolling hypervisor upgrade will then proceed:</p>

        <figure id="id-qmaiD"><img alt="" class="iimagesv2prismupgradehyp_upgrade_5png" src="imagesv2/Prism/upgrade/hyp_upgrade_5.png">
        <figcaption><span class="label">Figure 6-12. </span>Upgrade Hypervisor - Execution</figcaption>
        </figure>

        <p>Similar to the rolling nature of the Nutanix software upgrades, each host will be upgraded in a rolling manner with zero impact to running VMs. &nbsp;VMs will be live-migrated off the current host, the host will be upgraded, and then rebooted. &nbsp;This process will iterate through each host until all hosts in the cluster are upgraded.</p>

        <div  data-type="note" class="note" id="pro-tip-NMZuz"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>You can also get cluster wide upgrade status from any Nutanix CVM by running 'host_upgrade --status'. &nbsp;The detailed per host status is logged to ~/data/logs/host_upgrade.out on each CVM.</p>
        </div>

        <p>Once the upgrade is complete you'll see an updated status and have access to all of the new features:</p>

        <figure id="id-mDghy"><img alt="" class="iimagesv2prismupgradehyp_upgrade_6png" src="imagesv2/Prism/upgrade/hyp_upgrade_6.png">
        <figcaption><span class="label">Figure 6-13. </span>Upgrade Hypervisor - Complete</figcaption>
        </figure>

        <p>&nbsp;</p>
        </section>

        <section data-type="sect1" id="cluster-expansion-add-node-VGvSL">
        <h2>Cluster Expansion (add node)</h2>

        <p>Coming soon!</p>
        </section>

        <section data-type="sect1" id="capacity-planning-x8nTv">
        <h2>Capacity Planning</h2>

        <p>To get detailed capacity planning details you can click on a specific cluster under the 'cluster runway' section in Prism Central to get more details:</p>

        <figure id="id-0QqSr"><img alt="" class="iimagesv2prismpc_capplannerpng" src="imagesv2/Prism/pc_capplanner.png">
        <figcaption><span class="label">Figure 6-14. </span>Prism Central - Capacity Planning</figcaption>
        </figure>

        <p>This view provides detailed information on cluster runway and identifies the most constrained resource (limiting resource). &nbsp;You can also get detailed information on what the top consumers are as well as some potential options to clean up additional capacity or ideal node types for cluster expansion.</p>

        <figure id="id-e08I0"><img alt="" class="iimagesv2prismpc_recommendationpng" src="imagesv2/Prism/pc_recommendation.png">
        <figcaption><span class="label">Figure 6-15. </span>Prism Central - Capacity Planning - Recommendations</figcaption>
        </figure>
        </section>

        <p>The HTML5 UI is a key part to Prism to provide a simple, easy to use management interface. &nbsp;However, another core ability are the APIs which are available for automation. &nbsp;All functionality exposed through the Prism UI is also exposed through a full set of REST APIs to allow for the ability to programmatically interface with the Nutanix platform. &nbsp;This allow customers and partners to enable automation, 3rd-party tools, or even create their own UI. &nbsp;</p>

        <p>The following section covers these interfaces and provides some example usage.</p>
        </section>

        <section data-type="chapter" id="apis-and-interfaces-v8WtB">
        <h2>APIs and Interfaces</h2>

        <p>Core to any dynamic or “software-defined” environment, Nutanix provides a vast array of interfaces allowing for simple programability and interfacing. Here are the main interfaces:</p>

        <ul>
        	<li>REST API</li>
        	<li>CLI - ACLI &amp; NCLI</li>
        	<li>Scripting interfaces</li>
        </ul>

        <p>Core to this is the REST API which exposes every capability and data point of the Prism UI and allows for orchestration or automation tools to easily drive Nutanix action.&nbsp; This enables tools like vRealize Operations, System Center Orchestrator, Ansible, SALT, etc.&nbsp;to easily create custom workflows for Nutanix. Also, this means that any third-party developer could create their own custom UI and pull in Nutanix data via REST.</p>

        <p>The following figure shows a small snippet of the Nutanix REST API explorer which allows developers to interact with the API and see expected data formats:</p>

        <figure id="id-P1zHm"><img alt="" class="iimagesv2restapipng" src="imagesv2/RestAPI.png">
        <figcaption><span class="label">Figure 7-1. </span>REST API Explorer</figcaption>
        </figure>

        <p>Operations can be expanded to display details and examples of the REST call:</p>

        <figure id="id-2mpu4"><img alt="" class="iimagesv2restapi2png" src="imagesv2/RestAPI2.png">
        <figcaption><span class="label">Figure 7-2. </span>REST API Sample Call</figcaption>
        </figure>

        <div  data-type="note" class="note" id="api-authentication-schemes-1OAcG"><h6>Note</h6>
        <h1>API Authentication Scheme(s)</h1>

        <p>As of 4.5.x basic authentication over HTTPS is leveraged for client and HTTP call authentication.</p>
        </div>

        <section data-type="sect1" id="acli-e0acX">
        <h2>ACLI</h2>

        <p>The Acropolis CLI (ACLI) is the CLI for managing the Acropolis portion of the Nutanix product. &nbsp;These capabilities were enabled in releases after 4.1.2.</p>

        <p>NOTE: All of these actions can be performed via the HTML5 GUI and REST API.&nbsp; I just use these commands as part of my scripting to automate tasks.</p>

        <h3>Enter ACLI shell</h3>

        <p class="codedescription">Description: Enter ACLI shell (run from any CVM)</p>

        <p class="codetext">Acli</p>

        <p>OR</p>

         <p class="codedescription">Description: Execute ACLI command via Linux shell</p>

        <p class="codetext">ACLI &lt;Command&gt;</p>

        <h3>Output ACLI response in json format</h3>

         <p class="codedescription">Description: Lists Acropolis nodes in the cluster.</p>

        <p class="codetext">Acli –o json</p>

        <h3>List hosts</h3>

         <p class="codedescription">Description: Lists Acropolis nodes in the cluster.</p>

        <p class="codetext">host.list</p>

        <h3>Create network</h3>

         <p class="codedescription">Description: Create network based on VLAN</p>

        <p class="codetext">net.create &lt;TYPE&gt;.&lt;ID&gt;[.&lt;VSWITCH&gt;] ip_config=&lt;A.B.C.D&gt;/&lt;NN&gt;</p>

        <p class="codetext">Example: net.create vlan.133 ip_config=10.1.1.1/24</p>

        <h3>List network(s)</h3>

         <p class="codedescription">Description: List networks</p>

        <p class="codetext">net.list</p>

        <h3>Create DHCP scope</h3>

         <p class="codedescription">Description: Create dhcp scope</p>

        <p class="codetext">net.add_dhcp_pool &lt;NET NAME&gt; start=&lt;START IP A.B.C.D&gt; end=&lt;END IP W.X.Y.Z&gt;</p>

        <p class="note">Note: .254 is reserved and used by the Acropolis DHCP server if an address for the Acropolis DHCP server wasn’t set during network creation</p>

        <p class="codetext">Example: net.add_dhcp_pool vlan.100 start=10.1.1.100 end=10.1.1.200</p>

        <h3>Get an existing networks details</h3>

         <p class="codedescription">Description: Get a network's properties</p>

        <p class="codetext">net.get &lt;NET NAME&gt;</p>

        <p class="codetext">Example: net.get vlan.133</p>

        <h3>Get an existing networks details</h3>

         <p class="codedescription">Description: Get a network's VMs and details including VM name / UUID, MAC address and IP</p>

        <p class="codetext">net.list_vms &lt;NET NAME&gt;</p>

        <p class="codetext">Example: net.list_vms vlan.133</p>

        <h3>Configure DHCP DNS servers for network</h3>

         <p class="codedescription">Description: Set DHCP DNS</p>

        <p class="codetext">net.update_dhcp_dns &lt;NET NAME&gt; servers=&lt;COMMA SEPARATED DNS IPs&gt; domains=&lt;COMMA SEPARATED DOMAINS&gt;</p>

        <p class="codetext">Example: net.set_dhcp_dns vlan.100 servers=10.1.1.1,10.1.1.2 domains=splab.com</p>

        <h3>Create Virtual Machine</h3>

         <p class="codedescription">Description: Create VM</p>

        <p class="codetext">vm.create &lt;COMMA SEPARATED VM NAMES&gt; memory=&lt;NUM MEM MB&gt; num_vcpus=&lt;NUM VCPU&gt; num_cores_per_vcpu=&lt;NUM CORES&gt; ha_priority=&lt;PRIORITY INT&gt;</p>

        <p class="codetext">Example: vm.create testVM memory=2G num_vcpus=2</p>

        <h3>Bulk Create Virtual Machine</h3>

         <p class="codedescription">Description: Create bulk VM</p>

        <p class="codetext">vm.create &nbsp;&lt;CLONE PREFIX&gt;[&lt;STARTING INT&gt;..&lt;END INT&gt;]&nbsp;memory=&lt;NUM MEM MB&gt; num_vcpus=&lt;NUM VCPU&gt; num_cores_per_vcpu=&lt;NUM CORES&gt; ha_priority=&lt;PRIORITY INT&gt;</p>

        <p class="codetext">Example: vm.create testVM[000..999]&nbsp;memory=2G num_vcpus=2</p>

        <h3>Clone VM from existing</h3>

         <p class="codedescription">Description: Create clone of existing VM</p>

        <p class="codetext">vm.clone &lt;CLONE NAME(S)&gt; clone_from_vm=&lt;SOURCE VM NAME&gt;</p>

        <p class="codetext">Example: vm.clone testClone clone_from_vm=MYBASEVM</p>

        <h3>Bulk Clone VM from existing</h3>

         <p class="codedescription">Description: Create bulk clones of existing VM</p>

        <p class="codetext">vm.clone &lt;CLONE PREFIX&gt;[&lt;STARTING INT&gt;..&lt;END INT&gt;] clone_from_vm=&lt;SOURCE VM NAME&gt;</p>

        <p class="codetext">Example: vm.clone testClone[001..999]&nbsp;clone_from_vm=MYBASEVM</p>

        <h3>Create disk and add to VM</h3>

        <p class="codetext"># Description: Create disk for OS</p>

        <p class="codetext">vm.disk_create &lt;VM NAME&gt; create_size=&lt;Size and qualifier, e.g. 500G&gt; container=&lt;CONTAINER NAME&gt;</p>

        <p class="codetext"> class="codetext"Example: vm.disk_create testVM create_size=500G container=default</p>

        <h3>Add NIC to VM</h3>

         <p class="codedescription">Description: Create and add NIC</p>

        <p class="codetext">vm.nic_create &lt;VM NAME&gt; network=&lt;NETWORK NAME&gt; model=&lt;MODEL&gt;</p>

        <p class="codetext">Example: vm.nic_create testVM network=vlan.100</p>

        <h3>Set VM’s boot device to disk</h3>

         <p class="codedescription">Description: Set a VM boot device</p>

        <p>Set to boot form specific disk id</p>

        <p class="codetext">vm.update_boot_device &lt;VM NAME&gt; disk_addr=&lt;DISK BUS&gt;</p>

        <p class="codetext">Example: vm.update_boot_device testVM disk_addr=scsi.0</p>

        <h3>Set VM’s boot device to CDrom</h3>

        <p>Set to boot from CDrom</p>

        <p class="codetext">vm.update_boot_device &lt;VM NAME&gt; disk_addr=&lt;CDROM BUS&gt;</p>

        <p class="codetext">Example: vm.update_boot_device testVM disk_addr=ide.0</p>

        <h3>Mount ISO to CDrom</h3>

         <p class="codedescription">Description: Mount ISO to VM cdrom</p>

        <p>Steps:</p>

        <p>1. Upload ISOs to container</p>

        <p>2. Enable whitelist for client IPs</p>

        <p>3. Upload ISOs to share</p>

        <p>Create CDrom with ISO</p>

        <p class="codetext">vm.disk_create &lt;VM NAME&gt; clone_nfs_file=&lt;PATH TO ISO&gt; cdrom=true</p>

        <p class="codetext">Example: vm.disk_create testVM clone_nfs_file=/default/ISOs/myfile.iso cdrom=true</p>

        <p>If a CDrom is already created just mount it</p>

        <p class="codetext">vm.disk_update &lt;VM NAME&gt; &lt;CDROM BUS&gt; clone_nfs_file&lt;PATH TO ISO&gt;</p>

        <p class="codetext">Example: vm.disk_update atestVM1 ide.0 clone_nfs_file=/default/ISOs/myfile.iso</p>

        <h3>Detach ISO from CDrom</h3>

         <p class="codedescription">Description: Remove ISO from CDrom</p>

        <p class="codetext">vm.disk_update &lt;VM NAME&gt; &lt;CDROM BUS&gt; empty=true</p>

        <h3>Power on VM(s)</h3>

         <p class="codedescription">Description: Power on VM(s)</p>

        <p class="codetext">vm.on &lt;VM NAME(S)&gt;</p>

        <p class="codetext">Example: vm.on testVM</p>

        <p>Power on all VMs</p>

        <p class="codetext">Example: vm.on *</p>

        <p>Power on range of VMs</p>

        <p class="codetext">Example: vm.on testVM[01..99]</p>
        </section>

        <section data-type="sect1" id="ncli-0ozUK">
        <h2>NCLI</h2>

        <p>NOTE: All of these actions can be performed via the HTML5 GUI and REST API.&nbsp; I just use these commands as part of my scripting to automate tasks.</p>

        <h3>Add subnet to NFS whitelist</h3>

         <p class="codedescription">Description: Adds a particular subnet to the NFS whitelist</p>

        <p class="codetext">ncli cluster add-to-nfs-whitelist ip-subnet-masks=10.2.0.0/255.255.0.0</p>

        <h3>Display Nutanix Version</h3>

         <p class="codedescription">Description: Displays the current version of the Nutanix software</p>

        <p class="codetext">ncli cluster version</p>

        <h3>Display hidden NCLI options</h3>

         <p class="codedescription">Description: Displays the hidden ncli commands/options</p>

        <p class="codetext">ncli helpsys listall hidden=true [detailed=false|true]</p>

        <h3>List Storage Pools</h3>

         <p class="codedescription">Description: Displays the existing storage pools</p>

        <p class="codetext">ncli sp ls</p>

        <h3>List containers</h3>

         <p class="codedescription">Description: Displays the existing containers</p>

        <p class="codetext">ncli ctr ls</p>

        <h3>Create container</h3>

        <p class="codedescription">Description: Creates a new container</p>

        <p class="codetext">ncli ctr create name=&lt;NAME&gt; sp-name=&lt;SP NAME&gt;</p>

        <h3>List VMs</h3>

         <p class="codedescription">Description: Displays the existing VMs</p>

        <p class="codetext">ncli vm ls</p>

        <h3>List public keys</h3>

         <p class="codedescription">Description: Displays the existing public keys</p>

        <p class="codetext">ncli cluster list-public-keys</p>

        <h3>Add public key</h3>

         <p class="codedescription">Description: Adds a public key for cluster access</p>

        <p>SCP private key to CVM</p>

        <p>Add private key to cluster</p>

        <p class="codetext">ncli cluster add-public-key name=myPK file-path=~/mykey.pub</p>

        <h3>Remove public key</h3>

         <p class="codedescription">Description: Removes a public key for cluster access</p>

        <p class="codetext">ncli cluster remove-public-keys name=myPK</p>

        <h3>Create protection domain</h3>

         <p class="codedescription">Description: Creates a protection domain</p>

        <p class="codetext">ncli pd create name=&lt;NAME&gt;</p>

        <h3>Create remote site</h3>

         <p class="codedescription">Description: Create a remote site for replication</p>

        <p class="codetext">ncli remote-site create name=&lt;NAME&gt; address-list=&lt;Remote Cluster IP&gt;</p>

        <h3>Create protection domain for all VMs in container</h3>

         <p class="codedescription">Description: Protect all VMs in the specified container</p>

        <p class="codetext">ncli pd protect name=&lt;PD NAME&gt; ctr-id=&lt;Container ID&gt; cg-name=&lt;NAME&gt;</p>

        <h3>Create protection domain with specified VMs</h3>

         <p class="codedescription">Description: Protect the VMs specified</p>

        <p class="codetext">ncli pd protect name=&lt;PD NAME&gt; vm-names=&lt;VM Name(s)&gt; cg-name=&lt;NAME&gt;</p>

        <h3>Create protection domain for DSF files (aka vDisk)</h3>

         <p class="codedescription">Description: Protect the DSF Files specified</p>

        <p class="codetext">ncli pd protect name=&lt;PD NAME&gt; files=&lt;File Name(s)&gt; cg-name=&lt;NAME&gt;</p>

        <h3>Create snapshot of protection domain</h3>

         <p class="codedescription">Description: Create a one-time snapshot of the protection domain</p>

        <p class="codetext">ncli pd add-one-time-snapshot name=&lt;PD NAME&gt; retention-time=&lt;seconds&gt;</p>

        <h3>Create snapshot and replication schedule to remote site</h3>

         <p class="codedescription">Description: Create a recurring snapshot schedule and replication to n remote sites</p>

        <p class="codetext">ncli pd set-schedule name=&lt;PD NAME&gt; interval=&lt;seconds&gt; retention-policy=&lt;POLICY&gt; remote-sites=&lt;REMOTE SITE NAME&gt;</p>

        <h3>List replication status</h3>

         <p class="codedescription">Description: Monitor replication status</p>

        <p class="codetext">ncli pd list-replication-status</p>

        <h3>Migrate protection domain to remote site</h3>

         <p class="codedescription">Description: Fail-over a protection domain to a remote site</p>

        <p class="codetext">ncli pd migrate name=&lt;PD NAME&gt; remote-site=&lt;REMOTE SITE NAME&gt;</p>

        <h3>Activate protection domain</h3>

         <p class="codedescription">Description: Activate a protection domain at a remote site</p>

        <p class="codetext">ncli pd activate name=&lt;PD NAME&gt;</p>

        <h3>Enable DSF Shadow Clones</h3>

         <p class="codedescription">Description: Enables the DSF Shadow Clone feature</p>

        <p class="codetext">ncli cluster edit-params enable-shadow-clones=true</p>

        <h3>Enable Dedup for vDisk</h3>

         <p class="codedescription">Description: Enables fingerprinting and/or on disk dedup for a specific vDisk</p>

        <p class="codetext">ncli vdisk edit name=&lt;VDISK NAME&gt; fingerprint-on-write=&lt;true/false&gt; on-disk-dedup=&lt;true/false&gt;</p>
        </section>

        <section data-type="sect1" id="powershell-cmdlets-r2kfb">
        <h2>PowerShell CMDlets</h2>

        <p>The below will cover the Nutanix PowerShell CMDlets, how to use them and some general background on Windows PowerShell.</p>

        <h3>Basics</h3>

        <p>Windows PowerShell is a powerful shell (hence the name ;P) and scripting language built on the .NET framework.&nbsp; It is a very simple to use language and is built to be intuitive and interactive.&nbsp; Within PowerShell there are a few key constructs/Items:</p>

        <h3>CMDlets</h3>

        <p>CMDlets are commands or .NET classes which perform a particular operation.&nbsp; They are usually conformed to the Getter/Setter methodology and typically use a &lt;Verb&gt;-&lt;Noun&gt; based structure.&nbsp; For example: Get-Process, Set-Partition, etc.</p>

        <h3>Piping or Pipelining</h3>

        <p>Piping is an important construct in PowerShell (similar to its use in Linux) and can greatly simplify things when used correctly.&nbsp; With piping you’re essentially taking the output of one section of the pipeline and using that as input to the next section of the pipeline.&nbsp; The pipeline can be as long as required (assuming there remains output which is being fed to the next section of the pipe). A very simple example could be getting the current processes, finding those that match a particular trait or filter and then sorting them:</p>

        <p class="codetext">Get-Service | where {$_.Status -eq "Running"} | Sort-Object Name</p>

        <p>Piping can also be used in place of for-each, for example:</p>

        <p class="codetext"># For each item in my array
        <br/>
        $myArray | %{
        <br/>
        &nbsp; # Do something
        <br/>
        }</p>

        <h3>Key Object Types</h3>

        <p>Below are a few of the key object types in PowerShell.&nbsp; You can easily get the object type by using the .getType() method, for example: $someVariable.getType() will return the objects type.</p>

        <h3>Variable</h3>

        <p class="codetext">$myVariable = "foo"</p>

        <p class="note">Note: You can also set a variable to the output of a series or pipeline of commands:</p>

        <p class="codetext">$myVar2 = (Get-Process | where {$_.Status -eq "Running})</p>

        <p>In this example the commands inside the parentheses will be evaluated first then variable will be the outcome of that.</p>

        <h3>Array</h3>

        <p class="codetext">$myArray = @("Value","Value")</p>

        <p class="note">Note: You can also have an array of arrays, hash tables or custom objects</p>

        <h3>Hash Table</h3>

        <p class="codetext">$myHash = @{"Key" = "Value";"Key" = "Value"}</p>

        <h3>Useful commands</h3>

        <p>Get the help content for a particular CMDlet (similar to a man page in Linux)</p>

        <p class="codetext">Get-Help &lt;CMDlet Name&gt;</p>

        <p class="codetext">Example: Get-Help Get-Process</p>

        <p>List properties and methods of a command or object</p>

        <p class="codetext">&lt;Some expression or object&gt; | Get-Member</p>

        <p class="codetext">Example: $someObject | Get-Member</p>

        <h3>Core Nutanix CMDlets and Usage</h3>

        <p>Download Nutanix CMDlets Installer The Nutanix CMDlets can be downloaded directly from the Prism UI (post 4.0.1) and can be found on the drop down in the upper right hand corner:</p>

        <figure id="id-nzVfk"><img alt="" class="iimagesv2cmdlets_dlpng" src="imagesv2/cmdlets_dl.png">
        <figcaption><span class="label">Figure 7-3. </span>Prism CMDlets Installer Link</figcaption>
        </figure>

        <h3>Load Nutanix Snappin</h3>

        <p>Check if snappin is loaded and if not, load</p>

        <p class="codetext">if ( (Get-PSSnapin -Name NutanixCmdletsPSSnapin -ErrorAction SilentlyContinue) -eq $null )
        <br/>
        {
        <br/>
        &nbsp;&nbsp;&nbsp; Add-PsSnapin NutanixCmdletsPSSnapin
        <br/>
        }</p>

        <h3>List Nutanix CMDlets</h3>

        <p class="codetext">Get-Command | Where-Object{$_.PSSnapin.Name -eq "NutanixCmdletsPSSnapin"}</p>

        <h3>Connect to a Nutanix Cluster</h3>

        <p class="codetext">Connect-NutanixCluster -Server $server -UserName "myuser" -Password "myuser" -AcceptInvalidSSLCerts</p>

        <p>Or secure way prompting user for password</p>

        <p class="codetext">Connect-NutanixCluster -Server $server -UserName "myuser" -Password (Read-Host "Password: ") -AcceptInvalidSSLCerts</p>

        <h3>Get Nutanix VMs matching a certain search string</h3>

        <p>Set to variable</p>

        <p class="codetext">$searchString = "myVM"
        <br/>
        $vms = Get-NTNXVM | where {$_.vmName -match $searchString}</p>

        <p>Interactive</p>

        <p class="codetext">Get-NTNXVM | where {$_.vmName -match "myString"}</p>

        <p>Interactive and formatted</p>

        <p class="codetext">Get-NTNXVM | where {$_.vmName -match "myString"} | ft</p>

        <h3>Get Nutanix vDisks</h3>

        <p>Set to variable</p>

        <p class="codetext">$vdisks = Get-NTNXVDisk</p>

        <p>Interactive</p>

        <p class="codetext">Get-NTNXVDisk</p>

        <p>Interactive and formatted</p>

        <p class="codetext">Get-NTNXVDisk | ft</p>

        <h3>Get Nutanix Containers</h3>

        <p>Set to variable</p>

        <p class="codetext">$containers = Get-NTNXContainer</p>

        <p>Interactive</p>

        <p class="codetext">Get-NTNXContainer</p>

        <p>Interactive and formatted</p>

        <p class="codetext">Get-NTNXContainer | ft</p>

        <h3>Get Nutanix Protection Domains</h3>

        <p>Set to variable</p>

        <p class="codetext">$pds = Get-NTNXProtectionDomain</p>

        <p>Interactive</p>

        <p class="codetext">Get-NTNXProtectionDomain</p>

        <p>Interactive and formatted</p>

        <p class="codetext">Get-NTNXProtectionDomain | ft</p>

        <h3>Get Nutanix Consistency Groups</h3>

        <p>Set to variable</p>

        <p class="codetext">$cgs = Get-NTNXProtectionDomainConsistencyGroup</p>

        <p>Interactive</p>

        <p class="codetext">Get-NTNXProtectionDomainConsistencyGroup</p>

        <p>Interactive and formatted</p>

        <p class="codetext">Get-NTNXProtectionDomainConsistencyGroup | ft</p>

        <h3>Resources and Scripts:</h3>

        <ul>
        	<li>Nutanix Github - <a href="https://github.com/nutanix/Automation" target="_blank">https://github.com/nutanix/Automation</a></li>
        	<li>Manually Fingerprint vDisks - <a href="http://bit.ly/1syOqch" target="_blank">http://bit.ly/1syOqch</a></li>
        	<li>vDisk Report - <a href="http://bit.ly/1r34MIT" target="_blank">http://bit.ly/1r34MIT</a></li>
        	<li>Protection Domain Report - <a href="http://bit.ly/1r34MIT" target="_blank">http://bit.ly/1r34MIT</a></li>
        	<li>Ordered PD Restore - <a href="http://bit.ly/1pyolrb" target="_blank">http://bit.ly/1pyolrb</a></li>
        </ul>

        <p>You can find more scripts on the Nutanix Github located at https://github.com/nutanix</p>
        </section>
        </section>
        </div>

        <div data-type="part" id="book-of-acropolis-6YeSv">
        <h1><span class="label">Part III. </span>Book of Acropolis</h1>

        <p class="definition"><strong>a·crop·o·lis  -  /ɘ ' kräpɘlis/  -  noun  -  data plane</strong>
        <br/>
        storage, compute and virtualization platform.
        </p>


        <section data-type="chapter" id="architecture-59Gug">
        <h2>Architecture</h2>

        <p>Acropolis is a distributed multi-resource manager,&nbsp;orchestration platform and data plane.</p>

        <p>It is broken down into three main components:</p>

        <ul>
        	<li>Distributed Storage Fabric (DSF)
        	<ul>
        		<li>This is at the core and birth of the Nutanix platform and expands upon the Nutanix Distributed Filesystem (NDFS).&nbsp; NDFS has now evolved from a distributed system pooling storage resources into a much larger and capable storage platform.</li>
        	</ul>
        	</li>
        	<li>App Mobility Fabric (AMF)
        	<ul>
        		<li>Hypervisors abstracted the OS from hardware, and the AMF abstracts workloads (VMs, Storage, Containers, etc.) from the hypervisor.&nbsp; This will provide the ability to dynamically move the workloads between hypervisors, clouds, as well as provide the ability for Nutanix nodes to change hypervisors.</li>
        	</ul>
        	</li>
        	<li>Hypervisor
        	<ul>
        		<li>A multi-purpose hypervisor based upon the CentOS KVM hypervisor.</li>
        	</ul>
        	</li>
        </ul>

        <p>Building upon the distributed nature of everything Nutanix does, we’re expanding this into the virtualization and resource management space.&nbsp; Acropolis is a back-end service that allows for workload and resource management, provisioning, and operations.&nbsp; Its goal is to abstract the facilitating resource (e.g., hypervisor, on-premise, cloud, etc.) from the workloads running, while providing a single “platform” to operate.&nbsp;</p>

        <p>This gives workloads the ability to seamlessly move between hypervisors, cloud providers, and platforms.</p>

        <p>The figure highlights an image illustrating the conceptual nature of Acropolis at various layers:</p>

        <figure id="id-Rkdsp"><img alt="" class="iimagesv2arch_acropolispng" src="imagesv2/arch_acropolis.png">
        <figcaption><span class="label">Figure 4-1. </span>High-level Acropolis Architecture</figcaption>
        </figure>

        <div  data-type="note" class="note" id="supported-hypervisors-for-vm-management-RzaSN"><h6>Note</h6>
        <h1>Supported Hypervisors for VM Management</h1>

        <p>Currently, the only fully supported hypervisor for VM management is Acropolis Hypervisor, however this may expand in the future. &nbsp;The Volumes API and read-only operations are still supported on all.</p>
        </div>

        <section data-type="sect1" id="acropolis-services-68Pt2">
        <h2>Acropolis Services</h2>

        <p>An Acropolis Slave runs on every CVM with an elected Acropolis Master which is responsible for task scheduling, execution, IPAM, etc.&nbsp; Similar to other components which have a Master, if the Acropolis Master fails, a new one will be elected.</p>

        <p>The role breakdown for each can be seen below:</p>

        <ul>
        	<li>Acropolis Master
        	<ul>
        		<li>Task scheduling &amp; execution</li>
        		<li>Stat collection / publishing</li>
        		<li>Network Controller (for hypervisor)</li>
        		<li>VNC proxy (for hypervisor)</li>
        		<li>HA (for hypervisor)</li>
        	</ul>
        	</li>
        	<li>&nbsp;Acropolis Slave
        	<ul>
        		<li>Stat collection / publishing</li>
        		<li>VNC proxy (for hypervisor)</li>
        	</ul>
        	</li>
        </ul>

        <p>Here we show a conceptual view of the Acropolis Master / Slave relationship:</p>

        <figure id="id-lMxc1"><img alt="" class="iimagesv2acrop_componentspng image" src="imagesv2/acrop_components.png">
        <figcaption><span class="label">Figure 4-2. </span>Acropolis Services</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="converged-platform-JJlUl">
        <h2>Converged Platform</h2>

        <p>For a video explanation you can watch the following video: <a href="https://youtu.be/OPYA5-V0yRo">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//OPYA5-V0yRo"></iframe></div>

        <p>The Nutanix solution is a converged storage + compute solution which leverages local components and creates a distributed platform for virtualization, also known as a virtual computing platform. The solution is a bundled hardware + software appliance which houses 2 (6000/7000 series) or 4 nodes (1000/2000/3000/3050 series) in a 2U footprint.</p>

        <p>Each node runs an industry-standard hypervisor (ESXi, KVM, Hyper-V currently) and the Nutanix Controller VM (CVM).&nbsp; The Nutanix CVM is what runs the Nutanix software and serves all of the I/O operations for the hypervisor and all VMs running on that host.&nbsp; For the Nutanix units running VMware vSphere, the SCSI controller, which manages the SSD and HDD devices, is directly passed to the CVM leveraging VM-Direct Path (Intel VT-d).&nbsp; In the case of Hyper-V, the storage devices are passed through to the CVM.</p>

        <p>The following figure provides an example of what a typical node logically looks like:</p>

        <figure id="id-KpmhQ"><img alt="" class="iimagesv2converged_platformpng" src="imagesv2/converged_platform.png">
        <figcaption><span class="label">Figure 4-3. </span>Converged Platform</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="software-defined-jEXiJ">
        <h2>Software-Defined</h2>

        <p>As mentioned above (likely numerous times), the Nutanix platform is a software-based solution which ships as a bundled software + hardware appliance.&nbsp; The controller VM is where the vast majority of the Nutanix software and logic sits and was designed from the beginning to be an extensible and pluggable architecture. A key benefit to being software-defined and not relying upon any hardware offloads or constructs is around extensibility.&nbsp; As with any product life cycle,&nbsp;advancements and new features will always be introduced.&nbsp;</p>

        <p>By not relying on any custom ASIC/FPGA or hardware capabilities, Nutanix can develop and deploy these new features through a simple software update.&nbsp; This means that the deployment of a new feature (e.g., deduplication) can be deployed by upgrading the current version of the Nutanix software.&nbsp; This also allows newer generation features to be deployed on legacy hardware models. For example, say you’re running a workload running an older version of Nutanix software on a prior generation hardware platform (e.g., 2400).&nbsp; The running software version doesn’t provide deduplication capabilities which your workload could benefit greatly from.&nbsp; To get these features, you perform a rolling upgrade of the Nutanix software version while the workload is running, and you now have deduplication.&nbsp; It’s really that easy.</p>

        <p>Similar to features, the ability to create new “adapters” or interfaces into DSF is another key capability.&nbsp; When the product first shipped, it solely supported iSCSI for I/O from the hypervisor, this has now grown to include NFS and SMB.&nbsp; In the future, there is the ability to create new adapters for various workloads and hypervisors (HDFS, etc.).&nbsp; And again, all of this can be deployed via a software update. This is contrary to most legacy infrastructures, where a hardware upgrade or software purchase is normally required to get the “latest and greatest” features.&nbsp; With Nutanix, it’s different. Since all features are deployed in software, they can run on any hardware platform, any hypervisor, and be deployed through simple software upgrades.</p>

        <p>The following figure shows a logical representation of what this software-defined controller framework looks like:</p>

        <figure id="id-kDdFp"><img alt="" class="iimagesv2software_defined_controllerpng" src="imagesv2/software_defined_controller.png">
        <figcaption><span class="label">Figure 4-4. </span>Software-Defined Controller Framework</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="cluster-components-lMlIe">
        <h2>Cluster Components</h2>

        <p>For a visual explanation you can watch the following video:&nbsp;<a href="https://youtu.be/3v5RI_IbfV4">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//3v5RI_IbfV4"></iframe></div>

        <p>The Nutanix platform is composed of the following high-level components:</p>

        <figure id="id-xJrTD"><img alt="" class="iimagesv2cluster_componentspng" src="imagesv2/cluster_components.png">
        <figcaption><span class="label">Figure 4-5. </span>Cluster Components</figcaption>
        </figure>

        <h3>Cassandra</h3>

        <ul>
        	<li>Key Role: Distributed metadata store</li>
        	<li>Description: Cassandra stores and manages all of the cluster metadata in a distributed ring-like manner based upon a heavily modified Apache Cassandra.&nbsp; The Paxos algorithm is utilized to enforce strict consistency.&nbsp; This service runs on every node in the cluster.&nbsp; Cassandra is accessed via an interface called Medusa.</li>
        </ul>

        <h3>Zookeeper</h3>

        <ul>
        	<li>Key Role: Cluster configuration manager</li>
        	<li>Description: Zeus stores all of the cluster configuration including hosts, IPs, state, etc. and is based upon Apache Zookeeper.&nbsp; This service runs on three nodes in the cluster, one of which is elected as a leader.&nbsp; The leader receives all requests and forwards them to its peers.&nbsp; If the leader fails to respond, a new leader is automatically elected.&nbsp;&nbsp; Zookeeper is accessed via an interface called Zeus.</li>
        </ul>

        <h3>Stargate</h3>

        <ul>
        	<li>Key Role: Data I/O manager</li>
        	<li>Description: Stargate is responsible for all data management and I/O operations and is the main interface from the hypervisor (via NFS, iSCSI, or SMB).&nbsp; This service runs on every node in the cluster in order to serve localized I/O.</li>
        </ul>

        <h3>Curator</h3>

        <ul>
        	<li>Key Role: Map reduce cluster management and cleanup</li>
        	<li>Description: Curator is responsible for managing and distributing tasks throughout the cluster, including disk balancing, proactive scrubbing, and many more items.&nbsp; Curator runs on every node and is controlled by an elected Curator Master who is responsible for the task and job delegation.&nbsp; There are two scan types for Curator, a full scan which occurs around every 6 hours and a partial scan which occurs every hour.</li>
        </ul>

        <h3>Prism</h3>

        <ul>
        	<li>Key Role: UI and API</li>
        	<li>Description: Prism is the management gateway for component and administrators to configure and monitor the Nutanix cluster.&nbsp; This includes Ncli, the HTML5 UI, and REST API.&nbsp; Prism runs on every node in the cluster and uses an elected leader like all components in the cluster.</li>
        </ul>

        <h3>Genesis</h3>

        <ul>
        	<li>Key Role: Cluster component &amp; service manager</li>
        	<li>Description:&nbsp; Genesis is a process which runs on each node and is responsible for any services interactions (start/stop/etc.) as well as for the initial configuration.&nbsp; Genesis is a process which runs independently of the cluster and does not require the cluster to be configured/running.&nbsp; The only requirement for Genesis to be running is that Zookeeper is up and running.&nbsp; The cluster_init and cluster_status pages are displayed by the Genesis process.</li>
        </ul>

        <h3>Chronos</h3>

        <ul>
        	<li>Key Role: Job and task scheduler</li>
        	<li>Description: Chronos is responsible for taking the jobs and tasks resulting from a Curator scan and scheduling/throttling tasks among nodes.&nbsp; Chronos runs on every node and is controlled by an elected Chronos Master that is responsible for the task and job delegation and runs on the same node as the Curator Master.</li>
        </ul>

        <h3>Cerebro</h3>

        <ul>
        	<li>Key Role: Replication/DR manager</li>
        	<li>Description: Cerebro is responsible for the replication and DR capabilities of DSF.&nbsp; This includes the scheduling of snapshots, the replication to remote sites, and the site migration/failover.&nbsp; Cerebro runs on every node in the Nutanix cluster and all nodes participate in replication to remote clusters/sites.</li>
        </ul>

        <h3>Pithos</h3>

        <ul>
        	<li>Key Role: vDisk configuration manager</li>
        	<li>Description: Pithos is responsible for vDisk (DSF file) configuration data.&nbsp; Pithos runs on every node and is built on top of Cassandra.</li>
        </ul>
        </section>

        <section data-type="sect1" id="drive-breakdown-XjGH2">
        <h2>Drive Breakdown</h2>

        <p>In this section, I’ll cover how the various storage devices (SSD / HDD) are broken down, partitioned, and utilized by the Nutanix platform. NOTE: All of the capacities used are in Base2 Gibibyte (GiB) instead of the Base10 Gigabyte (GB).&nbsp; Formatting of the drives with a filesystem and associated overheads has also been taken into account.</p>

        <h3>SSD Devices</h3>

        <p>SSD devices store a few key items which are explained in greater detail above:</p>

        <ul>
        	<li>Nutanix Home (CVM core)</li>
        	<li>Cassandra (metadata storage)</li>
        	<li>OpLog (persistent write buffer)</li>
        	<li>Content Cache (SSD cache)</li>
        	<li>Extent Store (persistent storage)</li>
        </ul>

        <p>The following figure shows an example of the storage breakdown for a Nutanix node’s SSD(s):</p>

        <figure id="id-z2wIm"><img alt="" class="iimagesv2drive_ssdpng" src="imagesv2/drive_ssd.png">
        <figcaption><span class="label">Figure 4-6. </span>SSD Drive Breakdown</figcaption>
        </figure>

        <p>NOTE: The sizing for OpLog is done dynamically as of release 4.0.1 which will allow the extent store portion to grow dynamically.&nbsp; The values used are assuming a completely utilized OpLog.&nbsp; Graphics and proportions aren’t drawn to scale.&nbsp; When evaluating the Remaining GiB capacities, do so from the top down.&nbsp; For example, the Remaining GiB to be used for the OpLog calculation would be after Nutanix Home and Cassandra have been subtracted from the formatted SSD capacity.</p>

        <p>Most models ship with 1 or 2 SSDs, however the same construct applies for models shipping with more SSD devices. For example, if we apply this to an example 3060 or 6060 node which has 2 x 400GB SSDs, this would give us 100GiB of OpLog, 40GiB of Content Cache, and ~440GiB of Extent Store SSD capacity per node.</p>

        <h3>HDD Devices</h3>

        <p>Since HDD devices are primarily used for bulk storage, their breakdown is much simpler:</p>

        <ul>
        	<li>Curator Reservation (Curator storage)</li>
        	<li>Extent Store (persistent storage)</li>
        </ul>

        <figure id="id-2vLu4"><img alt="" class="iimagesv2drive_hddpng" src="imagesv2/drive_hdd.png">
        <figcaption><span class="label">Figure 4-7. </span>HDD Drive Breakdown</figcaption>
        </figure>

        <p>For example, if we apply this to an example 3060 node which has 4 x 1TB HDDs, this would give us 80GiB reserved for Curator and ~3.4TiB of Extent Store HDD capacity per node.</p>

        <p>NOTE: the above values are accurate as of 4.0.1 and may vary by release.</p>
        </section>
        </section>

        <section data-type="chapter" id="distributed-storage-fabric-WzDUk">
        <h2>Distributed Storage Fabric</h2>

        <p>Together, a group of Nutanix nodes forms a distributed platform called the Acropolis Distributed Storage Fabric (DSF).&nbsp; DSF appears to the hypervisor like any centralized storage array, however all of the I/Os are handled locally to provide the highest performance.&nbsp; More detail on how these nodes form a distributed system can be found in the next section.</p>

        <p>The following figure shows an example of how these Nutanix nodes form DSF:</p>

        <figure id="id-nEWsk"><img alt="" class="iimagesv2dsf_overviewpng" src="imagesv2/dsf_overview.png">
        <figcaption><span class="label">Figure 9-1. </span>Distributed Storage Fabric Overview</figcaption>
        </figure>

        <section data-type="sect1" id="data-structure-9zbIg">
        <h2>Data Structure</h2>

        <p>The Acropolis Distributed Storage Fabric is composed of the following high-level struct:</p>

        <h3>Storage Pool</h3>

        <ul>
        	<li>Key Role: Group of physical devices</li>
        	<li>Description: A storage pool is a group of physical storage devices including PCIe SSD, SSD, and HDD devices for the cluster.&nbsp; The storage pool can span multiple Nutanix nodes and is expanded as the cluster scales.&nbsp; In most configurations, only a single storage pool is leveraged.</li>
        </ul>

        <h3>Container</h3>

        <ul>
        	<li>Key Role: Group of VMs/files</li>
        	<li>Description: A container is a logical segmentation of the Storage Pool and contains a group of VM or files (vDisks).&nbsp; Some configuration options (e.g., RF) are configured at the container level, however are applied at the individual VM/file level.&nbsp; Containers typically have a 1 to 1 mapping with a datastore (in the case of NFS/SMB).</li>
        </ul>

        <h3>vDisk</h3>

        <ul>
        	<li>Key Role: vDisk</li>
        	<li>Description: A vDisk is any file over 512KB on DSF including .vmdks and VM hard disks.&nbsp; vDisks are composed of extents which are grouped and stored on disk as an extent group.</li>
        </ul>

        <p>The following figure shows how these map between DSF and the hypervisor:</p>

        <figure id="id-D9ZHK"><img alt="" class="iimagesv2data_structure_1png" src="imagesv2/data_structure_1.png">
        <figcaption><span class="label">Figure 9-2. </span>High-level Filesystem Breakdown</figcaption>
        </figure>

        <h3>Extent</h3>

        <ul>
        	<li>Key Role: Logically contiguous data</li>
        	<li>Description: An extent is a 1MB piece of logically contiguous data which consists of n number of contiguous blocks (varies depending on guest OS block size).&nbsp; Extents are written/read/modified on a sub-extent basis (aka slice) for granularity and efficiency.&nbsp; An extent’s slice may be trimmed when moving into the cache depending on the amount of data being read/cached.</li>
        </ul>

        <h3>Extent Group</h3>

        <ul>
        	<li>Key Role: Physically contiguous stored data</li>
        	<li>Description: An extent group is a 1MB or 4MB piece of physically contiguous stored data.&nbsp; This data is stored as a file on the storage device owned by the CVM.&nbsp; Extents are dynamically distributed among extent groups to provide data striping across nodes/disks to improve performance.&nbsp; NOTE: as of 4.0, extent groups can now be either 1MB or 4MB depending on dedupe.</li>
        </ul>

        <p>The following figure shows how these structs relate between the various file systems:&nbsp;</p>

        <figure id="id-PllIm"><img alt="" class="iimagesv2data_structure_2png" src="imagesv2/data_structure_2.png">
        <figcaption><span class="label">Figure 9-3. </span>Low-level Filesystem Breakdown</figcaption>
        </figure>

        <p>Here is another graphical representation of how these units are related:</p>

        <figure id="id-28LI4"><img alt="" class="iimagesv2data_structure_3png" src="imagesv2/data_structure_3.png">
        <figcaption><span class="label">Figure 6-8. </span>Graphical Filesystem Breakdown</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="io-path-overview-LdEtG">
        <h2>I/O Path Overview</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/SULqVPVXefY">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//SULqVPVXefY"></iframe></div>

        <p>The Nutanix I/O path is composed of the following high-level components:</p>

        <figure id="id-NMxHG"><img alt="" class="iimagesv2io_path_basepng" src="imagesv2/io_path_base.png">
        <figcaption><span class="label">Figure 9-5. </span>DSF I/O Path</figcaption>
        </figure>

        <h3>OpLog</h3>

        <ul>
        	<li>Key Role: Persistent write buffer</li>
        	<li>Description: The OpLog is similar to a filesystem journal and is built to handle bursts of random writes, coalesce them, and then sequentially drain the data to the extent store.&nbsp; Upon a write, the OpLog is synchronously replicated to another n number of CVM’s OpLog before the write is acknowledged for data availability purposes.&nbsp; All CVM OpLogs partake in the replication and are dynamically chosen based upon load.&nbsp; The OpLog is stored on the SSD tier on the CVM to provide extremely fast write I/O performance, especially for random I/O workloads.&nbsp; For sequential workloads, the OpLog is bypassed and the writes go directly to the extent store.&nbsp; If data is currently sitting in the OpLog and has not been drained, all read requests will be directly fulfilled from the OpLog until they have been drained, where they would then be served by the extent store/content cache.&nbsp; For containers where fingerprinting (aka Dedupe) has been enabled, all write I/Os will be fingerprinted using a hashing scheme allowing them to be deduplicated based upon fingerprint in the content cache.</li>
        </ul>

        <h3>Extent Store</h3>

        <ul>
        	<li>Key Role: Persistent data storage</li>
        	<li>Description: The Extent Store is the persistent bulk storage of DSF and spans SSD and HDD and is extensible to facilitate additional devices/tiers.&nbsp; Data entering the extent store is either being A) drained from the OpLog or B) is sequential in nature and has bypassed the OpLog directly.&nbsp; Nutanix ILM will determine tier placement dynamically based upon I/O patterns and will move data between tiers.</li>
        </ul>

        <h3>Content Cache</h3>

        <ul>
        	<li>Key Role: Dynamic read cache</li>
        	<li>Description: The Content Cache (aka “Elastic Dedupe Engine”) is a deduplicated read cache which spans both the CVM’s memory and SSD.&nbsp; Upon a read request of data not in the cache (or based upon a particular fingerprint), the data will be placed into the single-touch pool of the content cache which completely sits in memory, where it will use LRU until it is ejected from the cache.&nbsp; Any subsequent read request will “move” (no data is actually moved, just cache metadata) the data into the memory portion of the multi-touch pool, which consists of both memory and SSD.&nbsp; From here there are two LRU cycles, one for the in-memory piece upon which eviction will move the data to the SSD section of the multi-touch pool where a new LRU counter is assigned.&nbsp; Any read request for data in the multi-touch pool will cause the data to go to the peak of the multi-touch pool where it will be given a new LRU counter.&nbsp; Fingerprinting is configured at the container level and can be configured via the UI.&nbsp; By default, fingerprinting is disabled.</li>
        </ul>

        <p>The following figure shows a high-level overview of the Content Cache:</p>

        <figure id="id-Blqhp"><img alt="" class="iimagesv2content_cachepng" src="imagesv2/content_cache.png">
        <figcaption><span class="label">Figure 9-6. </span>DSF Content Cache</figcaption>
        </figure>

        <h3>1.1.1.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Extent Cache</h3>

        <ul>
        	<li>Key Role: In-memory read cache</li>
        	<li>Description: The Extent Cache is an in-memory read cache that is completely in the CVM’s memory.&nbsp; This will store non-fingerprinted extents for containers where fingerprinting and deduplication are disabled.&nbsp; As of version 3.5, this is separate from the Content Cache, however these will be merging in a subsequent release.</li>
        </ul>
        </section>

        <section data-type="sect1" id="data-protection-O7pCP">
        <h2>Data Protection</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/OWhdo81yTpk">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//OWhdo81yTpk"></iframe></div>

        <p>The Nutanix platform currently uses a resiliency factor, also known as a replication factor (RF), and checksum to ensure data redundancy and availability in the case of a node or disk failure or corruption.&nbsp; As explained above, the OpLog acts as a staging area to absorb incoming writes onto a low-latency SSD tier.&nbsp; Upon being written to the local OpLog, the data is synchronously replicated to another one or two Nutanix CVM’s OpLog (dependent on RF) before being acknowledged (Ack) as a successful write to the host.&nbsp; This ensures that the data exists in at least two or three independent locations and is fault tolerant. NOTE: For RF3, a minimum of 5 nodes is required since metadata will be RF5.&nbsp;</p>

        <p>Data RF is configured via Prism and is done at the container level. All nodes participate in OpLog replication to eliminate any “hot nodes”, ensuring linear performance at scale.&nbsp; While the data is being written, a checksum is computed and stored as part of its metadata. Data is then asynchronously drained to the extent store where the RF is implicitly maintained.&nbsp; In the case of a node or disk failure, the data is then re-replicated among all nodes in the cluster to maintain the RF.&nbsp; Any time the data is read, the checksum is computed to ensure the data is valid.&nbsp; In the event where the checksum and data don’t match, the replica of the data will be read and will replace the non-valid copy.</p>

        <p>The following figure shows an example of what this logically looks like:&nbsp;</p>

        <figure id="id-40WcA"><img alt="" class="iimagesv2data_protectionpng fse fs" src="imagesv2/data_protection.png">
        <figcaption><span class="label">Figure 9-7. </span>DSF Data Protection</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="scalable-metadata-nLVFV">
        <h2>Scalable Metadata</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/MlQczJhQI3U">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//MlQczJhQI3U"></iframe></div>

        <p>Metadata is at the core of any intelligent system and is even more critical for any filesystem or storage array.&nbsp; In terms of DSF, there are a few key structs that are critical for its success: it has to be right 100% of the time (known as“strictly consistent”), it has to be scalable, and it has to perform at massive scale.&nbsp; As mentioned in the architecture section above, DSF utilizes a “ring-like” structure as a key-value store which stores essential metadata as well as other platform data (e.g., stats, etc.). In order to ensure metadata availability and redundancy a RF is utilized among an odd amount of nodes (e.g., 3, 5, etc.). Upon a metadata write or update, the row is written to a node in the ring and then replicated to n number of peers (where n is dependent on cluster size).&nbsp; A majority of nodes must agree before anything is committed, which is enforced using the Paxos algorithm.&nbsp; This ensures strict consistency for all data and metadata stored as part of the platform.</p>

        <p>The following figure shows an example of a metadata insert/update for a 4 node cluster:</p>

        <figure id="id-OVDUY"><img alt="" class="iimagesv2metadata_1png" src="imagesv2/metadata_1.png">
        <figcaption><span class="label">Figure 9-8. </span>Cassandra Ring Structure</figcaption>
        </figure>

        <p>Performance at scale is also another important struct for DSF metadata.&nbsp; Contrary to traditional dual-controller or “master” models, each Nutanix node is responsible for a subset of the overall platform’s metadata.&nbsp; This eliminates the traditional bottlenecks by allowing metadata to be served and manipulated by all nodes in the cluster.&nbsp; A consistent hashing scheme is utilized to minimize the redistribution of keys during cluster size modifications (also known as&nbsp;“add/remove node”) When the cluster scales (e.g., from 4 to 8 nodes), the nodes are inserted throughout the ring between nodes for “block awareness” and reliability.</p>

        <p>The following figure shows an example of the metadata “ring” and how it scales:</p>

        <figure id="id-oqWIo"><img alt="" class="iimagesv2metadata_2png" src="imagesv2/metadata_2.png">
        <figcaption><span class="label">Figure 9-9. </span>Cassandra Ring Scale Out</figcaption>
        </figure>

        <p>&nbsp;</p>
        </section>

        <section data-type="sect1" id="data-path-resiliency-e0acX">
        <h2>Data Path Resiliency</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/SJIb_mTdMPg">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//SJIb_mTdMPg"></iframe></div>

        <p>Reliability and resiliency are key, if not the most important concepts within DSF or any primary storage platform.&nbsp;</p>

        <p>Contrary to traditional architectures which are built around the idea that hardware will be reliable, Nutanix takes a different approach: it expects hardware will eventually fail.&nbsp; By doing so, the system is designed to handle these failures in an elegant and non-disruptive manner.</p>

        <p>NOTE: That doesn’t mean the hardware quality isn’t there, just a concept shift.&nbsp; The Nutanix hardware and QA teams undergo an exhaustive qualification and vetting process.</p>

        <p><em>Potential levels of failure</em></p>

        <p>Being a distributed system, DSF is built to handle component, service, and CVM failures, which can be characterized on a few levels:</p>

        <ul>
        	<li>Disk Failure</li>
        	<li>CVM “Failure”</li>
        	<li>Node Failure</li>
        </ul>

        <h3>Disk Failure</h3>

        <p>A disk failure can be characterized as just that, a disk which has either been removed, had a dye failure, or is experiencing I/O errors and has been proactively removed.</p>

        <p>VM impact:</p>

        <ul>
        	<li>HA event: <strong>No</strong></li>
        	<li>Failed I/Os: <strong>No</strong></li>
        	<li>Latency: <strong>No impact</strong></li>
        </ul>

        <p>In the event of a disk failure, a Curator scan (MapReduce Framework) will occur immediately. &nbsp;It will scan the metadata (Cassandra) to find the data previously hosted on the failed disk and the nodes / disks hosting the replicas.</p>

        <p>Once it has found that data that needs to be “re-replicated”, it will distribute the replication tasks to the nodes throughout the cluster.&nbsp;</p>

        <p>An important thing to highlight here is given how Nutanix distributes data and replicas across all nodes / CVMs / disks; all nodes / CVMs / disks will participate in the re-replication.&nbsp;</p>

        <p>This substantially reduces the time required for re-protection, as the power of the full cluster can be utilized; the larger the cluster, the faster the re-protection.</p>

        <h3>CVM “Failure”</h3>

        <p>A CVM "failure” can be characterized as a CVM power action causing the CVM to be temporarily unavailable.&nbsp; The system is designed to transparently handle these gracefully.&nbsp; In the event of a failure, I/Os will be re-directed to other CVMs within the cluster.&nbsp; The mechanism for this will vary by hypervisor.&nbsp;</p>

        <p>The rolling upgrade process actually leverages this capability as it will upgrade one CVM at a time, iterating through the cluster.</p>

        <p>VM impact:</p>

        <ul>
        	<li>HA event: <strong>No</strong></li>
        	<li>Failed I/Os: <strong>No</strong></li>
        	<li>Latency: <strong>Potentially higher given I/Os over the network</strong></li>
        </ul>

        <p>In the event of a CVM "failure” the I/O which was previously being served from the down CVM, will be forwarded to other CVMs throughout the cluster.&nbsp; ESXi and Hyper-V handle this via a process called CVM Autopathing, which leverages HA.py (like “happy”), where it will modify the routes to forward traffic going to the internal address (192.168.5.2) to the external IP of other CVMs throughout the cluster.&nbsp; This enables the datastore to remain intact, just the CVM responsible for serving the I/Os is remote.</p>

        <p>Once the local CVM comes back up and is stable, the route would be removed and the local CVM would take over all new I/Os.</p>

        <p>In the case of KVM, iSCSI multi-pathing is leveraged where the primary path is the local CVM and the two other paths would be remote.&nbsp; In the event where the primary path fails, one of the other paths will become active.</p>

        <p>Similar to Autopathing with ESXi and Hyper-V, when the local CVM comes back online, it’ll take over as the primary path.</p>

        <h3>Node Failure</h3>

        <p>VM Impact:</p>

        <ul>
        	<li>HA event: <strong>Yes</strong></li>
        	<li>Failed I/Os: <strong>No</strong></li>
        	<li>Latency: <strong>No impact</strong></li>
        </ul>

        <p>In the event of a node failure, a VM HA event will occur restarting the VMs on other nodes throughout the virtualization cluster.&nbsp; Once restarted, the VMs will continue to perform I/Os as usual which will be handled by their local CVMs.</p>

        <p>Similar to the case of a disk failure above, a Curator scan will find the data previously hosted on the node and its respective replicas.</p>

        <p>Similar to the disk failure scenario above, the same process will take place to re-protect the data, just for the full node (all associated disks).</p>

        <p>In the event where the node remains down for a prolonged period of time, the down CVM will be removed from the metadata ring.&nbsp; It will be joined back into the ring after it has been up and stable for a duration of time.</p>
        </section>

        <section data-type="sect1" id="elastic-dedupe-engine-D7yFl">
        <h2>Elastic Dedupe Engine</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/C-rp13cDpNw">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//C-rp13cDpNw"></iframe></div>

        <p>The Elastic Dedupe Engine is a software-based feature of DSF which allows for data deduplication in the capacity (HDD) and performance (SSD/Memory) tiers.&nbsp; Streams of data are fingerprinted during ingest using a SHA-1 hash at a 16K granularity.&nbsp; This fingerprint is only done on data ingest and is then stored persistently as part of the written block’s metadata.&nbsp; NOTE: Initially a 4K granularity was used for fingerprinting, however after testing 16K offered the best blend of deduplication with reduced metadata overhead.&nbsp; Deduplicated data is pulled into the cache at a 4K granularity.</p>

        <p>Contrary to traditional approaches which utilize background scans requiring the data to be re-read, Nutanix performs the fingerprint in-line on ingest.&nbsp; For duplicate data that can be deduplicated in the capacity tier, the data does not need to be scanned or re-read, essentially duplicate copies can be removed.</p>

        <p>The following figure shows an example of how the Elastic Dedupe Engine scales and handles local VM I/O requests:</p>

        <figure id="id-ELlUP"><img alt="" class="iimagesv2dedup_1png" src="imagesv2/dedup_1.png">
        <figcaption><span class="label">Figure 9-10. </span>Elastic Dedupe Engine - Scale</figcaption>
        </figure>

        <p>Fingerprinting is done during data ingest of data with an I/O size of 64K or greater.&nbsp; Intel acceleration is leveraged for the SHA-1 computation which accounts for very minimal CPU overhead.&nbsp; In cases where fingerprinting is not done during ingest (e.g., smaller I/O sizes), fingerprinting can be done as a background process. The Elastic Deduplication Engine spans both the capacity disk tier (HDD), but also the performance tier (SSD/Memory).&nbsp; As duplicate data is determined, based upon multiple copies of the same fingerprints, a background process will remove the duplicate data using the DSF Map Reduce framework (Curator). For data that is being read, the data will be pulled into the DSF Content Cache which is a multi-tier/pool cache.&nbsp; Any subsequent requests for data having the same fingerprint will be pulled directly from the cache.&nbsp; To learn more about the Content Cache and pool structure, please refer to the ‘Content Cache’ sub-section in the I/O path overview.</p>

        <p>The following figure shows an example of how the Elastic Dedupe Engine interacts with the DSF I/O path:</p>

        <figure id="id-0o1Hr"><img alt="" class="iimagesv2dedup_2png" src="imagesv2/dedup_2.png">
        <figcaption><span class="label">Figure 9-11. </span>EDE I/O Path</figcaption>
        </figure>

        <p>You can view the current deduplication rates via Prism on the Storage &gt; Dashboard page.</p>

        <div  data-type="note" class="note" id="pro-tip-DOYcW"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Use performance tier deduplication on your base images (you can manually fingerprint them using vdisk_manipulator) to take advantage of the content cache.</p>

        <p>Use capacity tier deduplication for P2V / V2V,&nbsp;when using Hyper-V since ODX does a full data copy, or when doing cross-container clones (not usually recommended).</p>
        </div>
        </section>

        <section data-type="sect1" id="compression-Y0EC1">
        <h2>Compression</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/ERDqOCzDcQY">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//ERDqOCzDcQY"></iframe></div>

        <p>The Nutanix Capacity Optimization Engine (COE) is responsible for performing data transformations to increase data efficiency on disk.&nbsp; Currently compression is one of the key features of the COE to perform data optimization. DSF provides both in-line and post-process flavors of compression to best suit the customer’s needs and type of data.&nbsp;</p>

        <p>In-line compression will compress sequential streams of data or large I/O sizes in memory before it is written to disk, while post-process compression will initially write the data as normal (in an un-compressed state) and then leverage the Curator framework to compress the data cluster wide. When in-line compression is enabled but the I/Os are random in nature, the data will be written un-compressed in the OpLog, coalesced, and then compressed in memory before being written to the Extent Store. The Google Snappy compression library is leveraged which provides good compression ratios with minimal computational overhead and extremely fast compression / decompression rates.</p>

        <p>The following figure shows an example of how in-line compression interacts with the DSF write I/O path:</p>

        <figure id="id-xyYUD"><img alt="" class="iimagesv2compression_1png" src="imagesv2/compression_1.png">
        <figcaption><span class="label">Figure 9-12. </span>Inline Compression I/O Path</figcaption>
        </figure>

        <p>For post-process compression, all new write I/O is written in an un-compressed state and follows the normal DSF I/O path.&nbsp; After the compression delay (configurable) is met and the data has become cold (down-migrated to the HDD tier via ILM), the data is eligible to become compressed. Post-process compression uses the Curator MapReduce framework and all nodes will perform compression tasks.&nbsp; Compression tasks will be throttled by Chronos.</p>

        <p>The following figure shows an example of how post-process compression interacts with the DSF write I/O path:</p>

        <figure id="id-2e9U4"><img alt="" class="iimagesv2compression_2png" src="imagesv2/compression_2.png">
        <figcaption><span class="label">Figure 9-13. </span>Post-process Compression I/O Path</figcaption>
        </figure>

        <p>&nbsp;</p>

        <p>For read I/O, the data is first decompressed in memory and then the I/O is served.&nbsp; For data that is heavily accessed, the data will become decompressed in the HDD tier and can then leverage ILM to move up to the SSD tier as well as be stored in the cache.</p>

        <p>The following figure shows an example of how decompression interacts with the DSF I/O path during read:</p>

        <figure id="id-Y0QCl"><img alt="" class="iimagesv2compression_3png" src="imagesv2/compression_3.png">
        <figcaption><span class="label">Figure 9-14. </span>Decompression I/O Path</figcaption>
        </figure>

        <p>You can view the current compression rates via Prism on the Storage &gt; Dashboard page.</p>

        <div  data-type="note" class="note" id="pro-tip-mZWCd"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Almost always use inline compression (compression delay = 0) as it will only compress larger / sequential writes and not impact random write performance.</p>
        </div>
        </section>

        <section data-type="sect1" id="erasure-coding-ZLoid">
        <h2>Erasure Coding</h2>

        <p>The Nutanix platform relies on a replication factor (RF) for data protection and availability.&nbsp; This method provides the highest degree of availability because it does not require reading from more than one storage location or data re-computation on failure.&nbsp; However, this does come at the cost of storage resources as full copies are required.&nbsp;</p>

        <p>To provide a balance between availability while reducing the amount of storage required, DSF provides the ability to encode data using erasure codes (EC).</p>

        <p>Similar to the concept of RAID (levels 4, 5, 6, etc.) where parity is calculated, EC encodes a strip of data blocks on different nodes and calculates parity.&nbsp; In the event of a host and/or disk failure, the parity can be leveraged to calculate any missing data blocks (decoding).&nbsp; In the case of DSF, the data block is an extent group and each data block must be on a different node and belong to a different vDisk.</p>

        <p>The number of data and parity blocks in a strip is configurable based upon the desired failures to tolerate.&nbsp; The configuration is commonly referred to as &lt;number of data blocks&gt;/&lt;number of parity blocks&gt;.</p>

        <p>For example, “RF2 like” availability (e.g., N+1) could consist of 3 or 4 data blocks and 1 parity block in a strip (e.g., 3/1 or 4/1).&nbsp; “RF3 like” availability (e.g. N+2) could consist of 3 or 4 data blocks and 2 parity blocks in a strip (e.g. 3/2 or 4/2).</p>

        <p>The expected overhead can be calculated as &lt;# parity blocks&gt; / &lt;# parity blocks + # data blocks&gt;.&nbsp; For example, a 4/1 strip has a 20% overhead or 1.2X compared to the 2X of RF2.</p>

        <p>The encoding is done post-process and leverages the Curator MapReduce framework for task distribution.&nbsp; Since this is a post-process framework, the traditional write I/O path is unaffected.</p>

        <div  data-type="note" class="note" id="pro-tip-A1OTx"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>You can override the default strip size (4/1 for “RF2 like” or 4/2 for “RF3 like”) via NCLI ‘ctr [create / edit] … erasure-code=&lt;N&gt;/&lt;K&gt;’ where N is the number of data blocks and K is the number of parity blocks.</p>
        </div>

        <p>A normal environment using RF would look like the following:</p>

        <figure id="id-eqJf0"><img alt="" class="iimagesv2ec_1png" src="imagesv2/ec_1.png">
        <figcaption><span class="label">Figure 9-15. </span>Typical DSF RF Data Layout</figcaption>
        </figure>

        <p>In this scenario, we have a mix of both RF2 and RF3 data whose primary copies are local and replicas are distributed to other nodes throughout the cluster.</p>

        <p>When a Curator full scan runs, it will find eligible extent groups which are available to become encoded.&nbsp; After the eligible candidates are found, the encoding tasks will be distributed and throttled via Chronos.</p>

        <p>The following figure shows an example 4/1 and 3/2 strip:</p>

        <figure id="id-9Gbtm"><img alt="" class="iimagesv2ec_2png" src="imagesv2/ec_2.png">
        <figcaption><span class="label">Figure 9-16. </span>DSF Encoded Strip - Pre-savings</figcaption>
        </figure>

        <p>Once the data has been successfully encoded (strips and parity calculation), the replica extent groups are then removed.</p>

        <p>The following figure shows the environment after EC has run with the storage savings:</p>

        <figure id="id-ryKSA"><img alt="" class="iimagesv2ec_3png" src="imagesv2/ec_3.png">
        <figcaption><span class="label">Figure 9-17. </span>DSF Encoded Strip - Post-savings</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="storage-tiering-and-prioritization-7Wwhp">
        <h2>Storage Tiering and Prioritization</h2>

        <p>The Disk Balancing section above talked about how storage capacity was pooled among all nodes in a Nutanix cluster and that ILM would be used to keep hot data local.&nbsp; A similar concept applies to disk tiering, in which the cluster’s SSD and HDD tiers are cluster-wide and DSF ILM is responsible for triggering data movement events. A local node’s SSD tier is always the highest priority tier for all I/O generated by VMs running on that node, however all of the cluster’s SSD resources are made available to all nodes within the cluster.&nbsp; The SSD tier will always offer the highest performance and is a very important thing to manage for hybrid arrays.</p>

        <p>The tier prioritization can be classified at a high-level by the following:</p>

        <figure id="id-2L9h4"><img alt="" class="iimagesv2tiering_1png" src="imagesv2/tiering_1.png">
        <figcaption><span class="label">Figure 9-18. </span>DSF Tier Prioritization</figcaption>
        </figure>

        <p>Specific types of resources (e.g. SSD, HDD, etc.) are pooled together and form a cluster wide storage tier.&nbsp; This means that any node within the cluster can leverage the full tier capacity, regardless if it is local or not.</p>

        <p>The following figure shows a high level example of what this pooled tiering looks like:</p>

        <figure id="id-1qouA"><img alt="" class="iimagesv2tiering_2png" src="imagesv2/tiering_2.png">
        <figcaption><span class="label">Figure 9-19. </span>DSF Cluster-wide Tiering</figcaption>
        </figure>

        <p>A common question is what happens when a local node’s SSD becomes full?&nbsp; As mentioned in the Disk Balancing section, a key concept is trying to keep uniform utilization of devices within disk tiers.&nbsp; In the case where a local node’s SSD utilization is high, disk balancing will kick in to move the coldest data on the local SSDs to the other SSDs throughout the cluster.&nbsp; This will free up space on the local SSD to allow the local node to write to SSD locally instead of going over the network.&nbsp; A key point to mention is that all CVMs and SSDs are used for this remote I/O to eliminate any potential bottlenecks and remediate some of the hit by performing I/O over the network.</p>

        <figure id="id-nPytk"><img alt="" class="iimagesv2tiering_3png" src="imagesv2/tiering_3.png">
        <figcaption><span class="label">Figure 9-20. </span>DSF Cluster-wide Tier Balancing</figcaption>
        </figure>

        <p>The other case is when the overall tier utilization breaches a specific threshold [curator_tier_usage_ilm_threshold_percent (Default=75)] where DSF ILM will kick in and as part of a Curator job will down-migrate data from the SSD tier to the HDD tier.&nbsp; This will bring utilization within the threshold mentioned above or free up space by the following amount [curator_tier_free_up_percent_by_ilm (Default=15)], whichever is greater. The data for down-migration is chosen using last access time. In the case where the SSD tier utilization is 95%, 20% of the data in the SSD tier will be moved to the HDD tier (95% –&gt; 75%).&nbsp;</p>

        <p>However, if the utilization was 80%, only 15% of the data would be moved to the HDD tier using the minimum tier free up amount.</p>

        <figure id="id-BEKsp"><img alt="" class="iimagesv2tiering_4png" src="imagesv2/tiering_4.png">
        <figcaption><span class="label">Figure 9-21. </span>DSF Tier ILM</figcaption>
        </figure>

        <p>DSF ILM will constantly monitor the I/O patterns and (down/up) migrate data as necessary as well as bring the hottest data local regardless of tier.</p>
        </section>

        <section data-type="sect1" id="disk-balancing-emOTX">
        <h2>Disk Balancing</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/atbkgrmpVNo">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//atbkgrmpVNo"></iframe></div>

        <p>DSF is designed to be a very dynamic platform which can react to various workloads as well as allow heterogeneous node types: compute heavy (3050, etc.) and storage heavy (60X0, etc.) to be mixed in a single cluster.&nbsp; Ensuring uniform distribution of data is an important item when mixing nodes with larger storage capacities. DSF has a native feature, called disk balancing, which is used to ensure uniform distribution of data throughout the cluster.&nbsp; Disk balancing works on a node’s utilization of its local storage capacity and is integrated with DSF ILM.&nbsp; Its goal is to keep utilization uniform among nodes once the utilization has breached a certain threshold.</p>

        <p>The following figure shows an example of a mixed cluster (3050 + 6050) in an “unbalanced” state:</p>

        <figure id="id-BEvUp"><img alt="" class="iimagesv2disk_balancing_1png" src="imagesv2/disk_balancing_1.png">
        <figcaption><span class="label">Figure 9-22. </span>Disk Balancing - Unbalanced State</figcaption>
        </figure>

        <p>Disk balancing leverages the DSF Curator framework and is run as a scheduled process as well as when a threshold has been breached (e.g., local node capacity utilization &gt; n %).&nbsp; In the case where the data is not balanced, Curator will determine which data needs to be moved and will distribute the tasks to nodes in the cluster. In the case where the node types are homogeneous (e.g., 3050), utilization should be fairly uniform. However, if there are certain VMs running on a node which are writing much more data than others, there can become a skew in the per node capacity utilization.&nbsp; In this case, disk balancing would run and move the coldest data on that node to other nodes in the cluster. In the case where the node types are heterogeneous (e.g., 3050 + 6020/50/70), or where a node may be used in a “storage only” mode (not running any VMs), there will likely be a requirement to move data.</p>

        <p>The following figure shows an example the mixed cluster after disk balancing has been run in a “balanced” state:</p>

        <figure id="id-bNXsJ"><img alt="" class="iimagesv2disk_balancing_2png" src="imagesv2/disk_balancing_2.png">
        <figcaption><span class="label">Figure 9-23. </span>Disk Balancing - Balanced State</figcaption>
        </figure>

        <p>In some scenarios, customers might run some nodes in a “storage-only” state where only the CVM will run on the node whose primary purpose is bulk storage capacity.&nbsp; In this case, the full node's memory can be added to the CVM to provide a much larger read cache.</p>

        <p>The following figure shows an example of how a storage only node would look in a mixed cluster with disk balancing moving data to it from the active VM nodes:</p>

        <figure id="id-A7wiY"><img alt="" class="iimagesv2disk_balancing_3png" src="imagesv2/disk_balancing_3.png">
        <figcaption><span class="label">Figure 9-24. </span>Disk Balancing - Storage Only Node</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="availability-domains-gZxiv">
        <h2>Availability Domains</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/LDaNY9AJDn8">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//LDaNY9AJDn8"></iframe></div>

        <p>Availability Domains (aka node/block/rack awareness) is a key struct for distributed systems to abide by for determining component and data placement.&nbsp; DSF is currently node and block aware, however this will increase to rack aware as cluster sizes grow.&nbsp; Nutanix refers to a “block” as the chassis which contains either one, two, or four server “nodes”. NOTE: A minimum of 3 blocks must be utilized for block awareness to be activated, otherwise node awareness will be defaulted to.&nbsp;</p>

        <p>It is recommended to utilized uniformly populated blocks to ensure block awareness is enabled.&nbsp; Common scenarios and the awareness level utilized can be found at the bottom of this section.&nbsp; The 3-block requirement is due to ensure quorum. For example, a 3450 would be a block which holds 4 nodes.&nbsp; The reason for distributing roles or data across blocks is to ensure if a block fails or needs maintenance the system can continue to run without interruption.&nbsp; NOTE: Within a block, the redundant PSU and fans are the only shared components Awareness can be broken into a few key focus areas:</p>

        <ul>
        	<li>Data (The VM data)</li>
        	<li>Metadata (Cassandra)</li>
        	<li>Configuration Data (Zookeeper)</li>
        </ul>

        <h3>Data</h3>

        <p>With DSF, data replicas will be written to other blocks in the cluster to ensure that in the case of a block failure or planned downtime, the data remains available.&nbsp; This is true for both RF2 and RF3 scenarios, as well as in the case of a block failure. An easy comparison would be “node awareness”, where a replica would need to be replicated to another node which will provide protection in the case of a node failure.&nbsp; Block awareness further enhances this by providing data availability assurances in the case of block outages.</p>

        <p>The following figure shows how the replica placement would work in a 3-block deployment:</p>

        <figure id="id-XaouA"><img alt="" class="iimagesv2avail_dom_1png" src="imagesv2/avail_dom_1.png">
        <figcaption><span class="label">Figure 9-25. </span>Block Aware Replica Placement</figcaption>
        </figure>

        <p>In the case of a block failure, block awareness will be maintained and the re-replicated blocks will be replicated to other blocks within the cluster:</p>

        <figure id="id-oXxIo"><img alt="" class="iimagesv2avail_dom_2png" src="imagesv2/avail_dom_2.png">
        <figcaption><span class="label">Figure 9-26. </span>Block Failure Replica Placement</figcaption>
        </figure>

        <h3>Metadata</h3>

        <p>As mentioned in the Scalable Metadata section above, Nutanix leverages a heavily modified Cassandra platform to store metadata and other essential information.&nbsp; Cassandra leverages a ring-like structure and replicates to n number of peers within the ring to ensure data consistency and availability.</p>

        <p>The following figure shows an example of the Cassandra ring for a 12-node cluster:</p>

        <figure id="id-lzPH1"><img alt="" class="iimagesv2avail_dom_3png fse fs image" src="imagesv2/avail_dom_3.png" style="width: 50%; height: 50%">
        <figcaption><span class="label">Figure 9-27. </span>12 Node Cassandra Ring</figcaption>
        </figure>

        <p>Cassandra peer replication iterates through nodes in a clockwise manner throughout the ring.&nbsp; With block awareness, the peers are distributed among the blocks to ensure no two peers are on the same block.</p>

        <p>The following figure shows an example node layout translating the ring above into the block based layout:</p>

        <figure id="id-dwyHd"><img alt="" class="iimagesv2avail_dom_4png" src="imagesv2/avail_dom_4.png">
        <figcaption><span class="label">Figure 9-28. </span>Cassandra Node Block Aware Placement</figcaption>
        </figure>

        <p>With this block-aware nature, in the event of a block failure there will still be at least two copies of the data (with Metadata RF3 – In larger clusters RF5 can be leveraged).</p>

        <p>The following figure shows an example of all of the nodes replication topology to form the ring (yes – it’s a little busy):</p>

        <figure id="id-Q78hL"><img alt="" class="iimagesv2avail_dom_5png" src="imagesv2/avail_dom_5.png">
        <figcaption><span class="label">Figure 9-29. </span>Full Cassandra Node Block Aware Placement</figcaption>
        </figure>

        <p>&nbsp;</p>

        <h3>Configuration Data</h3>

        <p>Nutanix leverages Zookeeper to store essential configuration data for the cluster.&nbsp; This role is also distributed in a block-aware manner to ensure availability in the case of a block failure.</p>

        <p>The following figure shows an example layout showing 3 Zookeeper nodes distributed in a block-aware manner:</p>

        <figure id="id-W0QTZ"><img alt="" class="iimagesv2avail_dom_6png" src="imagesv2/avail_dom_6.png">
        <figcaption><span class="label">Figure 9-30. </span>Zookeeper Block Aware Placement</figcaption>
        </figure>

        <p>In the event of a block outage, meaning one of the Zookeeper nodes will be gone, the Zookeeper role would be transferred to another node in the cluster as shown below:</p>

        <figure id="id-kxYsp"><img alt="" class="iimagesv2avail_dom_7png" src="imagesv2/avail_dom_7.png">
        <figcaption><span class="label">Figure 9-31. </span>Zookeeper Placement Block Failure</figcaption>
        </figure>

        <p>When the block comes back online, the Zookeeper role would be transferred back to maintain block awareness.</p>

        <p>NOTE: Prior to 4.5, this migration was not automatic and must be done manually.</p>

        <p>Below we breakdown some common scenarios and what level of awareness will be utilized:</p>

        <ul>
        	<li>&lt; 3 blocks –&gt; <strong>NODE</strong> awareness</li>
        	<li>3+ blocks uniformly populated –&gt; <strong>BLOCK + NODE</strong> awareness</li>
        	<li>3+ blocks not uniformly populated
        	<ul>
        		<li>If SSD <strong>or</strong> HDD tier variance between blocks is &gt; max variance –&gt; <strong>NODE</strong> awareness</li>
        		<li>If SSD and HDD tier variance between blocks is &lt; max variance&nbsp; –&gt; <strong>BLOCK</strong> + NODE awareness</li>
        	</ul>
        	</li>
        </ul>

        <p>NOTE: max tier variance is calculated as: 100 / (RF+1)</p>

        <ul>
        	<li>E.g., 33% for RF2 or 25% for RF3</li>
        </ul>
        </section>

        <section data-type="sect1" id="snapshots-and-clones-eKOHX">
        <h2>Snapshots and Clones</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/uK5wWR44UYE">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//uK5wWR44UYE"></iframe></div>

        <p>DSF provides native support for offloaded snapshots and clones which can be leveraged via VAAI, ODX, ncli, REST, Prism, etc.&nbsp; Both the snapshots and clones leverage the redirect-on-write algorithm which is the most effective and efficient. As explained in the Data Structure section above, a virtual machine consists of files (vmdk/vhdx) which are vDisks on the Nutanix platform.&nbsp;</p>

        <p>A vDisk is composed of extents which are logically contiguous chunks of data, which are stored within extent groups which are physically contiguous data stored as files on the storage devices. When a snapshot or clone is taken, the base vDisk is marked immutable and another vDisk is created as read/write.&nbsp; At this point, both vDisks have the same block map, which is a metadata mapping of the vDisk to its corresponding extents. Contrary to traditional approaches which require traversal of the snapshot chain (which can add read latency), each vDisk has its own block map.&nbsp; This eliminates any of the overhead normally seen by large snapshot chain depths and allows you to take continuous snapshots without any performance impact.</p>

        <p>The following figure shows an example of how this works when a snapshot is taken (NOTE: I need to give some credit to NTAP as a base for these diagrams, as I thought their representation was the clearest):</p>

        <figure id="id-MQOfz"><img alt="" class="iimagesv2snap_1png" src="imagesv2/snap_1.png">
        <figcaption><span class="label">Figure 9-32. </span>Example Snapshot Block Map</figcaption>
        </figure>

        <p>The same method applies when a snapshot or clone of a previously snapped or cloned vDisk is performed:</p>

        <figure id="id-vJ0t9"><img alt="" class="iimagesv2snap_2png" src="imagesv2/snap_2.png">
        <figcaption><span class="label">Figure 9-33. </span>Multi-snap Block Map &amp; New Write</figcaption>
        </figure>

        <p>The same methods are used for both snapshots and/or clones of a VM or vDisk(s).&nbsp; When a VM or vDisk is cloned, the current block map is locked and the clones are created.&nbsp; These updates are metadata only, so no I/O actually takes place.&nbsp; The same method applies for clones of clones; essentially the previously cloned VM acts as the “Base vDisk” and upon cloning, that block map is locked and two “clones” are created: one for the VM being cloned and another for the new clone.&nbsp;</p>

        <p>They both inherit the prior block map and any new writes/updates would take place on their individual block maps.</p>

        <figure id="id-m8Ehy"><img alt="" class="iimagesv2snap_3png" src="imagesv2/snap_3.png">
        <figcaption><span class="label">Figure 9-34. </span>Multi-Clone Block Maps</figcaption>
        </figure>

        <p>As mentioned previously, each VM/vDisk has its own individual block map.&nbsp; So in the above example, all of the clones from the base VM would now own their block map and any write/update would occur there.&nbsp;</p>

        <p>The following figure shows an example of what this looks like:</p>

        <figure id="id-EvgfP"><img alt="" class="iimagesv2snap_4png" src="imagesv2/snap_4.png">
        <figcaption><span class="label">Figure 9-35. </span>Clone Block Maps - New Write</figcaption>
        </figure>

        <p>Any subsequent clones or snapshots of a VM/vDisk would cause the original block map to be locked and would create a new one for R/W access.</p>
        </section>

        <section data-type="sect1" id="replication-and-multi-site-disaster-recovery-OKAsP">
        <h2>Replication and Multi-Site Disaster Recovery</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/AoKwKI7CXIM">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//AoKwKI7CXIM"></iframe></div>

        <p>Nutanix provides native DR and replication capabilities, which build upon the same features explained in the Snapshots &amp; Clones section.&nbsp; Cerebro is the component responsible for managing the DR and replication in DSF.&nbsp; Cerebro runs on every node and a Cerebro master is elected (similar to NFS master) and is responsible for managing replication tasks.&nbsp; In the event the CVM acting as Cerebro master fails, another is elected and assumes the role.&nbsp; The Cerebro page can be found on &lt;CVM IP&gt;:2020. The DR function can be broken down into a few key focus areas:</p>

        <ul>
        	<li>Replication Topologies</li>
        	<li>Implementation Constructs</li>
        	<li>Replication Lifecycle</li>
        	<li>Global Deduplication</li>
        </ul>

        <h3>Replication Topologies</h3>

        <p>Traditionally, there are a few key replication topologies: Site to site, hub and spoke, and full and/or partial mesh.&nbsp; Contrary to traditional solutions which only allow for site to site or hub and spoke, Nutanix provides a fully mesh or flexible many-to-many model.</p>

        <figure id="id-yE8cG"><img alt="" class="iimagesv2dr_1png" src="imagesv2/dr_1.png">
        <figcaption><span class="label">Figure 9-36. </span>Example Replication Topologies</figcaption>
        </figure>

        <p>Essentially, this allows the admin to determine a replication capability that meets their company's needs.</p>

        <h3>Implementation Constructs</h3>

        <p>Within Nutanix DR, there are a few key constructs which are explained below:</p>

        <h4>Remote Site</h4>

        <ul>
        	<li>Key Role: A remote Nutanix cluster</li>
        	<li>Description: A remote Nutanix cluster which can be leveraged as a target for backup or DR purposes.</li>
        	<li>cases,</li>
        </ul>

        <div  data-type="note" class="note" id="pro-tip-qrBIN"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Ensure the target site has ample capacity (compute/storage) to handle a full site failure.&nbsp; In certain cases replication/DR between racks within a single site can also make sense.</p>
        </div>

        <h4>Protection Domain (PD)</h4>

        <ul>
        	<li>Key Role: Macro group of VMs and/or files to protect</li>
        	<li>Description: A group of VMs and/or files to be replicated together on a desired schedule.&nbsp; A PD can protect a full container or you can select individual VMs and/or files</li>
        	<li>(e.g.,</li>
        </ul>

        <div  data-type="note" class="note" id="pro-tip-mrLCd"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Create multiple PDs for various services tiers driven by a desired RPO/RTO.&nbsp; For file distribution (e.g. golden images, ISOs, etc.) you can create a PD with the files to replication.</p>
        </div>

        <h4>Consistency Group (CG)</h4>

        <ul>
        	<li>Key Role: Subset of VMs/files in PD to be crash-consistent</li>
        	<li>Description: VMs and/or files which are part of a Protection Domain which need to be snapshotted in a crash-consistent manner.&nbsp; This ensures that when VMs/files are recovered, they come up in a consistent state.&nbsp; A protection domain can have multiple consistency groups.</li>
        	<li>Group-dependent(e.g.,</li>
        </ul>

        <div  data-type="note" class="note" id="pro-tip-KbBH8"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Group dependent application or service VMs in a consistency group to ensure they are recovered in a consistent state (e.g. App and DB)</p>
        </div>

        <h4>Replication Schedule</h4>

        <ul>
        	<li>Key Role: Snapshot and replication schedule</li>
        	<li>Description: Snapshot and replication schedule for VMs in a particular PD and CG</li>
        </ul>

        <div  data-type="note" class="note" id="pro-tip-GMJHx"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>The snapshot schedule should be equal to your desired RPO</p>
        </div>

        <h4>Retention Policy</h4>

        <ul>
        	<li>Key Role: Number of local and remote snapshots to keep</li>
        	<li>Description: The retention policy defines the number of local and remote snapshots to retain.&nbsp; NOTE: A remote site must be configured for a remote retention/replication policy to be configured.</li>
        	<li>VM/file.</li>
        </ul>

        <div  data-type="note" class="note" id="pro-tip-yVJFj"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>The retention policy should equal the number of restore points required per VM/file</p>
        </div>

        <p>The following figure shows a logical representation of the relationship between a PD, CG, and VM/Files for a single site:</p>

        <figure id="id-rGPUA"><img alt="" class="iimagesv2dr_2png" src="imagesv2/dr_2.png">
        <figcaption><span class="label">Figure 9-37. </span>DR Contrsuct Mapping</figcaption>
        </figure>

        <p>It’s important to mention that a full container can be protected for simplicity; however the platform provides the ability to protect down to the granularity of a single VM and/or file level.</p>

        <h3>Replication Lifecycle</h3>

        <p>Nutanix replication leverages the Cerebro service mentioned above.&nbsp; The Cerebro service is broken into a “Cerebro Master”, which is a dynamically elected CVM, and Cerebro Slaves, which run on every CVM.&nbsp; In the event where the CVM acting as the “Cerebro Master” fails, a new “Master” is elected.</p>

        <p>The Cerebro Master is responsible for managing task delegation to the local Cerebro Slaves as well as coordinating with remote Cerebro Master(s) when remote replication is occurring.</p>

        <p>During a replication, the Cerebro Master will figure out which data needs to be replicated, and delegate the replication tasks to the Cerebro Slaves which will then tell Stargate which data to replicate and to where.</p>

        <p>The following figure shows a representation of this architecture:</p>

        <figure id="id-oypho"><img alt="" class="iimagesv2dr_3png" src="imagesv2/dr_3.png">
        <figcaption><span class="label">Figure 9-38. </span>Replication Architecture</figcaption>
        </figure>

        <p>It is also possible to configure a remote site with a proxy which will be used as a bridgehead for all coordination and replication traffic coming from a cluster.</p>

        <div  data-type="note" class="note" id="pro-tip-lOQUQ"><h6>Note</h6>
        <h1>Pro Tip</h1>

        <p>When using a remote site configured with a proxy, always utilize the cluster IP as that will always be hosted by the Prism Leader and available, even if CVM(s) go down.</p>
        </div>

        <p>The following figure shows a representation of the replication architecture using a proxy:</p>

        <figure id="id-NQmfG"><img alt="" class="iimagesv2dr_4png" src="imagesv2/dr_4.png">
        <figcaption><span class="label">Figure 9-39. </span>Replication Architecture - Proxy</figcaption>
        </figure>

        <p>In certain scenarios, it is also possible to configure a remote site using a SSH tunnel where all traffic will flow between two CVMs.</p>

        <div  data-type="note" class="note" id="note-7gGfo"><h6>Note</h6>
        <h1>Note</h1>
        <p>This should only be used for non-production scenarios and the cluster IPs should be used to ensure availability.</p>
        </div>

        <p>The following figure shows a representation of the replication architecture using a SSH tunnel:</p>

        <figure id="id-1w4cA"><img alt="" class="iimagesv2dr_5png" src="imagesv2/dr_5.png">
        <figcaption><span class="label">Figure 9-40. </span>Replication Architecture - SSH Tunnel</figcaption>
        </figure>

        <h3>Global Deduplication</h3>

        <p>As explained in the Elastic Deduplication Engine section above, DSF has the ability to deduplicate data by just updating metadata pointers. The same concept is applied to the DR and replication feature.&nbsp; Before sending data over the wire, DSF will query the remote site and check whether or not the fingerprint(s) already exist on the target (meaning the data already exists).&nbsp; If so, no data will be shipped over the wire and only a metadata update will occur. For data which doesn’t exist on the target, the data will be compressed and sent to the target site.&nbsp; At this point, the data exists on both sites is usable for deduplication.</p>

        <p>The following figure shows an example three site deployment where each site contains one of more protection domains (PD):</p>

        <figure id="id-84jiW"><img alt="" class="iimagesv2dr_6png" src="imagesv2/dr_6.png">
        <figcaption><span class="label">Figure 9-41. </span>Replication Deduplication</figcaption>
        </figure>

        <div  data-type="note" class="note" id="note-7gnso"><h6>Note</h6>
        <h1>Note</h1>

        <p>Fingerprinting must be enabled on the source and target container / vstore for replication deduplication to occur.</p>
        </div>
        </section>

        <section data-type="sect1" id="cloud-connect-rGxtb">
        <h2>Cloud Connect</h2>

        <p>Building upon the native DR / replication capabilities of DSF, Cloud Connect extends this capability into cloud providers (currently Amazon Web Services, or AWS).&nbsp; NOTE: This feature is currently limited to just backup / replication.</p>

        <p>Very similar to creating a remote site to be used for native DR / replication, a “cloud remote site” is just created.&nbsp; When a new cloud remote site is created, Nutanix will automatically spin up an instance in EC2 (currently m1.xlarge) to be used as the endpoint.</p>

        <p>The Amazon Machine Image (AMI) running in AWS is based upon the same NOS code-base leveraged for locally running clusters.&nbsp; This means that all of the native replication capabilities (e.g., global deduplication, delta based replications, etc.) can be leveraged.</p>

        <p>In the case where multiple Nutanix clusters are leveraging Cloud Connect, they can either A) share the same AMI instance running in the region, or B) spin up a new instance.</p>

        <p>The following figure shows a logical representation of an AWS based “remote site” used for Cloud Connect:</p>

        <figure id="id-AlYCY"><img alt="" class="iimagesv2cloudconn_1png" src="imagesv2/cloudconn_1.png">
        <figcaption><span class="label">Figure 9-42. </span>Cloud Connect Region</figcaption>
        </figure>

        <p>Since an AWS based remote site is similar to any other Nutanix remote site, a cluster can replicate to multiple regions if higher availability is required (e.g., data availability in the case of a full region outage):</p>

        <figure id="id-DY1FK"><img alt="" class="iimagesv2cloudconn_2png" src="imagesv2/cloudconn_2.png">
        <figcaption><span class="label">Figure 9-43. </span>Cloud Connect Multi-region</figcaption>
        </figure>

        <p>The same replication / retention policies are leveraged for data replicated using Cloud Connect.&nbsp; As data / snapshots become stale, or expire, the Nutanix CVM in AWS will clean up data as necessary.</p>

        <p>If replication isn’t frequently occurring (e.g., daily or weekly), the platform can be configured to power up the AWS CVM(s) prior to a scheduled replication and down after a replication has completed.</p>

        <p>Data that is replicated to any AWS region can also be pulled down and restored to any existing, or newly created Nutanix cluster which has the AWS remote site(s) configured:</p>

        <figure id="id-zLLum"><img alt="" class="iimagesv2cloudconn_3png" src="imagesv2/cloudconn_3.png">
        <figcaption><span class="label">Figure 9-44. </span>Cloud Connect Restore</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="metro-availability-dPMtp">
        <h2>Metro Availability</h2>

        <p>Nutanix provides native “stretch clustering” capabilities which allow for a compute and storage cluster to span multiple physical sites.&nbsp; In these deployments, the compute cluster spans two locations and has access to a shared pool of storage.</p>

        <p>This expands the VM HA domain from a single site to between two sites providing a near 0 RTO and a RPO of 0.</p>

        <p>In this deployment, each site has its own Nutanix cluster, however the containers are “stretched” by synchronously replicating to the remote site before acknowledging writes.</p>

        <p>The following figure shows a high-level design of what this architecture looks like:</p>

        <figure id="id-jz1FG"><img alt="" class="iimagesv2metro_1png" src="imagesv2/metro_1.png">
        <figcaption><span class="label">Figure 9-45. </span>Metro Availability - Normal State</figcaption>
        </figure>

        <p>In the event of a site failure, an HA event will occur where the VMs can be restarted on the other site.</p>

        <p>The following figure shows an example site failure:</p>

        <figure id="id-qBluD"><img alt="" class="iimagesv2metro_2png" src="imagesv2/metro_2.png">
        <figcaption><span class="label">Figure 9-46. </span>Metro Availability - Site Failure</figcaption>
        </figure>

        <p>In the event where there is a link failure between the two sites, each cluster will operate independently.&nbsp; Once the link comes back up, the sites will be re-synchronized and synchronous replication will start occurring.</p>

        <p>The following figure shows an example link failure:</p>

        <figure id="id-nk0ik"><img alt="" class="iimagesv2metro_3png" src="imagesv2/metro_3.png">
        <figcaption><span class="label">Figure 9-47. </span>
        <p>Metro Availability - Link Failure</p>
        </figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="volumes-api-Zxdsd">
        <h2>Volumes API</h2>

        <p>The Acropolis Volumes API exposes back-end DSF storage to external consumers (guest OS, physical hosts, containers, etc.) via iSCSI (today).</p>

        <p>This allows any operating system to access DSF and leverage its storage capabilities.&nbsp; In this deployment scenario, the OS is talking directly to Nutanix bypassing any hypervisor.&nbsp;</p>

        <p>Core use-cases for the Volumes API:</p>

        <ul>
        	<li>Shared Disks
        	<ul>
        		<li>Oracle RAC, Microsoft Failover Clustering, etc.</li>
        	</ul>
        	</li>
        	<li>Disks as first-class entities
        	<ul>
        		<li>Where execution contexts are ephemeral and data is critical</li>
        		<li>Containers, OpenStack, etc.</li>
        	</ul>
        	</li>
        	<li>Guest-initiated iSCSI
        	<ul>
        		<li>Bare-metal consumers</li>
        		<li>Exchange on vSphere (for Microsoft Support)</li>
        	</ul>
        	</li>
        </ul>

        <p>The following entities compose the volumes API:</p>

        <ul>
        	<li><strong>Volume Group:</strong> iSCSI target and group of disk devices allowing for centralized management, snapshotting, and policy application</li>
        	<li><strong>Disks:</strong> Storage devices in the Volume Group (seen as LUNs for the iSCSI target)</li>
        	<li><strong>Attachment:</strong> Allowing a specified initiator IQN access to the volume group</li>
        </ul>

        <p>NOTE: On the backend, a VG’s disk is just a vDisk on DSF.</p>

        <p>To use the Volumes API, the following process is leveraged:</p>

        <ol>
        	<li>Create new Volume Group</li>
        	<li>Add disk(s) to Volume Group</li>
        	<li>Attach an initiator IQN to the Volume Group</li>
        </ol>

        <div data-type="example" id="create-volume-group-oobh7">
        <h5><span class="label">Example 9-1. </span>Create Volume Group</h5>

        <p>Create VG</p>

        <p class="codetext">vg.create &lt;VG Name&gt;</p>

        <p>Add disk(s) to VG</p>

        <p class="codetext">Vg.disk_create &lt;VG Name&gt; container=&lt;CTR Name&gt; create_size=&lt;Disk size, e.g. 500G&gt;</p>

        <p>Attach initiator IQN to VG</p>

        <p class="codetext">Vg.attach_external &lt;VG Name&gt; &lt;Initiator IQN&gt;</p>
        </div>

        <p>The following figure shows an example with a VM running on Nutanix, with its OS hosted on the normal Nutanix storage, mounting the volumes directly:</p>

        <figure id="id-no0fk"><img alt="" class="iimagesv2volapi_1png" src="imagesv2/volapi_1.png">
        <figcaption><span class="label">Figure 9-48. </span>Volume API - Example</figcaption>
        </figure>

        <p>In Windows deployments, iSCSI multi-pathing can be configured leveraging the Windows MPIO feature.&nbsp; It is recommended to leverage the ‘Failover only’ policy (default) to ensure vDisk ownership doesn’t change.</p>

        <figure id="id-dmLFd"><img alt="" class="iimagesv2volapi_2png" src="imagesv2/volapi_2.png">
        <figcaption><span class="label">Figure 9-49. </span>MPIO Example - Normal State</figcaption>
        </figure>

        <p>In the event there are multiple disk devices, each disk will have an active path to the local CVM:</p>

        <figure id="id-1PMTA"><img alt="" class="iimagesv2volapi_3png" src="imagesv2/volapi_3.png">
        <figcaption><span class="label">Figure 9-50. </span>MPIO Example - Multi-disk</figcaption>
        </figure>

        <p>In the event where the active CVM goes down, another path would become active and I/Os would resume:</p>

        <figure id="id-n1aTk"><img alt="" class="iimagesv2volapi_4png" src="imagesv2/volapi_4.png">
        <figcaption><span class="label">Figure 9-51. </span>MPIO Example - Path Failure</figcaption>
        </figure>

        <p>In our testing, we’ve seen MPIO to take ~15-16 seconds to complete, which is within the Windows disk I/O timeout (default is 60 seconds).</p>

        <p>If RAID or LVM is desired, the attached disk devices can be put into a dynamic or logical disk:</p>

        <figure id="id-BzBIp"><img alt="" class="iimagesv2volapi_5png" src="imagesv2/volapi_5.png">
        <figcaption><span class="label">Figure 9-52. </span>RAID / LVM Example - Single-path</figcaption>
        </figure>

        <p>In the event where the local CVM is under heavy utilization, it is possible to have active paths to other CVMs.&nbsp; This will balance the I/O load across multiple CVMs, however will take a hit by having to traverse the network for the primary I/O:</p>

        <figure id="id-Q27SL"><img alt="" class="iimagesv2volapi_6png" src="imagesv2/volapi_6.png">
        <figcaption><span class="label">Figure 9-53. </span>RAID / LVM Example - Multi-path</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="networking-and-io-OObcP">
        <h2>Networking and I/O</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/Bz37Eu_TgxY">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//Bz37Eu_TgxY"></iframe></div>

        <p>The Nutanix platform does not leverage any backplane for inter-node communication and only relies on a standard 10GbE network.&nbsp; All storage I/O for VMs running on a Nutanix node is handled by the hypervisor on a dedicated private network.&nbsp; The I/O request will be handled by the hypervisor, which will then forward the request to the private IP on the local CVM.&nbsp; The CVM will then perform the remote replication with other Nutanix nodes using its external IP over the public 10GbE network. For all read requests, these will be served completely locally in most cases and never touch the 10GbE network. This means that the only traffic touching the public 10GbE network will be DSF remote replication traffic and VM network I/O.&nbsp; There will, however, be cases where the CVM will forward requests to other CVMs in the cluster in the case of a CVM being down or data being remote.&nbsp; Also, cluster-wide tasks, such as disk balancing, will temporarily generate I/O on the 10GbE network.</p>

        <p>The following figure shows an example of how the VM’s I/O path interacts with the private and public 10GbE network:</p>

        <figure id="id-kNZUp"><img alt="" class="iimagesv2net_iopng" src="imagesv2/net_io.png">
        <figcaption><span class="label">Figure 9-54. </span>DSF Networking</figcaption>
        </figure>

        <p>&nbsp;</p>
        </section>

        <section data-type="sect1" id="data-locality-n17FV">
        <h2>Data Locality</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/ocLD5nBbUTU">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//ocLD5nBbUTU"></iframe></div>

        <p>Being a converged (compute+storage) platform, I/O and data locality are critical to cluster and VM performance with Nutanix.&nbsp; As explained above in the I/O path, all read/write IOs are served by the local Controller VM (CVM) which is on each hypervisor adjacent to normal VMs.&nbsp; A VM’s data is served locally from the CVM and sits on local disks under the CVM’s control.&nbsp; When a VM is moved from one hypervisor node to another (or during a HA event), the newly migrated VM’s data will be served by the now local CVM. When reading old data (stored on the now remote node/CVM), the I/O will be forwarded by the local CVM to the remote CVM.&nbsp; All write I/Os will occur locally right away.&nbsp; DSF will detect the I/Os are occurring from a different node and will migrate the data locally in the background, allowing for all read I/Os to now be served locally.&nbsp; The data will only be migrated on a read as to not flood the network.</p>

        <p>The following figure shows an example of how data will “follow” the VM as it moves between hypervisor nodes:</p>

        <figure id="id-OOAHY"><img alt="" class="iimagesv2data_locality2png" src="imagesv2/data_locality2.png">
        <figcaption><span class="label">Figure 9-55. </span>Data Locality</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="shadow-clones-KmPIo">
        <h2>Shadow Clones</h2>

        <p>For a visual explanation, you can watch the following video: <a href="https://youtu.be/oqfFDMYQFJg">LINK</a></p>

        <div class="embed-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//oqfFDMYQFJg"></iframe></div>

        <p>The Acropolis Distributed Storage Fabric has a feature called ‘Shadow Clones’, which allows for distributed caching of particular vDisks or VM data which is in a ‘multi-reader’ scenario.&nbsp; A great example of this is during a VDI deployment many ‘linked clones’ will be forwarding read requests to a central master or ‘Base VM’.&nbsp; In the case of VMware View, this is called the replica disk and is read by all linked clones, and in XenDesktop, this is called the MCS Master VM.&nbsp; This will also work in any scenario which may be a multi-reader scenario (e.g., deployment servers, repositories, etc.). Data or I/O locality is critical for the highest possible VM performance and a key struct of DSF.&nbsp;</p>

        <p>With Shadow Clones, DSF will monitor vDisk access trends similar to what it does for data locality.&nbsp; However, in the case there are requests occurring from more than two remote CVMs (as well as the local CVM), and all of the requests are read I/O, the vDisk will be marked as immutable.&nbsp; Once the disk has been marked as immutable, the vDisk can then be cached locally by each CVM making read requests to it (aka Shadow Clones of the base vDisk). This will allow VMs on each node to read the Base VM’s vDisk locally. In the case of VDI, this means the replica disk can be cached by each node and all read requests for the base will be served locally.&nbsp; NOTE:&nbsp; The data will only be migrated on a read as to not flood the network and allow for efficient cache utilization.&nbsp; In the case where the Base VM is modified, the Shadow Clones will be dropped and the process will start over.&nbsp; Shadow clones are enabled by default (as of 4.0.2) and can be enabled/disabled using the following NCLI command: ncli cluster edit-params enable-shadow-clones=&lt;true/false&gt;.</p>

        <p>The following figure shows an example of how Shadow Clones work and allow for distributed caching:</p>

        <figure id="id-nWask"><img alt="" class="iimagesv2shadow_clonepng" src="imagesv2/shadow_clone.png">
        <figcaption><span class="label">Figure 9-56. </span>Shadow Clones</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="storage-layers-and-monitoring-9lJcg">
        <h2>Storage Layers and Monitoring</h2>

        <p>The Nutanix platform monitors storage at multiple layers throughout the stack, ranging from the VM/Guest OS all the way down to the physical disk devices.&nbsp; Knowing the various tiers and how these relate is important whenever monitoring the solution and allows you to get full visibility of how the ops relate. The following figure shows the various layers of where operations are monitored and the relative granularity which are explained below:</p>

        <figure id="id-MeMcz"><img alt="" class="iimagesv2storage_layerspng" src="imagesv2/storage_layers.png">
        <figcaption><span class="label">Figure 9-57. </span>Storage Layers</figcaption>
        </figure>

        <p>&nbsp;</p>

        <h3>Virtual Machine Layer</h3>

        <ul>
        	<li>Key Role: Metrics reported by the hypervisor for the VM</li>
        	<li>Description: Virtual Machine or guest level metrics are pulled directly from the hypervisor and represent the performance the VM is seeing and is indicative of the I/O performance the application is seeing.</li>
        	<li>When to use: When troubleshooting or looking for VM level detail</li>
        </ul>

        <h3>Hypervisor Layer</h3>

        <ul>
        	<li>Key Role: Metrics reported by the Hypervisor(s)</li>
        	<li>Description: Hypervisor level metrics are pulled directly from the hypervisor and represent the most accurate metrics the hypervisor(s) are seeing.&nbsp; This data can be viewed for one of more hypervisor node(s) or the aggregate cluster.&nbsp; This layer will provide the most accurate data in terms of what performance the platform is seeing and should be leveraged in most cases.&nbsp; In certain scenarios the hypervisor may combine or split operations coming from VMs which can show the difference in metrics reported by the VM and hypervisor.&nbsp; These numbers will also include cache hits served by the Nutanix CVMs.</li>
        	<li>When to use: Most common cases as this will provide the most detailed and valuable metrics.</li>
        </ul>

        <h3>Controller Layer</h3>

        <ul>
        	<li>Key Role: Metrics reported by the Nutanix Controller(s)</li>
        	<li>Description: Controller level metrics are pulled directly from the Nutanix Controller VMs (e.g.,&nbsp;Stargate 2009 page) and represent what the Nutanix front-end is seeing from NFS/SMB/iSCSI or any back-end operations (e.g., ILM, disk balancing, etc.).&nbsp; This data can be viewed for one of more Controller VM(s) or the aggregate cluster.&nbsp; The metrics seen by the Controller Layer should normally match those seen by the hypervisor layer, however will include any backend operations (e.g., ILM, disk balancing). These numbers will also include cache hits served by memory.&nbsp; In certain cases, metrics like (IOPS), might not match as the NFS / SMB / iSCSI client might split a large IO into multiple smaller IOPS.&nbsp; However, metrics like bandwidth should match.</li>
        	<li>When to use: Similar to the hypervisor layer, can be used to show how much backend operation is taking place.</li>
        </ul>

        <h3>Disk Layer</h3>

        <ul>
        	<li>Key Role: Metrics reported by the Disk Device(s)</li>
        	<li>Description: Disk level metrics are pulled directly from the physical disk devices (via the CVM) and represent what the back-end is seeing.&nbsp; This includes data hitting the OpLog or Extent Store where an I/O is performed on the disk.&nbsp; This data can be viewed for one of more disk(s), the disk(s) for a particular node, or the aggregate disks in the cluster.&nbsp; In common cases, it is expected that the disk ops should match the number of incoming writes as well as reads not served from the memory portion of the cache.&nbsp; Any reads being served by the memory portion of the cache will not be counted here as the op is not hitting the disk device.</li>
        	<li>When to use: When looking to see how many ops are served from cache or hitting the disksm</li>
        </ul>
        </section>
        </section>

        <section data-type="chapter" id="application-mobility-fabric-coming-soon-Me4Iq">
        <h2>Application Mobility Fabric - coming soon!</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="chapter" id="hypervisor-QwvHl">
        <h2>Hypervisor</h2>

        <section data-type="sect1" id="node-architecture-KAPho">
        <h2>Node Architecture</h2>

        <p>In Acropolis Hypervisor deployments, the Controller VM (CVM) runs as a VM and disks are presented using PCI passthrough.&nbsp; This allows the full PCI controller (and attached devices) to be passed through directly to the CVM and bypass the hypervisor.&nbsp; Acropolis Hypervisor is based upon CentOS KVM.</p>

        <figure id="id-gQnIJ"><img alt="" class="iimagesv2acrop_nodepng" src="imagesv2/acrop_node.png">
        <figcaption><span class="label">Figure 11-1. </span>Acropolis Hypervisor Node</figcaption>
        </figure>

        <p>The Acropolis Hypervisor is built upon the CentOS KVM foundation and extends its base functionality to include features like HA, live migration, etc.&nbsp;</p>

        <p>Acropolis Hypervisor is validated as part of the Microsoft Server Virtualization Validation Program and is validated to run Microsoft OS and applications.</p>
        </section>

        <section data-type="sect1" id="kvm-architecture-DxPIl">
        <h2>KVM Architecture</h2>

        <p>Within KVM there are a few main components:</p>

        <ul>
        	<li>KVM-kmod
        	<ul>
        		<li>KVM kernel module</li>
        	</ul>
        	</li>
        	<li>Libvirtd
        	<ul>
        		<li>An API, daemon and management tool for managing KVM and QEMU.&nbsp; Communication between Acropolis and KVM / QEMU occurs through libvirtd.</li>
        	</ul>
        	</li>
        	<li>Qemu-kvm
        	<ul>
        		<li>A machine emulator and virtualizer that runs in userspace for every Virtual Machine (domain).&nbsp; In the Acropolis Hypervisor it is used for hardware-assisted virtualization and VMs run as HVMs.</li>
        	</ul>
        	</li>
        </ul>

        <p>The following figure shows the relationship between the various components:</p>

        <figure id="id-NxaIG"><img alt="" class="iimagesv2kvm_overviewpng" src="imagesv2/kvm_overview.png">
        <figcaption><span class="label">Figure 11-2. </span>KVM Component Relationship</figcaption>
        </figure>

        <p>Communication between Acropolis and KVM occurs via Libvirt.&nbsp;</p>
        </section>

        <section data-type="sect1" id="configuration-maximums-and-scalability-zyySo">
        <h2>Configuration Maximums and Scalability</h2>

        <p>The following configuration maximums and scalability limits are applicable:</p>

        <ul>
        	<li>Maximum cluster size: <strong>N/A – same as Nutanix cluster size</strong></li>
        	<li>Maximum vCPUs per VM: <strong>Number of physical cores per host</strong></li>
        	<li>Maximum memory per VM: <strong>2TB</strong></li>
        	<li>Maximum VMs per host: <strong>N/A – Limited by memory</strong></li>
        	<li>Maximum VMs per cluster: <strong>N/A – Limited by memory</strong></li>
        </ul>
        </section>

        <section data-type="sect1" id="networking-qLgu2">
        <h2>Networking</h2>

        <p>Acropolis Hypervisor leverages Open vSwitch (OVS) for all VM networking.&nbsp; VM networking is configured through Prism / ACLI and each VM nic is connected into a tap interface.</p>

        <p>The following figure shows a conceptual diagram of the OVS architecture:</p>

        <figure id="id-9a0Cm"><img alt="" class="iimagesv2acrop_netpng" src="imagesv2/acrop_net.png">
        <figcaption><span class="label">Figure 11-3. </span>Open vSwitch Network Overview</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="how-it-works-8KNc2">
        <h2>How It Works</h2>

        <section data-type="sect2" id="iscsi-multi-pathing-dDWtp">
        <h3>iSCSI Multi-pathing</h3>

        <p>On each KVM host there is a iSCSI redirector daemon running which checks Stargate health throughout the cluster using NOP OUT commands.</p>

        <p>QEMU is configured with the iSCSI redirector as the iSCSI target portal.&nbsp; Upon a login request, the redirector will perform and iSCSI login redirect to a healthy Stargate (preferably the local one).</p>

        <figure id="id-OZdfY"><img alt="" class="iimagesv2iscsi_mp_1png" src="imagesv2/iscsi_mp_1.png">
        <figcaption><span class="label">Figure 11-4. </span>iSCSI Multi-pathing - Normal State</figcaption>
        </figure>

        <p>In the event where the active Stargate goes down (thus failing to respond to the NOP OUT command), the iSCSI redirector will mark the local Stargate as unhealthy.&nbsp; When QEMU retries the iSCSI login, the redirector will redirect the login to another healthy Stargate.</p>

        <figure id="id-p7LIp"><img alt="" class="iimagesv2iscsi_mp_2png" src="imagesv2/iscsi_mp_2.png">
        <figcaption><span class="label">Figure 11-5. </span>iSCSI Multi-pathing - Local CVM Down</figcaption>
        </figure>

        <p>Once the local Stargate comes back up (and begins responding to the NOP OUT commands), the iSCSI redirector will perform a TCP kill to kill all connections to remote Stargates.&nbsp; QEMU will then attempt an iSCSI login again and will be redirected to the local Stargate.</p>

        <figure id="id-1gjiA"><img alt="" class="iimagesv2iscsi_mp_3png" src="imagesv2/iscsi_mp_3.png">
        <figcaption><span class="label">Figure 11-6. </span>iSCSI Multi-pathing - Local CVM Back Up</figcaption>
        </figure>
        </section>

        <section data-type="sect2" id="ip-address-management-DkjFl">
        <h3>IP Address Management</h3>

        <p>The Acropolis IP address management (IPAM) solution provides the ability to establish a DHCP scope and assign addresses to VMs.&nbsp; This leverages VXLAN and OpenFlow rules to intercept the DHCP request and respond with a DHCP response.</p>

        <p>Here we show an example DHCP request using the Nutanix IPAM solution where the Acropolis Master is running locally:</p>

        <figure id="id-md4Cy"><img alt="" class="iimagesv2acrop_ipam_1png" src="imagesv2/acrop_ipam_1.png">
        <figcaption><span class="label">Figure 11-7. </span>IPAM - Local Acropolis Master</figcaption>
        </figure>

        <p>If the Acropolis Master is running remotely, the same VXLAN tunnel will be leveraged to handle the request over the network.&nbsp;</p>

        <figure id="id-77EtV"><img alt="" class="iimagesv2acrop_ipam_2png" src="imagesv2/acrop_ipam_2.png">
        <figcaption><span class="label">Figure 11-8. </span>IPAM - Remote Acropolis Master</figcaption>
        </figure>

        <p>Traditional DHCP / IPAM solutions can also be leveraged in an ‘unmanaged’ network scenario.</p>
        </section>
        </section>

        <section data-type="sect1" id="administration-Pm8fr">
        <h2>Administration</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="important-pages-jXjFJ">
        <h2>Important Pages</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="command-reference-oMmTL">
        <h2>Command Reference</h2>

        <h3>Enable 10GbE links only on OVS</h3>

         <p class="codedescription">Description: Enable 10g only on bond0</p>

        <p class="codetext">manage_ovs --interfaces 10g update_uplinks
        <br/>
        allssh “source /etc/profile &gt; /dev/null 2&gt;&amp;1; manage_ovs --interfaces 10g update_uplinks”</p>

        <h3>Show OVS uplinks</h3>

         <p class="codedescription">Description: Show ovs uplinks</p>

        <p class="codetext">manage_ovs show_uplinks</p>

         <p class="codedescription">Description: Show ovs uplinks for full cluster</p>

        <p class="codetext">allssh “source /etc/profile &gt; /dev/null 2&gt;&amp;1; manage_ovs show_uplinks”</p>

        <h3>Show OVS interfaces</h3>

         <p class="codedescription">Description: Show ovs interfaces</p>

        <p class="codetext">manage_ovs show_interfaces</p>

        <p>Show interfaces for full cluster</p>

        <p class="codetext">allssh “source /etc/profile &gt; /dev/null 2&gt;&amp;1; manage_ovs show_interfaces”</p>

        <h3>Show OVS switch information</h3>

         <p class="codedescription">Description: Show switch information</p>

        <p class="codetext">ovs-vsctl show</p>

        <h3>List OVS bridges</h3>

         <p class="codedescription">Description: List bridges</p>

        <p class="codetext">ovs-vsctl list br</p>

        <h3>Show OVS bridge information</h3>

         <p class="codedescription">Description: Show OVS port information</p>

        <p class="codetext">ovs-vsctl list port br0
        <br/>
        ovs-vsctl list port &lt;bond&gt;</p>

        <h3>Show OVS interface information</h3>

         <p class="codedescription">Description: Show interface information</p>

        <p class="codetext">ovs-vsctl list interface br0</p>

        <h3>Show ports / interfaces on bridge</h3>

        <p class="codedescription">Description: Show ports on a bridge</p>

        <p class="codetext">ovs-vsctl list-ports br0</p>

        <p class="codedescription">Description: Show ifaces on a bridge</p>

        <p class="codetext">ovs-vsctl list-ifaces br0</p>

        <h3>Create OVS bridge</h3>

        <p class="codedescription">Description: Create bridge</p>

        <p class="codetext">ovs-vsctl add-br &lt;bridge&gt;</p>

        <h3>Add ports to bridge</h3>

         <p class="codedescription">Description: Add port to bridge</p>

        <p class="codetext">ovs-vsctl add-port &lt;bridge&gt; &lt;port&gt;</p>

        <p class="codedescription">Description: Add bond port to bridge</p>

        <p class="codetext">ovs-vsctl add-bond &lt;bridge&gt; &lt;port&gt; &lt;iface&gt;</p>

        <h3>Show OVS bond details</h3>

        <p class="codedescription">Description: Show bond details</p>

        <p class="codetext">ovs-appctl bond/show &lt;bond&gt;</p>

        <p>Example:</p>

        <p class="codetext">ovs-appctl bond/show bond0</p>

        <h3>Set bond mode and configure LACP on bond</h3>

        <p class="codedescription">Description: Enable LACP on ports</p>

        <p class="codetext">ovs-vsctl set port &lt;bond&gt; lacp=&lt;active/passive&gt;</p>

        <p class="codedescription">Description: Enable on all hosts for bond0</p>

        <p class="codetext">for i in `hostips`;do echo $i; ssh $i source /etc/profile &gt; /dev/null 2&gt;&amp;1; ovs-vsctl set port bond0 lacp=active;done</p>

        <h3>Show LACP details on bond</h3>

        <p class="codedescription">Description: Show LACP details</p>

        <p class="codetext">ovs-appctl lacp/show &lt;bond&gt;</p>

        <h3>Set bond mode</h3>

        <p class="codedescription">Description: Set bond mode on ports</p>

        <p class="codetext">ovs-vsctl set port &lt;bond&gt; bond_mode=&lt;active-backup, balance-slb, balance-tcp&gt;</p>

        <h3>Show OpenFlow information</h3>

        <p class="codedescription">Description: Show OVS openflow details</p>

        <p class="codetext">ovs-ofctl show br0</p>

        <p class="codedescription">Description: Show OpenFlow rules</p>

        <p class="codetext">ovs-ofctl dump-flows br0</p>

        <h3>Get QEMU PIDs and top information</h3>

        <p class="codedescription">Description: Get QEMU PIDs</p>

        <p class="codetext">ps aux | grep qemu | awk '{print $2}'</p>

        <p class="codedescription">Description: Get top metrics for specific PID</p>

        <p class="codetext">top -p &lt;PID&gt;</p>

        <h3>Get active Stargate for QEMU processes</h3>

        <p class="codedescription">Description: Get active Stargates for storage I/O for each QEMU processes</p>

        <p class="codetext">netstat –np | egrep tcp.*qemu</p>
        </section>

        <section data-type="sect1" id="metrics-and-thresholds-E72IA">
        <h2>Metrics and Thresholds</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="troubleshooting-andamp-advanced-administration-K0mSo">
        <h2>Troubleshooting &amp; Advanced Administration</h2>

        <h3>Check iSCSI Redirector Logs</h3>

        <p class="codedescription">Description: Check iSCSI Redirector Logs for all hosts</p>

        <p class="codetext">for i in `hostips`; do echo $i; ssh root@$i cat /var/log/iscsi_redirector;done</p>

        <p>Example for single host</p>

        <p class="codetext">Ssh root@&lt;HOST IP&gt;
        <br/>
        Cat /var/log/iscsi_redirector</p>

        <h3>Monitor CPU steal (stolen CPU)</h3>

        <p class="codedescription">Description: Monitor CPU steal time (stolen CPU)</p>

        <p>Launch top and look for %st (bold below)</p>

        <p class="codetext">Cpu(s):&nbsp; 0.0%us, 0.0%sy,&nbsp; 0.0%ni, 96.4%id,&nbsp; 0.0%wa,&nbsp; 0.0%hi,&nbsp; 0.1%si,&nbsp; <strong>0.0%st</strong></p>

        <h3>Monitor VM network resource stats</h3>

        <p class="codedescription">Description: Monitor VM resource stats</p>

        <p>Launch virt-top</p>

        <p class="codetext">Virt-top</p>

        <p>Go to networking page</p>

        <p>2 – Networking</p>
        </section>
        </section>

        <section data-type="chapter" id="administration-xWrIv">
        <h2>Administration</h2>

        <section data-type="sect1" id="important-pages-OexCP">
        <h2>Important Pages</h2>

        <p>These are advanced Nutanix pages besides the standard user interface that allow you to monitor detailed stats and metrics.&nbsp; The URLs are formatted in the following way: http://&lt;Nutanix CVM IP/DNS&gt;:&lt;Port/path (mentioned below)&gt;&nbsp; Example: http://MyCVM-A:2009&nbsp; NOTE: if you’re on a different subnet IPtables will need to be disabled on the CVM to access the pages.</p>

        <h3>2009 Page</h3>

        <p>This is a Stargate page used to monitor the back end storage system and should only be used by advanced users.&nbsp; I’ll have a post that explains the 2009 pages and things to look for.</p>

        <h3>2009/latency Page</h3>

        <p>This is a Stargate page used to monitor the back end latency.</p>

        <h3>2009/vdisk_stats Page</h3>

        <p>This is a Stargate page used to show various vDisk stats including histograms of I/O sizes, latency, write hits (e.g., OpLog, eStore), read hits (cache, SSD, HDD, etc.) and more.</p>

        <h3>2009/h/traces Page</h3>

        <p>This is the Stargate page used to monitor activity traces for operations.</p>

        <h3>2009/h/vars Page</h3>

        <p>This is the Stargate page used to monitor various counters.</p>

        <h3>2010 Page</h3>

        <p>This is the Curator page which is used for monitoring Curator runs.</p>

        <h3>2010/master/control Page</h3>

        <p>This is the Curator control page which is used to manually start Curator jobs</p>

        <h3>2011 Page</h3>

        <p>This is the Chronos page which monitors jobs and tasks scheduled by Curator.</p>

        <h3>2020 Page</h3>

        <p>&nbsp;This is the Cerebro page which monitors the protection domains, replication status and DR.</p>

        <h3>2020/h/traces Page</h3>

        <p>This is the Cerebro page used to monitor activity traces for PD operations and replication.</p>

        <h3>2030 Page</h3>

        <p>This is the main Acropolis page and shows details about the environment hosts, any currently running tasks and networking details..</p>

        <h3>2030/sched Page</h3>

        <p>This is an Acropolis page used to show information about VM and resource scheduling used for placement decisions.&nbsp; This page shows the available host resources and VMs running on each host.</p>

        <h3>2030/tasks Page</h3>

        <p>This is an Acropolis page used to show information about Acropolis tasks and their state.&nbsp; You can click on the task UUID to get detailed JSON about the task.</p>

        <h3>2030/vms Page</h3>

        <p>This is an Acropolis page used to show information about Acropolis VMs and details about them.&nbsp; You can click on the VM Name to connect to the console.</p>
        </section>

        <section data-type="sect1" id="cluster-commands-D8wHl">
        <h2>Cluster Commands</h2>

        <h3>Check cluster status</h3>

        <p class="codedescription">Description: Check cluster status from the CLI</p>

        <p class="codetext">cluster status</p>

        <h3>Check local CVM service status</h3>

        <p class="codedescription">Description: Check a single CVM's service status from the CLI</p>

        <p class="codetext">genesis status</p>

        <h3>Nutanix cluster upgrade</h3>

        <p class="codedescription">Description: Perform rolling (aka "live") cluster upgrade from the CLI</p>

        <p>Upload upgrade package to ~/tmp/ on one CVM</p>

        <p>Untar package</p>

        <p class="codetext">tar xzvf ~/tmp/nutanix*</p>

        <p>Perform upgrade</p>

        <p class="codetext">~/tmp/install/bin/cluster -i ~/tmp/install upgrade</p>

        <p>Check status</p>

        <p class="codetext">upgrade_status</p>

        <h3>Hypervisor upgrade status</h3>

        <p class="codedescription">Description: Check hypervisor upgrade status from the CLI on any CVM</p>

        <p class="codetext">host_upgrade --status</p>

        <p>Detailed logs (on every CVM)</p>

        <p class="codetext">~/data/logs/host_upgrade.out</p>

        <h3>Restart cluster service from CLI</h3>

        <p class="codedescription">Description: Restart a single cluster service from the CLI</p>

        <p>Stop service</p>

        <p class="codetext">cluster stop &lt;Service Name&gt;</p>

        <p>Start stopped services</p>

        <p class="codetext">cluster start&nbsp; #NOTE: This will start all stopped services</p>

        <h3>Start cluster service from CLI</h3>

        <p class="codedescription">Description: Start stopped cluster services from the CLI</p>

        <p>Start stopped services</p>

        <p class="codetext">cluster start&nbsp; #NOTE: This will start all stopped services</p>

        <p>OR</p>

        <p>Start single service</p>

        <p class="codetext">Start single service: cluster start&nbsp; &lt;Service Name&gt;</p>

        <h3>Restart local service from CLI</h3>

         <p class="codedescription">Description: Restart a single cluster service from the CLI</p>

        <p>Stop Service</p>

        <p class="codetext">genesis stop &lt;Service Name&gt;</p>

        <p>Start Service</p>

        <p class="codetext">cluster start</p>

        <h3>Start local service from CLI</h3>

        <p class="codedescription">Description: Start stopped cluster services from the CLI</p>

        <p class="codetext">cluster start #NOTE: This will start all stopped services</p>

        <h3>Cluster add node from cmdline</h3>

        <p class="codedescription">Description: Perform cluster add-node from CLI</p>

        <p class="codetext">ncli cluster discover-nodes | egrep "Uuid" | awk '{print $4}' | xargs -I UUID ncli cluster add-node node-uuid=UUID</p>

        <h3>Find number of vDisks</h3>

        <p class="codedescription">Description: Displays the number of vDisks</p>

        <p class="codetext">vdisk_config_printer | grep vdisk_id | wc -l</p>

        <h3>Find cluster id</h3>

        <p class="codedescription">Description: Find the cluster ID for the current cluster</p>

        <p class="codetext">zeus_config_printer | grep cluster_id</p>

        <h3>Open port</h3>

        <p class="codedescription">Description: Enable port through IPtables</p>

        <p class="codetext">sudo vi /etc/sysconfig/iptables
        <br/>
        -A INPUT -m state --state NEW -m tcp -p tcp --dport &lt;PORT&gt; -j ACCEPT
        <br/>
        sudo service iptables restart</p>

        <h3>Check for Shadow Clones</h3>

        <p class="codedescription">Description: Displays the shadow clones in the following format:&nbsp; name#id@svm_id</p>

        <p class="codetext">vdisk_config_printer | grep '#'</p>

        <h3>Reset Latency Page Stats</h3>

        <p class="codedescription">Description: Reset the Latency Page (&lt;CVM IP&gt;:2009/latency) counters</p>

        <p class="codetext">allssh “ wget $i:2009/latency/reset”</p>

        <h3>Find Number of vDisks</h3>

        <p class="codedescription">Description: Find the current number of vDisks (files) on DSF</p>

        <p class="codetext">vdisk_config_printer | grep vdisk_id | wc -l</p>

        <h3>Start Curator scan from CLI</h3>

        <p class="codedescription">Description: Starts a Curator full scan from the CLI</p>

        <p class="codetext">allssh “ wget -O - "http://$i:2010/master/api/client/StartCuratorTasks?task_type=2"; “</p>

        <h3>Compact ring</h3>

        <p class="codedescription">Description: Compact the metadata ring</p>

        <p class="codetext">allssh “nodetool -h localhost compact”</p>

        <h3>Find NOS version</h3>

        <p class="codedescription">Description: Find the NOS&nbsp; version (NOTE: can also be done using NCLI)</p>

        <p class="codetext">allssh “cat /etc/nutanix/release_version”</p>

        <h3>Find CVM version</h3>

        <p class="codedescription">Description: Find the CVM image version</p>

        <p class="codetext">allssh “cat /etc/nutanix/svm-version”</p>

        <h3>Manually fingerprint vDisk(s)</h3>

         <p class="codedescription">Description: Create fingerprints for a particular vDisk (For dedupe)&nbsp; NOTE: dedupe must be enabled on the container</p>

        <p class="codetext">vdisk_manipulator –vdisk_id=&lt;vDisk ID&gt; --operation=add_fingerprints</p>

        <h3>Echo Factory_Config.json for all cluster nodes</h3>

        <p class="codedescription">Description: Echos the factory_config.jscon for all nodes in the cluster</p>

        <p class="codetext">allssh “cat /etc/nutanix/factory_config.json”</p>

        <h3>Upgrade a single Nutanix node’s NOS version</h3>

        <p class="codedescription">Description: Upgrade a single node's NOS version to match that of the cluster</p>

        <p class="codetext">~/cluster/bin/cluster -u &lt;NEW_NODE_IP&gt; upgrade_node</p>

        <h3>&nbsp;List files (vDisk) on DSF</h3>

        <p class="codedescription">Description: List files and associated information for vDisks stored on DSF</p>

        <p class="codetext">Nfs_ls</p>

        <p>Get help text</p>

        <p class="codetext">Nfs_ls --help</p>

        <h3>Install Nutanix Cluster Check (NCC)</h3>

        <p class="codedescription">Description: Installs the Nutanix Cluster Check (NCC) health script to test for potential issues and cluster health</p>

        <p>Download NCC from the Nutanix Support Portal (portal.nutanix.com)</p>

        <p>SCP .tar.gz to the /home/nutanix directory</p>

        <p>Untar NCC .tar.gz</p>

        <p class="codetext">tar xzmf &lt;ncc .tar.gz file name&gt; --recursive-unlink</p>

        <p>Run install script</p>

        <p class="codetext">./ncc/bin/install.sh -f &lt;ncc .tar.gz file name&gt;</p>

        <p>Create links</p>

        <p class="codetext">source ~/ncc/ncc_completion.bash
        <br/>
        echo "source ~/ncc/ncc_completion.bash" &gt;&gt; ~/.bashrc</p>

        <h3>Run Nutanix Cluster Check (NCC)</h3>

        <p class="codedescription">Description: Runs the Nutanix Cluster Check (NCC) health script to test for potential issues and cluster health.&nbsp; This is a great first step when troubleshooting any cluster issues.</p>

        <p>Make sure NCC is installed (steps above)</p>

        <p>Run NCC health checks</p>

        <p class="codetext">ncc health_checks run_all</p>
        </section>

        <section data-type="sect1" id="metrics-and-thresholds-WQack">
        <h2>Metrics and Thresholds</h2>

        <p>The following section will cover specific metrics and thresholds on the Nutanix back end.&nbsp; More updates to these coming shortly!</p>
        </section>

        <section data-type="sect1" id="gflags-DApul">
        <h2>Gflags</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="troubleshooting-andamp-advanced-administration-J4lFl">
        <h2>Troubleshooting &amp; Advanced Administration</h2>

        <h3>Find Acropolis logs</h3>

        <p class="codedescription">Description: Find Acropolis logs for the cluster</p>

        <p class="codetext">allssh “cat ~/data/logs/Acropolis.log”</p>

        <h3>Find cluster error logs</h3>

        <p class="codedescription">Description: Find ERROR logs for the cluster</p>

        <p class="codetext">allssh "cat ~/data/logs/&lt;COMPONENT NAME or *&gt;.ERROR"</p>

        <p>Example for Stargate</p>

        <p class="codetext">allssh "cat ~/data/logs/Stargate.ERROR"</p>

        <h3>Find cluster fatal logs</h3>

        <p class="codedescription">Description: Find FATAL logs for the cluster</p>

        <p class="codetext">allssh "cat ~/data/logs/&lt;COMPONENT NAME or *&gt;.FATAL"</p>

        <p>Example for Stargate</p>

        <p class="codetext">allssh "cat ~/data/logs/Stargate.FATAL"</p>

        <h3>Using the 2009 Page (Stargate)</h3>

        <p>In most cases Prism should be able to give you all of the information and data points you require. &nbsp;However,&nbsp;in certain scenarios, or if you want some more detailed data you can leverage the Stargate aka 2009 page. &nbsp;The 2009 page can be viewed by navigating to &lt;CVM IP&gt;:2009.</p>

        <div  data-type="note" class="note" id="accessing-back-end-pages-4X9HE"><h6>Note</h6>
        <h1>Accessing back-end pages</h1>

        <p>If you're on a different network segment (L2 subnet)&nbsp;you'll need to add a rule in IP tables to access any of the back-end pages.</p>
        </div>

        <p>At the top of the page is the overview details which show various details about the cluster:</p>

        <figure id="id-9dPSm"><img alt="" class="iimagesv22009pagesstargate_overview2png" src="imagesv2/2009Pages/stargate_overview2.png">
        <figcaption><span class="label">Figure 12-1. </span>2009 Page - Stargate Overview</figcaption>
        </figure>

        <p>In this section there are two key areas I look out for, the first being the I/O queues which shows the number of admitted / outstanding operations.</p>

        <p>The figure shows the queues portion of the overview section:</p>

        <figure id="id-rKmuA"><img alt="" class="iimagesv22009pagesstargate_io_queuespng" src="imagesv2/2009Pages/stargate_io_queues.png">
        <figcaption><span class="label">Figure 12-2. </span>2009 Page - Stargate Overview - Queues</figcaption>
        </figure>

        <p>The second portion is the content cache details which shows information on cache sizes and hit rates.</p>

        <p>The figure shows the content cache portion of the overview section:</p>

        <figure id="id-XbmhA"><img alt="" class="iimagesv22009pagesstargate_contentcache2png" src="imagesv2/2009Pages/stargate_contentCache2.png">
        <figcaption><span class="label">Figure 12-3. </span>2009 Page - Stargate Overview - Content Cache</figcaption>
        </figure>

        <div  data-type="note" class="note" id="pro-tip-gzDFr"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>In ideal cases the hit rates should be above 80-90%+ if the workload is read heavy for the best possible read performance.</p>
        </div>

        <p>NOTE: these values are per Stargate / CVM</p>

        <p>The next section is the 'Cluster State' which shows details on the various Stargates in the cluster and their disk usages.</p>

        <p>The figure shows the Stargates and disk utilization&nbsp;(available/total):</p>

        <figure id="id-vwZc9"><img alt="" class="iimagesv22009pagesstargate_diskutilpng" src="imagesv2/2009Pages/stargate_diskUtil.png">
        <figcaption><span class="label">Figure 12-4. </span>2009 Page - Cluster State - Disk Usage</figcaption>
        </figure>

        <p>The next section is the 'NFS Slave' section which will show various details and stats per vDisk.</p>

        <p>The figure shows the vDisks and various I/O details:</p>

        <figure id="id-mkPHy"><img alt="" class="iimagesv22009pagesstargate_vdiskstatpng" src="imagesv2/2009Pages/stargate_vdiskStat.png">
        <figcaption><span class="label">Figure 12-5. </span>2009 Page - NFS Slave - vDisk Stats</figcaption>
        </figure>

        <div  data-type="note" class="note" id="pro-tip-EMvS1"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>When looking at any potential performance issues I always look at the following:</p>

        <ol>
        	<li>Avg. latency</li>
        	<li>Avg. op size</li>
        	<li>Avg. outstanding</li>
        </ol>

        <p>For more specific details the vdisk_stats page holds a plethora of information.</p>
        </div>

        <h3>Using the 2009/vdisk_stats Page</h3>

        <p>The 2009 vdisk_stats page is a detailed page which provides even further data points per vDisk. &nbsp;This page includes details and a histogram of items like randomness, latency histograms, I/O sizes and working set details.</p>

        <p>You can navigate to the vdisk_stats page by clicking on the 'vDisk Id' in the left hand column.</p>

        <p>The figure shows the section and hyperlinked vDisk Id:</p>

        <figure id="id-GEXio"><img alt="" class="iimagesv22009pagesstargate_hostedvdiskbriefpng" src="imagesv2/2009Pages/stargate_hostedVdiskBrief.png">
        <figcaption><span class="label">Figure 12-6. </span>2009 Page - Hosted vDisks</figcaption>
        </figure>

        <p>This will bring you to the vdisk_stats page which will give you the detailed vDisk stats. &nbsp;NOTE: Theses values are real-time and can be updated by refreshing the page.</p>

        <p>The first key area is the 'Ops and Randomness' section which will show a breakdown of whether the I/O patterns are random or sequential in nature.</p>

        <p>The figure shows the 'Ops and Randomness' section:</p>

        <figure id="id-ZVYfY"><img alt="" class="iimagesv22009pagesstargate_opsrandomnesspng" src="imagesv2/2009Pages/stargate_opsRandomness.png">
        <figcaption><span class="label">Figure 12-7. </span>2009 Page - vDisk Stats - Ops and Randomness</figcaption>
        </figure>

        <p>The next area shows a histogram of the frontend read and write I/O latency&nbsp;(aka the latency the VM / OS sees).</p>

        <p>The figure shows the 'Frontend Read Latency' histogram:</p>

        <figure id="id-wYriV"><img alt="" class="iimagesv22009pagesstargate_readlat_fepng" src="imagesv2/2009Pages/stargate_readLat_FE.png">
        <figcaption><span class="label">Figure 12-8. </span>2009 Page - vDisk Stats - Frontend Read Latency</figcaption>
        </figure>

        <p>The figure shows the 'Frontend Write Latency' histogram:</p>

        <figure id="id-XzmSA"><img alt="" class="iimagesv22009pagesstargate_writelat_fepng" src="imagesv2/2009Pages/stargate_writeLat_FE.png">
        <figcaption><span class="label">Figure 12-9. </span>2009 Page - vDisk Stats - Frontend Write Latency</figcaption>
        </figure>

        <p>The next key area is the I/O size distribution which shows a histogram of the read and write I/O sizes.</p>

        <p>The figure shows the 'Read Size Distribution' histogram:</p>

        <figure id="id-gLgTJ"><img alt="" class="iimagesv22009pagesstargate_readsizepng" src="imagesv2/2009Pages/stargate_readSize.png">
        <figcaption><span class="label">Figure 12-10. </span>2009 Page - vDisk Stats - Read I/O Size</figcaption>
        </figure>

        <p>The figure shows the 'Write Size Distribution'&nbsp;histogram:</p>

        <figure id="id-vNZi9"><img alt="" class="iimagesv22009pagesstargate_writesizepng" src="imagesv2/2009Pages/stargate_writeSize.png">
        <figcaption><span class="label">Figure 12-11. </span>2009 Page - vDisk Stats - Write I/O Size</figcaption>
        </figure>

        <p>The next key area is the 'Working Set Size' section which provides insight on working set sizes for the last 2 minutes and 1 hour. &nbsp;This is broken down for both read and write I/O.</p>

        <p>The figure shows the 'Working Set Sizes' table:</p>

        <figure id="id-mjPIy"><img alt="" class="iimagesv22009pagesstargate_workingsetpng" src="imagesv2/2009Pages/stargate_workingSet.png">
        <figcaption><span class="label">Figure 12-12. </span>2009 Page - vDisk Stats - Working Set</figcaption>
        </figure>

        <p>The 'Read Source' provides details on which tier or location the read I/O are being served from.</p>

        <p>The figure shows the 'Read Source' details:</p>

        <figure id="id-1xwFA"><img alt="" class="iimagesv22009pagesstargate_readsourcepng" src="imagesv2/2009Pages/stargate_readSource.png">
        <figcaption><span class="label">Figure 12-13. </span>2009 Page - vDisk Stats - Read Source</figcaption>
        </figure>

        <div  data-type="note" class="note" id="pro-tip-NBrIz"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>If you're seeing high read latency take a look at the read source for the vDisk and take a look where the I/Os are being served from. &nbsp;In most cases high latency could be caused by reads coming from HDD (Estore HDD).</p>
        </div>

        <p>The 'Write Destination' section will show where the new write I/O are coming in to.</p>

        <p>The figure shows the 'Write Destination' table:</p>

        <figure id="id-dVeTd"><img alt="" class="iimagesv22009pagesstargate_writedestpng" src="imagesv2/2009Pages/stargate_writeDest.png">
        <figcaption><span class="label">Figure 12-14. </span>2009 Page - vDisk Stats - Write Destination</figcaption>
        </figure>

        <div  data-type="note" class="note" id="pro-tip-QEdsV"><h6>Note</h6>
        <h1>Pro tip</h1>

        <p>Random or smaller I/Os (&lt;64K) will be written to the Oplog. &nbsp;Larger or sequential I/Os will bypass the Oplog and be directly written to the Extent Store (Estore).</p>
        </div>

        <p>Another interesting data point is what data is being up-migrated from HDD to SSD via ILM. &nbsp;The 'Extent Group Up-Migration' table shows data that has been up-migrated in the last 300, 3,600 and 86,400 seconds.</p>

        <p>The figure shows the 'Extent Group Up-Migration' table:</p>

        <figure id="id-aZYT8"><img alt="" class="iimagesv22009pagesstargate_egroupilmpng" src="imagesv2/2009Pages/stargate_eGroupILM.png">
        <figcaption><span class="label">Figure 12-15. </span>2009 Page - vDisk Stats - Extent Group Up-Migration</figcaption>
        </figure>

        <h3>Using the 2010 Page (Curator)</h3>

        <p>The 2010 page is a detailed page for monitoring the Curator MapReduce framework. &nbsp;This page provides details on jobs, scans, and associated tasks.&nbsp;</p>

        <p>You can navigate to the Curator page by navigating to http://&lt;CVM IP&gt;:2010. &nbsp;NOTE: if you're not on the Curator Master click on the IP hyperlink after 'Curator Master: '. &nbsp;</p>

        <p>The top of the page will show various details about the Curator Master including uptime, build version, etc.</p>

        <p>The next section is the 'Curator Nodes' table which shows various details about the nodes in the cluster, the roles, and health status. &nbsp;These will be the nodes Curator leverages for the distributed processing and delegation of tasks.</p>

        <p>The figure shows the 'Curator Nodes' table:</p>

        <figure id="id-4MnFA"><img alt="" class="iimagesv22010pagescurator_nodes2png" src="imagesv2/2010Pages/curator_nodes2.png">
        <figcaption><span class="label">Figure 12-16. </span>2010 Page - Curator Nodes</figcaption>
        </figure>

        <p>The next section is the 'Curator Jobs' table which shows the completed or currently running jobs. &nbsp;</p>

        <p>There are two main types of jobs which include a partial scan which is eligible to run every 60 minutes and a full scan which is eligible to run every 6 hours. &nbsp;NOTE: the timing will be variable based upon utilization and other activities.</p>

        <p><span style="letter-spacing: 0.01em; line-height: 1.3em;">These scans will run on their periodic schedules however can also be triggered by certain cluster events.</span></p>

        <p>Here are some of the reasons for a jobs execution:</p>

        <ul>
        	<li>Periodic (normal state)</li>
        	<li>Disk / Node / Block failure</li>
        	<li>ILM Imbalance</li>
        	<li>Disk / Tier Imbalance</li>
        </ul>

        <p>The figure shows the 'Curator Jobs' table:</p>

        <figure class="large" id="id-2ZLS4"><img alt="" class="iimagesv22010pagescurator_jobs2png" src="imagesv2/2010Pages/curator_jobs2.png">
        <figcaption><span class="label">Figure 12-17. </span>2010 Page - Curator Jobs</figcaption>
        </figure>

        <p>The table shows some of the high-level activities performed by each job:</p>

        <table border="1" cellpadding="1" cellspacing="1" style="width: 100%;">
        	<caption>Curator Scan Tasks</caption>
        	<thead>
        		<tr>
        			<th scope="col"><strong>Activity</strong></th>
        			<th scope="col"><strong>Full Scan</strong></th>
        			<th scope="col"><strong>Partial Scan</strong></th>
        		</tr>
        	</thead>
        	<tbody>
        		<tr>
        			<th scope="row">ILM</th>
        			<td>X</td>
        			<td>X</td>
        		</tr>
        		<tr>
        			<th scope="row">Disk Balancing</th>
        			<td>X</td>
        			<td>X</td>
        		</tr>
        		<tr>
        			<th scope="row">Compression</th>
        			<td>X</td>
        			<td>X</td>
        		</tr>
        		<tr>
        			<th scope="row">Deduplication</th>
        			<td>X</td>
        			<td>&nbsp;</td>
        		</tr>
        		<tr>
        			<th scope="row">Erasure Coding</th>
        			<td>X</td>
        			<td>&nbsp;</td>
        		</tr>
        		<tr>
        			<th scope="row">Garbage Cleanup</th>
        			<td>X</td>
        			<td>&nbsp;</td>
        		</tr>
        	</tbody>
        </table>

        <p>Clicking on the 'Execution id' will bring you to the job details page which displays various job stats as well as generated tasks.</p>

        <p>The table at the top of the page will show various details on the job including the type, reason, tasks and duration.</p>

        <p>The next section is the 'Background Task Stats' table which displays various details on the type of tasks, quantity generated and priority.</p>

        <p>The figure shows the job details table:</p>

        <figure id="id-jYNUG"><img alt="" class="iimagesv22010pagesjob_details2png" src="imagesv2/2010Pages/job_details2.png">
        <figcaption><span class="label">Figure 12-18. </span>2010 Page - Curator Job - Details</figcaption>
        </figure>

        <p>The figure shows the 'Background Task Stats' table:</p>

        <figure id="id-gY0fJ"><img alt="" class="iimagesv22010pagesjob_tasks2png" src="imagesv2/2010Pages/job_tasks2.png">
        <figcaption><span class="label">Figure 12-19. </span>2010 Page - Curator Job - Tasks</figcaption>
        </figure>

        <p>The next section is the 'MapReduce Jobs' table which shows the actual MapReduce jobs started by each Curator job. &nbsp;Partial scans will have a single MapReduce Job, full scans will have four MapReduce Jobs.</p>

        <p>The figure shows the 'MapReduce Jobs' table:</p>

        <figure id="id-lYNh1"><img alt="" class="iimagesv22010pagescurator_mrjobs2png" src="imagesv2/2010Pages/curator_mrjobs2.png">
        <figcaption><span class="label">Figure 12-20. </span>2010 Page - MapReduce Jobs</figcaption>
        </figure>

        <p>Clicking on the 'Job id' will bring you to the MapReduce job details page which displays the tasks status,&nbsp;various counters and details about the MapReduce job.</p>

        <p>The figure shows a sample of some of the job counters:</p>

        <figure id="id-dYeUd"><img alt="" class="iimagesv22010pagesjob_counters2png" src="imagesv2/2010Pages/job_counters2.png">
        <figcaption><span class="label">Figure 12-21. </span>2010 Page - MapReduce Job - Counters</figcaption>
        </figure>

        <p>The next section on the main page is the 'Queued Curator Jobs' and 'Last Successful Curator Scans' section. These tables show when the periodic scans are eligible to run and the last successful scan's details.</p>

        <p>The figure shows the&nbsp;'Queued Curator Jobs' and 'Last Successful Curator Scans' section:</p>

        <figure id="id-Y8ySl"><img alt="" class="iimagesv22010pagescurator_queue_lastsuccessful2png" src="imagesv2/2010Pages/curator_queue_lastsuccessful2.png">
        <figcaption><span class="label">Figure 12-22. </span>2010 Page - Queued and Successful Scans</figcaption>
        </figure>
        </section>
        </section>
        </div>

        <div data-type="part" id="book-of-vsphere-6YeSv">
        <h1><span class="label">Part IV. </span>Book of vSphere</h1>

        <section data-type="chapter" id="architecture-RWaFk">
        <h1>Architecture</h1>

        <section data-type="sect1" id="node-architecture-3GWta">
        <h1>Node Architecture</h1>

        <p>In ESXi deployments, the Controller VM (CVM) runs as a VM and disks are presented using VMDirectPath I/O.&nbsp; This allows the full PCI controller (and attached devices) to be passed through directly to the CVM and bypass the hypervisor.</p>

        <figure id="id-RNycG"><img alt="" class="iimagesv2esx_nodepng" src="imagesv2/esx_node.png">
        <figcaption><span class="label">Figure 13-1. </span>ESXi Node Architecture</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="configuration-maximums-and-scalability-6ZBUd">
        <h1>Configuration Maximums and Scalability</h1>

        <p>The following configuration maximums and scalability limits are applicable:</p>

        <ul>
        	<li>Maximum cluster size: <strong>64</strong></li>
        	<li>Maximum vCPUs per VM: <strong>128</strong></li>
        	<li>Maximum memory per VM: <strong>4TB</strong></li>
        	<li>Maximum VMs per host: <strong>1,024</strong></li>
        	<li>Maximum VMs per cluster: <strong>8,000 (2,048 per datastore if HA is enabled)</strong></li>
        </ul>

        <p>NOTE: As of vSphere 6.0</p>
        </section>
        </section>

        <section data-type="chapter" id="how-it-works-5qaC2">
        <h1>How It Works</h1>

        <section data-type="sect1" id="array-offloads-vaai-6YyH1">
        <h2>Array Offloads – VAAI</h2>

        <p>The Nutanix platform supports the VMware APIs for Array Integration (VAAI),&nbsp;which allows the hypervisor to offload certain tasks to the array.&nbsp; This is much more efficient as the hypervisor doesn’t need to be the 'man in the middle'. Nutanix currently supports the VAAI primitives for NAS, including the ‘full file clone’, ‘fast file clone’, and ‘reserve space’ primitives.&nbsp; Here’s a good article explaining the various primitives: http://cormachogan.com/2012/11/08/vaai-comparison-block-versus-nas/.&nbsp;</p>

        <p>For both the full and fast file clones, a DSF 'fast clone' is done, meaning a writable snapshot (using re-direct on write) for each clone that is created.&nbsp; Each of these clones has its own block map, meaning that chain depth isn’t anything to worry about. The following will determine whether or not VAAI will be used for specific scenarios:</p>

        <ul>
        	<li>Clone VM with Snapshot –&gt; VAAI will NOT be used</li>
        	<li>Clone VM without Snapshot which is Powered Off –&gt; VAAI WILL be used</li>
        	<li>Clone VM to a different Datastore/Container –&gt; VAAI will NOT be used</li>
        	<li>Clone VM which is Powered On&nbsp; –&gt; VAAI will NOT be used</li>
        </ul>

        <p>These scenarios apply to VMware View:</p>

        <ul>
        	<li>View Full Clone (Template with Snapshot) –&gt; VAAI will NOT be used</li>
        	<li>View Full Clone (Template w/o Snapshot) –&gt; VAAI WILL be used</li>
        	<li>View Linked Clone (VCAI) –&gt; VAAI WILL be used</li>
        </ul>

        <p>You can validate VAAI operations are taking place by using the ‘NFS Adapter’ Activity Traces page.</p>
        </section>

        <section data-type="sect1" id="cvm-autopathing-aka-hapy-3a0fQ">
        <h2>CVM Autopathing aka Ha.py</h2>

        <p>In this section, I’ll cover how CVM 'failures' are handled (I’ll cover how we handle component failures in future update).&nbsp; A CVM 'failure' could include a user powering down the CVM, a CVM rolling upgrade, or any event which might bring down the CVM. DSF has a feature called autopathing where when a local CVM becomes unavailable, the I/Os are then transparently handled by other CVMs in the cluster. The hypervisor and CVM communicate using a private 192.168.5.0 network on a dedicated vSwitch (more on this above).&nbsp; This means that for all storage I/Os, these are happening to the internal IP addresses on the CVM (192.168.5.2).&nbsp; The external IP address of the CVM is used for remote replication and for CVM communication.</p>

        <p>The following figure shows an example of what this looks like:</p>

        <figure id="id-122tA"><img alt="" class="iimagesv2esx_hapy_1png" src="imagesv2/esx_hapy_1.png">
        <figcaption><span class="label">Figure 14-1. </span>ESXi Host Networking</figcaption>
        </figure>

        <p>In the event of a local CVM failure, the local 192.168.5.2 addresses previously hosted by the local CVM are unavailable.&nbsp; DSF will automatically detect this outage and will redirect these I/Os to another CVM in the cluster over 10GbE.&nbsp; The re-routing is done transparently to the hypervisor and VMs running on the host.&nbsp; This means that even if a CVM is powered down, the VMs will still continue to be able to perform I/Os to DSF.&nbsp; DSF is also self-healing, meaning it will detect the CVM has been powered off and will automatically reboot or power-on the local CVM.&nbsp; Once the local CVM is back up and available, traffic will then seamlessly be transferred back and served by the local CVM.</p>

        <p>The following figure shows a graphical representation of how this looks for a failed CVM:</p>

        <figure id="id-N1yFG"><img alt="" class="iimagesv2esx_hapy_2png" src="imagesv2/esx_hapy_2.png">
        <figcaption><span class="label">Figure 14-2. </span>ESXi Host Networking - Local CVM Down</figcaption>
        </figure>
        </section>
        </section>

        <section data-type="chapter" id="administration-ZoBsd">
        <h1>Administration</h1>

        <section data-type="sect1" id="important-pages-yJDfd">
        <h2>Important Pages</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="command-reference-wbKIx">
        <h2>Command Reference</h2>

        <h3>ESXi cluster upgrade</h3>

         <p class="codedescription">Description: Perform an automated upgrade of ESXi hosts using the CLI</p>

        <p># Upload upgrade offline bundle to a Nutanix NFS container</p>

        <p># Log in to Nutanix CVM</p>

        <p># Perform upgrade</p>

        <p>for i in `hostips`;do echo $i &amp;&amp; ssh root@$i “esxcli software vib install -d /vmfs/volumes/&lt;Datastore Name&gt;/&lt;Offline bundle name&gt;”;done</p>

        <p># Example</p>

        <p>for i in `hostips`;do echo $i &amp;&amp; ssh root@$i “esxcli software vib install -d /vmfs/volumes/NTNX-upgrade/update-from-esxi5.1-5.1_update01.zip”;done</p>

        <p>Performing a rolling reboot of ESXi hosts: For PowerCLI on automated hosts reboots</p>

        <h3>Restart ESXi host services</h3>

         <p class="codedescription">Description: Restart each ESXi hosts services in a incremental manner</p>

        <p>for i in `hostips`;do ssh root@$i "services.sh restart";done</p>

        <h3>Display ESXi host nics in ‘Up’ state</h3>

         <p class="codedescription">Description: Display the ESXi host's nics which are in a 'Up' state</p>

        <p>for i in `hostips`;do echo $i &amp;&amp; ssh root@$i esxcfg-nics -l | grep Up;done</p>

        <h3>Display ESXi host 10GbE nics and status</h3>

         <p class="codedescription">Description: Display the ESXi host's 10GbE nics and status</p>

        <p>for i in `hostips`;do echo $i &amp;&amp; ssh root@$i esxcfg-nics -l | grep ixgbe;done</p>

        <h3>Display ESXi host active adapters</h3>

         <p class="codedescription">Description: Display the ESXi host's active, standby and unused adapters</p>

        <p>for i in `hostips`;do echo $i &amp;&amp;&nbsp; ssh root@$i "esxcli network vswitch standard policy failover get --vswitch-name vSwitch0";done</p>

        <h3>Display ESXi host routing tables</h3>

         <p class="codedescription">Description: Display the ESXi host's routing tables</p>

        <p>for i in `hostips`;do ssh root@$i 'esxcfg-route -l';done</p>

        <h3>Check if VAAI is enabled on datastore</h3>

         <p class="codedescription">Description: Check whether or not VAAI is enabled/supported for a datastore</p>

        <p>vmkfstools -Ph /vmfs/volumes/&lt;Datastore Name&gt;</p>

        <h3>Set VIB acceptance level to community supported</h3>

         <p class="codedescription">Description: Set the vib acceptance level to CommunitySupported allowing for 3rd party vibs to be installed</p>

        <p>esxcli software acceptance set --level CommunitySupported</p>

        <h3>Install VIB</h3>

         <p class="codedescription">Description: Install a vib without checking the signature</p>

        <p>esxcli software vib install --viburl=/&lt;VIB directory&gt;/&lt;VIB name&gt; --no-sig-check</p>

        <p># OR</p>

        <p>esxcli software vib install --depoturl=/&lt;VIB directory&gt;/&lt;VIB name&gt; --no-sig-check</p>

        <h3>Check ESXi ramdisk space</h3>

         <p class="codedescription">Description: Check free space of ESXi ramdisk</p>

        <p>for i in `hostips`;do echo $i; ssh root@$i 'vdf -h';done</p>

        <h3>Clear pynfs logs</h3>

         <p class="codedescription">Description: Clears the pynfs logs on each ESXi host</p>

        <p>for i in `hostips`;do echo $i; ssh root@$i '&gt; /pynfs/pynfs.log';done</p>
        </section>

        <section data-type="sect1" id="metrics-and-thresholds-WjDtk">
        <h2>Metrics and Thresholds</h2>

        <p>More coming soon!</p>
        </section>
        </section>

        <section data-type="sect1" id="troubleshooting-andamp-advanced-administration-Dvlcl">
        <h2>Troubleshooting &amp; Advanced Administration</h2>

        <p>More coming soon!</p>
        </section>
        </div>

        <div data-type="part" id="book-of-hyper-v-6YeSv">
        <h1><span class="label">Part V. </span>Book of Hyper-V</h1>

        <section data-type="chapter" id="architecture-RWaFk">
        <h1>Architecture</h1>

        <section data-type="sect1" id="node-architecture-3GWta">
        <h2>Node Architecture</h2>

        <p>In Hyper-V deployments, the Controller VM (CVM) runs as a VM and disks are presented using disk passthrough.</p>

        <figure id="id-RNycG"><img alt="" class="iimagesv2hyperv_nodepng" src="imagesv2/hyperv_node.png">
        <figcaption><span class="label">Figure 13-1. </span>Hyper-V Node Architecture</figcaption>
        </figure>
        </section>

        <section data-type="sect1" id="configuration-maximums-and-scalability-6ZBUd">
        <h2>Configuration Maximums and Scalability</h2>

        <p>The following configuration maximums and scalability limits are applicable:</p>

        <ul>
        	<li>Maximum cluster size: <strong>64</strong></li>
        	<li>Maximum vCPUs per VM: <strong>64</strong></li>
        	<li>Maximum memory per VM: <strong>1TB</strong></li>
        	<li>Maximum VMs per host: <strong>1,024</strong></li>
        	<li>Maximum VMs per cluster: <strong>8,000</strong></li>
        </ul>

        <p>NOTE: As of Hyper-V 2012 R2</p>
        </section>
        </section>

        <section data-type="chapter" id="how-it-works-5qaC2">
        <h1>How It Works</h1>

        <section data-type="sect1" id="array-offloads-odx-6YyH1">
        <h2>Array Offloads – ODX</h2>

        <p>The Nutanix platform supports the Microsoft Offloaded Data Transfers (ODX), which allow the hypervisor to offload certain tasks to the array.&nbsp; This is much more efficient as the hypervisor doesn’t need to be the 'man in the middle'. Nutanix currently supports the ODX primitives for SMB, which include full copy and zeroing operations.&nbsp; However, contrary to VAAI which has a 'fast file' clone operation (using writable snapshots), the ODX primitives do not have an equivalent and perform a full copy.&nbsp; Given this, it is more efficient to rely on the native DSF clones which can currently be invoked via nCLI, REST, or Powershell CMDlets. Currently ODX IS invoked for the following operations:</p>

        <ul>
        	<li>In VM or VM to VM file copy on DSF SMB share</li>
        	<li>SMB share file copy</li>
        </ul>

        <p>Deploy the template from the SCVMM Library (DSF SMB share) – NOTE: Shares must be added to the SCVMM cluster using short names (e.g., not FQDN).&nbsp; An easy way to force this is to add an entry into the hosts file for the cluster (e.g. 10.10.10.10&nbsp;&nbsp;&nbsp;&nbsp; nutanix-130).</p>

        <p>ODX is NOT invoked for the following operations:</p>

        <ul>
        	<li>Clone VM through SCVMM</li>
        	<li>Deploy template from SCVMM Library (non-DSF SMB Share)</li>
        	<li>XenDesktop Clone Deployment</li>
        </ul>

        <p>You can validate ODX operations are taking place by using the ‘NFS Adapter’ Activity Traces page (yes, I said NFS, even though this is being performed via SMB).&nbsp; The operations activity show will be ‘NfsSlaveVaaiCopyDataOp‘ when copying a vDisk and ‘NfsSlaveVaaiWriteZerosOp‘ when zeroing out a disk.</p>
        </section>
        </section>

        <section data-type="chapter" id="administration-5bnsD">
        <h1>Administration</h1>

        <section data-type="sect1" id="important-pages-3a0fQ">
        <h2>Important Pages</h2>

        <p>More coming soon!</p>
        </section>

        <section data-type="sect1" id="command-reference-AboId">
        <h2>Command Reference</h2>

        <h3>Execute command on multiple remote hosts</h3>

         <p class="codedescription">Description: Execute a powershell on one or many remote hosts</p>

        <p class="codetext">$targetServers = "Host1","Host2","Etc"
        <br/>
        Invoke-Command -ComputerName&nbsp; $targetServers {
        <br/>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;COMMAND or SCRIPT BLOCK&gt;
        <br/>
        }</p>

        <h3>Check available VMQ Offloads</h3>

         <p class="codedescription">Description: Display the available number of VMQ offloads for a particular host</p>

        <p class="codetext">gwmi –Namespace “root\virtualization\v2” –Class Msvm_VirtualEthernetSwitch | select elementname, MaxVMQOffloads</p>

        <h3>Disable VMQ for VMs matching a specific prefix</h3>

        <p class="codedescription">Description: Disable VMQ for specific VMs</p>

        <p class="codetext">$vmPrefix = "myVMs"
        <br/>
        Get-VM | Where {$_.Name -match $vmPrefix} | Get-VMNetworkAdapter | Set-VMNetworkAdapter -VmqWeight 0</p>

        <h3>Enable VMQ for VMs matching a certain prefix</h3>

        <p class="codedescription">Description: Enable VMQ for specific VMs</p>

        <p class="codetext">$vmPrefix = "myVMs"
        <br/>
        Get-VM | Where {$_.Name -match $vmPrefix} | Get-VMNetworkAdapter | Set-VMNetworkAdapter -VmqWeight 1</p>

        <h3>Power-On VMs matching a certain prefix</h3>

        <p class="codedescription">Description: Power-On VMs matchin a certain prefix</p>

        <p class="codetext">$vmPrefix = "myVMs"
        <br/>
        Get-VM | Where {$_.Name -match $vmPrefix -and $_.StatusString -eq "Stopped"} | Start-VM</p>

        <h3>Shutdown VMs matching a certain prefix</h3>

        <p class="codedescription">Description: Shutdown VMs matchin a certain prefix</p>

        <p class="codetext">$vmPrefix = "myVMs"
        <br/>
        Get-VM | Where {$_.Name -match $vmPrefix -and $_.StatusString -eq "Running"}} | Shutdown-VM -RunAsynchronously</p>

        <h3>Stop VMs matching a certain prefix</h3>

        <p class="codedescription">Description: Stop VMs matchin a certain prefix</p>

        <p class="codetext">$vmPrefix = "myVMs"
        <br/>
        Get-VM | Where {$_.Name -match $vmPrefix} | Stop-VM</p>

        <h3>Get Hyper-V host RSS settings</h3>

        <p class="codedescription">Description: Get Hyper-V host RSS (recieve side scaling) settings</p>

        <p class="codetext">Get-NetAdapterRss</p>

        <h3>Check Winsh and WinRM connectivity</h3>

        <p class="codedescription">Description: Check Winsh and WinRM connectivity / status by performing a sample query which should return the computer system object not an error</p>

        <p class="codetext">allssh “'source /etc/profile &gt; /dev/null 2&gt;&amp;1; winsh "get-wmiobject win32_computersystem"'; done</p>
        </section>

        <section data-type="sect1" id="metrics-and-thresholds-EYksA">
        <h2>Metrics and Thresholds</h2>

        <p>More coming soon!</p>
        </section>
        </section>

        <section data-type="sect1" id="troubleshooting-andamp-advanced-administration-KpXSo">
        <h2>Troubleshooting &amp; Advanced Administration</h2>

        <p>More coming soon!</p>
        </section>
        </div>

        <section data-type="afterword" id="afterword-5AoSd">
        <h1>Afterword</h1>

        <p>Thank you for reading The Nutanix Bible!&nbsp; Stay tuned for many more upcoming updates and enjoy the Nutanix platform!</p>
        </section>

         
        </div> <!-- END CONTAINER -->
  </body>
</html>
